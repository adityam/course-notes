[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Single- and Multi-Agent Decision Theory",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "href": "notes/stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "title": "1  Introduction",
    "section": "1.1 The stochastic optimization problem",
    "text": "1.1 The stochastic optimization problem\nNow consider the simplest stochastic optimization problem. A decision maker has to choose an action \\(a \\in \\ALPHABET A\\). Upon choosing the action \\(a\\), the decision maker incurs a cost \\(c(a,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable with known probability distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(a, W) ]\\), where the expectation is with respect to the random variable \\(W\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:stochastic}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(a, W) ].\n\\end{equation}\\]\nDefine \\(J(a) = \\EXP[ c(a, W) ]\\). Then Problem \\eqref{eq:stochastic} is conceptually the same as Problem \\eqref{eq:basic} with the cost function \\(J(a)\\). Numerically, Problem \\eqref{eq:stochastic} is more difficult because computing \\(J(a)\\) involves evaluating an expectation, but we ignore the computational complexity for the time being."
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html#key-simplifying-idea",
    "href": "notes/stochastic-optimization/intro.html#key-simplifying-idea",
    "title": "1  Introduction",
    "section": "1.2 Key simplifying idea",
    "text": "1.2 Key simplifying idea\nIn the stochastic optimization problems considered above, the decision maker does not observe any data before making a decision. In many situations, the decision maker does observe some data, which is captured by the following model. Suppose a decision maker observes a random variable \\(S \\in \\ALPHABET S\\) and then chooses an action \\(A \\in \\ALPHABET A\\) as a function of his observation according to a decision rule \\(π\\), i.e., \\[ A = π(S). \\]\nUpon choosing the action \\(A\\), the decision maker incurs a cost \\(c(S,A,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable. We assume that the primitive random variables \\((S,W)\\) are defined on a common probability space and have a known joint distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(S, π(S), W)]\\), where the expectation is taken with respect to the joint probability distribution of \\((S,W)\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:obs} \\tag{P1}\n  \\min_{π \\colon \\ALPHABET S \\to \\ALPHABET A} \\EXP[ c(S, π(S), W) ].\n\\end{equation}\\]\nDefine \\(J(π) = \\EXP[ c(S, π(S), W) ]\\). Then, Problem \\eqref{eq:obs} is conceptually the same as Problem \\eqref{eq:basic} with one difference: In Problem \\eqref{eq:basic}, the minimization is over a parameter \\(a\\), while in Problem \\eqref{eq:obs}, the minimization is over a function \\(π\\).\nWhen \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are finite sets, the optimal policy can be obtained by an exhaustive search over all policies as follows: for each policy \\(π\\) compute the performance \\(J(π)\\) and then pick the policy \\(π\\) with the smallest expected cost.\nSuch an exhaustive search is not satisfying for two reasons. First, it has a high computational cost. There are \\(| \\ALPHABET A |^{| \\ALPHABET S |}\\) policies and, for each policy, we have to evaluate an expectation, which can be expensive. Second, the above enumeration procedure does not work when \\(\\ALPHABET S\\) or \\(\\ALPHABET A\\) are continuous sets.\nThere is an alternative way of viewing the problem that simplifies it considerably. Instead of viewing the optimization problem before the system starts running (i.e., the ex ante view), imagine that the decision maker waits until they see the realization \\(s\\) of \\(S\\) (i.e., the interim view). they then asks what action \\(a\\) should they take to minimize the expected conditional cost \\(Q(s,a) := \\EXP[ c(s,a, W) | S = s]\\), i.e., they consider the problem\n\\[\\begin{equation} \\label{eq:cond-1} \\tag{P2}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a,W) | S = s], \\quad\n  \\forall s \\in \\ALPHABET S.\n\\end{equation}\\]\nThus, Problem \\eqref{eq:obs}, which is a functional optimization problem, has been reduced to a collection of parameter optimization problems (Problem \\eqref{eq:cond-1}), one for each possible of \\(s\\).\nNow define \\[ \\begin{equation} \\label{eq:cond} \\tag{P2-policy}\n  π^∘(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]\n\\end{equation} \\] where ties (in the minimization) are broken arbitrarily.\n\nTheorem 1.1 The decision rule \\(π^∘\\) defined in \\eqref{eq:cond} is optimal for Problem \\ref{eq:basic}.\n\n\n\n\n\n\n\nRemark\n\n\n\nWe restricted the proof to finite \\(\\ALPHABET S\\), \\(\\ALPHABET A\\), \\(\\ALPHABET W\\). This is to avoid any measurability issues. If \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are continuous sets, we need to restrict to measurable \\(π\\) in Problem \\ref{eq:basic} (otherwise the expectation is not well defined; of course the cost \\(c\\) also has to be measurable). However, it is not immediately obvious that \\(π^∘\\) defined in \\eqref{eq:cond} is measurable. Conditions that ensure this are known as measurable selection theorems.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π\\) be any other decision rule. Then, \\[ \\begin{align*}\n  \\EXP[ c(S, π(S), W) ] &\\stackrel{(a)}= \\EXP[ \\EXP[c(S, π(S), W) | S ] ] \\\\\n  &\\stackrel{(b)}\\ge \\EXP[\\EXP[ c(S, π^∘(S), W) | S ] ] \\\\\n  &\\stackrel{(c)}= \\EXP[ c(S, π^∘(S), W) ],\n\\end{align*} \\] where \\((a)\\) and \\((c)\\) follow from the law of iterated expectations and \\((b)\\) follows from the definition of \\(π^∘\\) in \\eqref{eq:cond}.\n\n\n\nWe can also provide a partial converse of Theorem 1.1.\n\nTheorem 1.2 If \\(\\PR(S = s) > 0\\) for all \\(s\\), then any optimal policy \\(π^∘\\) for Problem \\(\\ref{eq:basic}\\) must satisfy \\(\\eqref{eq:cond}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove this by contradiction. Suppose \\(π^*\\) is an optimal policy that does not satisfy \\eqref{eq:cond}. By definition of \\(π^∘\\), it must be the case that for all states \\[\\begin{equation}\n   \\EXP[ c(s, π^∘(s), W) | S = s ]\n   \\le\n   \\EXP[ c(s, π^*(s), W) | S = s ] .\n   \\label{eq:ineq:1}\n\\end{equation}\\] Now, since \\(π^*\\) does not satisfy \\eqref{eq:cond}, there exists some state \\(s^∘ \\in \\ALPHABET S\\) such that \\[\\begin{equation}\n   \\EXP[ c(s^∘, π^*(s^∘), W) | S = s^∘ ]\n   >\n   \\EXP[ c(s^∘, π^∘(s^∘), W) | S = s^∘ ] .\n   \\label{eq:ineq:2}\n\\end{equation}\\] Therefore, \\[\\begin{align*}\n   \\EXP[ c(S, π^*(S), W) ]\n   &=\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^*(s), W) | S = s ] ]\n   \\\\\n   & \\stackrel{(a)}>\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^∘(s), W) | S = s ] ]\n   \\\\\n   &=\n   \\EXP[ c(S, π^∘(S), W) ]\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:ineq:1} and \\eqref{eq:ineq:2} and the inequality is strict becase \\(\\PR(S = s^∘) > 0\\). Thus, \\(J(π^*) > J(π^∘)\\) and, hence, \\(π^*\\) cannot be an optimal policy."
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "href": "notes/stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "title": "1  Introduction",
    "section": "1.3 Blackwell’s principle of irrelevant information",
    "text": "1.3 Blackwell’s principle of irrelevant information\nIn many scenarios, the decision maker may observe data which is irrelevant for evaluating performance. In such instances, the decision maker may ignore such information without affecting performance. Formally, we have the following result, which is known as Blackwell’s principle of irrelevant information.\n\nTheorem 1.3 (Blackwell’s principle of irrelevant information) Let \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), and \\(\\ALPHABET A\\) be standard Borel spaces and \\(S \\in \\ALPHABET S\\), \\(Y \\in \\ALPHABET Y\\), \\(W \\in \\ALPHABET W\\) be random variables defined on a common probability space.\nA decision maker observes \\((S,Y)\\) and chooses \\(A = π(S,Y)\\) to minimize \\(\\EXP[c(S,A,W)]\\), where \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is a measurable function.\nThen, if \\(W\\) is conditionally independent of \\(Y\\) given \\(S\\), then there is no loss of optimality in choosing \\(A\\) only as a function of \\(S\\).\nFormally, there exists a \\(π^* \\colon \\ALPHABET S \\to \\ALPHABET A\\) such that for all \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), \\[ \\EXP[c(S, π^*(S), W)] \\le \\EXP[ c(S, π(S,Y), W) ]. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result for the case when \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), \\(\\ALPHABET A\\) are finite.\nDefine \\[π^*(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]. \\] Then, by construction, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), we have that \\[ \\EXP[ c(s, π^*(s), W ) | S = s]  \\le \\EXP[ c(s,a,W) | S = s]. \\] Hence, for any \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), and for any \\(s \\in \\ALPHABET S\\) and \\(y \\in \\ALPHABET Y\\), we have \\[ \\begin{equation} \\label{eq:opt}\n  \\EXP[ c(s, π^*(s), W) | S = s] \\le \\EXP[ c(s, π(s,y),W) | S = s].\n\\end{equation} \\] The result follows by taking the expectation of both sides of \\eqref{eq:opt}.\n\n\n\nThe above proof doesn’t work for general Borel spaces because \\(π^*\\) defined above may not exist (inf vs min) or may not be measurable. See Blackwell (1964) for a formal proof."
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html#exercises",
    "href": "notes/stochastic-optimization/intro.html#exercises",
    "title": "1  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.1 (Computing optimal policies) Suppose \\(\\ALPHABET S = \\{1, 2 \\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET W\\) with joint distribution \\(P\\) shown below.\n\\[ P = \\MATRIX{ 0.25 & 0.15 & 0.05  \\\\ 0.30 & 0.10 & 0.15 } \\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S=2, W=1) = P_{21} = 0.30\\).\nThe cost function \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is shown below\n\\[\nc(\\cdot,\\cdot,1) = \\MATRIX{3 & 5 & 1 \\\\ 2 & 3 & 1 }, \\quad\nc(\\cdot,\\cdot,2) = \\MATRIX{4 & 3 & 1 \\\\ 1 & 2 & 8 }, \\quad\nc(\\cdot,\\cdot,3) = \\MATRIX{1 & 2 & 2 \\\\ 4 & 1 & 3 }.\n\\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(a\\). For example \\(c(s=1,a=2,w=1) = 5\\).\nFind the policy \\(π \\colon \\ALPHABET S \\to \\ALPHABET A\\) that minimizes \\(\\EXP[ c(S, π(S), W) ]\\).\n\n\nExercise 1.2 (Blackwell’s principle) Suppose \\(\\ALPHABET S = \\{1, 2\\}\\), \\(\\ALPHABET Y = \\{1, 2\\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,Y,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET Y × \\ALPHABET W\\), with joint distribution \\(Q\\) shown below. \\[\nQ_{Y = 1} = \\MATRIX{0.15 & 0.10 & 0.00 \\\\ 0.15 & 0.05 & 0.10}\n\\qquad\nQ_{Y = 2} = \\MATRIX{0.10 & 0.05 & 0.05 \\\\ 0.15 & 0.05 & 0.05}\n\\] For a fixed value of \\(y\\), the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S = 1, Y = 1, W = 3) = 0\\).\nThe cost function \\(c \\colon \\ALPHABET S × \\ALPHABET A × \\ALPHABET W \\to \\reals\\) is the same as the previous exercise.\n\nFind the policy \\(π \\colon \\ALPHABET S × \\ALPHABET Y \\to \\ALPHABET A\\) that minimizes \\(\\EXP[c(S, π(S,Y), W)]\\).\nCompare the solution with the solution of the previous exercise in view of Blackwell’s principle of irrelevant information. Clearly explain your observations.\n\n\n\n\nExercise 1.3 (Pollution monitoring) Consider the problem of monitoring the pollution level of a river. The river can have a high pollution level if there is a catastrophic failure of a factory upstream. There are then two “pollution states” indicating whether such a failure has not occured. We denote them by \\(S = 0\\) (indicating no failure) and \\(S = 1\\) (indicating catastrophic failure). Let \\([p, 1-p]\\) denote the prior probability mass function of \\(S\\).\nThe pollution monitoring system has a sensor which takes a measurement \\(y\\) of the pollution level. Let \\(f_s(y)\\) denote the probabiity density of the observation \\(y\\) conditional on the value of \\(s\\), \\(s \\in \\{0, 1\\}\\). Two actions are available at the monitoring system: raise an alarm or not raise an alarm. The cost of raising the alarm is \\(C_0\\) if the state \\(S\\) is \\(0\\) or zero if the state \\(S\\) is \\(1\\); the cost of not raising the alarm is zero if the state \\(S\\) is \\(0\\) or \\(C_1\\) if the state \\(S\\) is \\(1\\).\nShow that it is optimal to raise the alarm if \\[ p f_0(y) C_0 < (1 - p) f_1(y) C_1. \\] That is, it is optimal to raise the alarm if the likelihood ratio \\(f_1(y)/f_0(y)\\) exceeds the threshold value \\(p C_0/(1-p) C_1\\)."
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html#notes",
    "href": "notes/stochastic-optimization/intro.html#notes",
    "title": "1  Introduction",
    "section": "Notes",
    "text": "Notes\nTheorem 1.3 is due to Blackwell (1964) in a short 2.5 page paper. A similar result was used by Witsenhausen (1979) to show the structure of optimal coding strategies in real-time communication. Also see the blog post by Maxim Ragisnsky.\nExercise 1.3 is adaptive from Whittle (1996). It is a special instance of Bayesian hypothesis testing problem. We will study a generalization of this model later in sequential hypothesis testing\n\n\n\n\n\nBlackwell, D. 1964. Memoryless strategies in finite-stage dynamic programming. The Annals of Mathematical Statistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nWhittle, P. 1996. Optimal control: Basics and beyond. Wiley.\n\n\nWitsenhausen, H.S. 1979. On the structure of real-time source coders. Bell System Technical Journal 58, 6, 1437–1451."
  },
  {
    "objectID": "notes/stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "href": "notes/stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "title": "2  The newsvendor problem",
    "section": "2.1 Interlude with continuous version",
    "text": "2.1 Interlude with continuous version\nThe problem above has discrete action and discrete demand. To build intuition, we first consider the case where both the actions and demand are continuous. Let \\(f(w)\\) denote the probability density of the demand and \\(F(w)\\) denote the cumulative probability density. Then, the expected reward is \\[ \\begin{equation} \\label{eq:J}\nJ(a) = \\int_{0}^a [ q w - p a ] f(w) dw + \\int_{a}^\\infty [ q a - p a ] f(w) dw.\n\\end{equation}\\]\nTo fix ideas, we consider an example where \\(p = 0.5\\), \\(q = 1\\), and the demand is a :Kumaraswamy distribution with parameters \\((a,b) = (2,5)\\) and support \\([0,100]\\). The performance of a function of action is shown below.\n\np = 0.5\nq = 1\nr = function(w,a){ if(w<=a) { return q*w - p*a } else { return q*a - p*a } }\n\na_opt = inverseCDF( (q-p)/q )\n\nconfig = ({\n  // Kumaraswamy Distribution: https://en.wikipedia.org/wiki/Kumaraswamy_distribution\n  a: 2,\n  b: 5,\n  max: 100\n})\n\npdf = {\n  const a = config.a\n  const b = config.b\n\n  return function(x) {\n    var normalized = x/config.max\n    return a*b*normalized**(a-1)*(1 - normalized**a)**(b-1)\n  }\n}\n\ninverseCDF= {\n  const a = config.a\n  const b = config.b\n  return function(y) {\n     // Closed form expression for inverse CDF of Kumaraswamy distribution \n     return config.max * (1 - (1-y)**(1/b))**(1/a)\n  }\n}\n\npoints = { \n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,action) }\n  }\n  return points\n}\n\ncost_values = {\n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\nJ = {\n  const a = config.a\n  const b = config.b\n\n  return function(action) {\n    const n = 1000\n    var cost = 0\n    var w = 0\n    for (var i = 0; i < n; i++) {\n      w = config.max*i/n\n      if (w <= action) {\n        cost += (q*w - p*action)*pdf(w)/n\n      } else {\n        cost += (q*action - p*action)*pdf(w)/n\n      }\n    }\n    return cost\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof action = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncost = Math.round(J(action)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJ = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [action, J(action)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_opt, J(a_opt)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_values, {x:\"x\", y:\"y\"})\n  ]\n})\n\nplotPDF = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [action,0], [action, pdf(action)] ], {stroke: \"blue\"}),\n    Plot.line(points,{x:\"x\", y:\"y\"}),\n    Plot.areaY(points.filter(pt => pt.x <= action),{x:\"x\", y:\"y\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt => pt.x > action),{x:\"x\", y:\"y\", fill: \"pink\"})\n  ]\n})\n\nplotReward = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {x:\"x\", y:\"reward\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 2.1: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn Figure 2.1(a), the plot of \\(J(a)\\) is concave. We can verify that this is true in general.\n\n\n\n\n\n\nVerify that \\(J(a)\\) is concave\n\n\n\n\n\nTo verify that the function \\(J(a)\\) is concave, we compute the second derivative: \\[\n  \\frac{d^2 J(a)}{da^2} = - p f(a) - (q - p) f(a) = -q f(a) \\le 0.\n\\]\n\n\n\nThis suggests that we can use calculus to find the optimal value. In particular, to find the optimal action, we need to compute the \\(a\\) such that \\(dJ(a)/da = 0\\).\n\nProposition 2.1 For the newsvendor problem with continuous demand, the optimal action is \\[\n    a = F^{-1}\\left( 1 - \\frac{p}{q} \\right).\n  \\] In the literature, the quantity \\(1 - (p/q)\\) is called the critical fractile.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nLeibniz integral rule\n\n\n\n\n\n\\[ \\dfrac{d}{dx} \\left( \\int_{p(x)}^{q(x)} f(x,t) dt \\right)\n   = f(x, q(x)) \\cdot \\dfrac {d}{dx} q(x)\n   - f(x, p(x)) \\cdot \\dfrac {d}{dx} p(x)\n   + \\int_{p(x)}^{q(x)} \\dfrac{\\partial}{\\partial x} f(x,t) dt.\n\\]\n\n\n\nUsing the Leibniz integral rule, the derivative of the first term of \\(\\eqref{eq:J}\\) is \\[ [q a - p a ] f(a) + \\int_{0}^a [ -p ] f(w) dw\n= [q a - p a ] f(a) - p F(a).\n\\]\nSimilarly, the derivative of the second term of \\(\\eqref{eq:J}\\) is \\[ - [q a - p a] f(a) + \\int_{a}^{\\infty} (q-p)f(w)dw\n= - [q a - p a] f(a) + (q -p)[ 1 - F(a)].\n\\]\nCombining the two, we get that \\[ \\dfrac{dJ(a)}{da} = - p F(a) + (q - p) [ 1 - F(a) ]. \\]\nEquating this to \\(0\\), we get \\[ F(a) = \\dfrac{ q - p }{ q}\n\\quad\\text{or}\\quad\na = F^{-1} \\left( 1 - \\dfrac{  p }{ q } \\right).\n\\]"
  },
  {
    "objectID": "notes/stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "href": "notes/stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "title": "2  The newsvendor problem",
    "section": "2.2 Back to discrete version",
    "text": "2.2 Back to discrete version\nNow, we come back to the problem with discrete actions and discrete demand. Suppose \\(W\\) takes the values \\(\\ALPHABET W = \\{ w_1, w_2, \\dots, w_k \\}\\) (where \\(w_1 < w_2 < \\cdots < w_k\\)) with probabilities \\(\\{ μ_1, μ_2, \\dots, μ_k \\}\\). It is ease to see that in this case the action \\(a\\) should be in the set \\(\\{ w_1, w_2, \\dots, w_k \\}\\).\nTo fix ideas, we repeat the above numerical example when \\(\\ALPHABET W = \\{0, 1, \\dots, 100\\}\\).\n\npointsD = { \n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,actionD) }\n  }\n  return points\n}\n\ncost_valuesD = {\n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\n\na_optD = Math.round(a_opt*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof actionD = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncostD = Math.round(J(actionD)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJD = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [actionD, J(actionD)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_optD, J(a_optD)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_valuesD, {x:\"x\", y:\"y\", curve: \"step-after\"})\n  ]\n})\n\nplotPDFD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [actionD,0], [actionD, pdf(actionD)] ], {stroke: \"blue\"}),\n    Plot.line(pointsD,{x:\"x\", y:\"y\", curve:\"step-after\"}),\n    Plot.areaY(points.filter(pt => pt.x <= actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt => pt.x > actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"pink\"})\n  ]\n})\n\nplotRewardD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(pointsD, {x:\"x\", y:\"reward\", curve: \"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 2.2: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn the discrete case, the brute force search is easier (because there are a finite rather than continuous number of values). We cannot directly use the ideas from calculus because functions over discrete domain are not differentiable. But we can use a very similar idea. Instead of checking if \\(dJ(a)/da = 0\\), we check the sign of \\(J(w_{i+1}) - J(w_i)\\).\n\nProposition 2.2 Let \\(\\{M_i\\}_{i \\ge 1}\\) denote the cumulative mass function of the demand. Then, the optimal action is the largest value of \\(w_i\\) such that \\[\n    M_i \\le 1 - \\frac{p}{q}.\n  \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe expected reward for choice \\(w_i\\) is \\[ \\begin{align*} J(w_i) &=\n\\sum_{j < i} μ_j [ q w_j - p w_i ] + \\sum_{j \\ge i} μ_j [q w_i - p w_i]\n\\\\\n&= -p w_i + q \\Bigl[ \\sum_{j < i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr].\n\\end{align*}\\]\nThus, \\[ \\begin{align*}\n  J(w_{i+1}) - J(w_i) &=\n  -p w_{i+1} + q \\Bigl[ \\sum_{j < i+1}  μ_j w_j + \\sum_{j \\ge i+1} μ_j w_{i+1} \\Bigr]\n  \\\\\n  &\\quad + p w_i - q \\Bigl[ \\sum_{j < i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr]\n  \\\\\n  &= -p (w_{i+1} - w_i) + q \\Bigl[ \\sum_{j \\ge i + 1} μ_j ( w_{i+1} - w_i) \\Bigr]\n  \\\\\n  &= \\big( - p + q [ 1 - M_i ] \\big) (w_{i+1} - w_i).\n\\end{align*}\\] Note that \\[\nM_i \\le \\dfrac{q-p}{q}\n\\iff\n-p + q [ 1 - M_i ] \\ge 0.\n\\] Thus, for all \\(i\\) such that \\(M_i \\le (q-p)/q\\), we have \\(J(w_{i+1}) \\ge J(w_i)\\). On the other hand, for all \\(i\\) such that \\(M_i > (q-p)/q)\\), we have \\(J(w_{i+1}) < J(w_i)\\). Thus, the optimal amount to order is the largest \\(w_i\\) such that \\(M_i \\le (q-p)/q\\).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that the structure of the optimal solution is the same for continuous and discrete demand distributions."
  },
  {
    "objectID": "notes/stochastic-optimization/newsvendor.html#exercises",
    "href": "notes/stochastic-optimization/newsvendor.html#exercises",
    "title": "2  The newsvendor problem",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 2.1 (Qualitative properties of optimal solution) Intuitively, we expect that if the purchase price of the newspaper increases but the selling price remains the same, then the newsvendor should buy less newspapers. Formally prove this statement.\nHint: The CDF of a distribution is a weakly increasing function.\n\n\nExercise 2.2 (Monotonicity of optimal action) Consider two scenarios for the case with continuous demand and actions. In scenario 1, the demand is distributed according to PDF \\(f_1\\). In scenario 2, it is distributed according to PDF \\(f_2\\). Suppose \\(F_1(w) \\le F_2(w)\\) for all \\(w\\). Show that the optimal action \\(a_1\\) for scenario 1 is greater than the optimal action \\(a_2\\) for scenario 2.\nHint: Plot the two CDFs and try to interpret the optimal decision rule graphically.\n\n\nExercise 2.3 (Selling random wind) The amount \\(W\\) of power generated by the wind turbine is a positive real-valued random variable with probability density function \\(f\\). The operator of the wind turbine has to commit to provide a certain amount of power in the day-ahead market. The price of power is \\(\\$p\\) per MW.\nIf the operator commits to provide \\(a\\) MW of power and the wind generation \\(W\\) is less than \\(a\\), then he has to buy the balance \\(a - W\\) from a reserves market at the cost of \\(\\$ q\\) per unit, where \\(q > p\\). Thus, the reward of the operator is \\(r(a,W)\\) where \\[ r(a, w) = \\begin{cases}\n  p a, & \\text{if } w > a \\\\\n  p a - q (a  - w), & \\text{if } w < a.\n\\end{cases}\\]\nFind the value of commitment \\(a\\) that maximizes the expected reward."
  },
  {
    "objectID": "notes/stochastic-optimization/newsvendor.html#notes",
    "href": "notes/stochastic-optimization/newsvendor.html#notes",
    "title": "2  The newsvendor problem",
    "section": "Notes",
    "text": "Notes\nPerhaps the earliest model of the newsvendor problem appeared in Edgeworth (1888) in the context of a bank setting the level of cash reserves to cover demands from its customers. The solution to the basic model presented above and some of its variants was provided in Morse and Kimball (1951); Arrow et al. (1952); Whitin (1953). See Porteus (2008) for an accessible introduction.\nThe property \\(F_1(w) \\le F_2(w)\\) used in Exercise 2 is called stochastic dominance. Later in the course, we will study how stochastic dominance is useful to establish monotonicity properties of general MDPs.\n\nThe example of selling random wind in Exercise 2.3 is taken from Bitar et al. (2012).\n\n\n\n\n\nArrow, K.J., Harris, T., and Marschak, J. 1952. Optimal inventory policy. Econometrica 20, 1, 250–272. DOI: 10.2307/1907830.\n\n\nBitar, E., Poolla, K., Khargonekar, P., Rajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind. 2012 45th hawaii international conference on system sciences, IEEE, 1931–1937.\n\n\nEdgeworth, F.Y. 1888. The mathematical theory of banking. Journal of the Royal Statistical Society 51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nMorse, P. and Kimball, G. 1951. Methods of operations research. Technology Press of MIT.\n\n\nPorteus, E.L. 2008. Building intuition: Insights from basic operations management models and principles. In: D. Chhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nWhitin, S. 1953. The theory of inventory management. Princeton University Press."
  },
  {
    "objectID": "notes/mdps/gambling.html#computational-experiment",
    "href": "notes/mdps/gambling.html#computational-experiment",
    "title": "4  Optimal gambling",
    "section": "4.1 Computational experiment",
    "text": "4.1 Computational experiment\nTo fix ideas, let’s try to find the optimal policy on our own. An example strategy is given below.\n\nviewof code = Inputs.textarea({label: \"\", height:800, rows:11, width: 800, submit: true,\n   value: `// function bet(t, states, outcomes) {\n// t: current time\n// states: Array of states\n// outcomes: Array of outcomes\n// \n// modify the (javascript) code between the lines:\n// ===============================\n     // As an illustration, we implement the policy to bet\n     //  half of the wealth as long as one is winning. \n     if(t == 0) { \n        return 0.5*states[t] \n     } else { \n        return outcomes[t-1] == 1 ? 0.5*states[t] : 0\n     }\n// ================================\n//}`\n                              })\nviewof strategy = Inputs.radio([\"user code\", \"optimal\"], {value: \"user code\", label: \"Select strategy\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT = 100\nn = 25\nS1 = 100\n\nBernoulli = function(p) { return Math.random() <= p ? 1 : -1 }\n\nuser_strategy = new Function('t', 'states', 'outcomes', code)\n\noptimal_strategy = function(t,states,outcomes) {\n  return p < 0.5 ? 0 : (2*p - 1)*states[t]\n}\n\nbet = function(t, states, outcomes) {\n  return strategy == \"optimal\" ? optimal_strategy(t, states, outcomes) : user_strategy(t, states, outcomes) \n}\n\ndata = { \n  run;\n  var states = new Array(T+1)\n  var outcomes = new Array(T+1)\n  var trajectory = new Array(T+1)\n  var sum = 0\n\n  const initial = 100\n  var idx = 0\n\n  for (var i = 0; i < n; i++) {\n      // Initialize the array to NaN values.\n      for (var t = 0; t < T+1; t++) {\n        states[t] = NaN\n        outcomes[t] = Bernoulli(p)\n      }\n    \n      states[0] = initial\n      var action = 0\n    \n      for (var t = 0; t < T; t++, idx++) {\n        action = bet(t, states, outcomes)\n        states[t+1] = states[t] + outcomes[t] * action\n        trajectory[idx] = { \n          time: t+1, \n          state: states[t],\n          action: action, \n          outcome: outcomes[t],\n          reward: Math.log10(states[t]),\n          sample: i,\n        }\n      }\n      sum += Math.log10(states[T])\n  }\n  return { trajectories: trajectory, mean: sum/n }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming that $S_1 = $ , we plot the performance of this policy below. Choosing “optimal” in the radio button above gives the performance of the optimal policy (derived below).\n\nviewof p = Inputs.range([0, 1], {value: 0.6, label: \"p\", step: 0.01})\n\nviewof run = Inputs.button(\"Re-run simulation\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrewardPlot = Plot.plot({\n  grid: true,\n  marginRight: 40,\n  marks: [\n    // Data\n    Plot.line(data.trajectories, {x: \"time\", y: \"reward\", z: \"sample\", stroke: \"gray\", curve: \"step-after\"}),\n    Plot.line(data.trajectories, Plot.groupX({y: \"mean\"}, {x:\"time\", y: \"reward\", stroke: \"red\", strokeWidth: 2, curve: \"step-after\"})),\n\n    // Final value\n    Plot.dot([ [T,data.mean] ], { fill: \"red\"}),\n    Plot.text([ [T,data.mean] ], { text: Math.round(data.mean*100)/100, dx:18, fill:\"red\", fontWeight:\"bold\" }),\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n  ]\n})\n\n\n\n\n\nFigure 4.1: Plot of the performance of the strategy for a horizon of \\(T=\\) . The curves in gray show the performance over $n = $  difference sample paths and the red curve shows its mean. For ease of visualization, we are plotting the utility at each stage (i.e., \\(\\log s_t\\)), even though the reward is only received at the terminal time step. The red line shows the mean performance over the \\(n\\) sample paths. The final mean value of the reward is shown in red. You can toggle the select strategy button to see how the optimal strategy performs (and how close you came to it).\n\n\n\nAs we can see, most intuitive policies do not do so well. We will now see how to compute the optimal policy using dynamic programming."
  },
  {
    "objectID": "notes/mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "href": "notes/mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "title": "4  Optimal gambling",
    "section": "4.2 Optimal gambling strategy and value functions",
    "text": "4.2 Optimal gambling strategy and value functions\nThe above model of optimal gambling is a Markov decision process. Therefore, the optimal solution is given by dynamic programming.\n\nProposition 4.1 (Dynamic programming decomposition) Define the following value function \\(V_t \\colon \\reals_{\\ge 0} \\to \\reals\\) \\[ V_T(s) = \\log s \\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[ \\begin{align*}\nQ_t(s,a) &= \\EXP[ r_t(s,a) + V_{t+1}(S_{t+1}) \\,|\\, S_t = s, A_t = a] \\\\\n&= p V_{t+1}(s+a) + (1-p) V_{t+1}(s-a),\n\\end{align*}\n\\] and \\[ \\begin{align*}\nV_t(s) &=  \\max_{a \\in [0, s]} Q_t(s,a), \\\\\nπ_t(s) &= \\arg \\max_{a \\in [0, s]} Q_t(s,a). \\\\\n\\end{align*}\n\\]\nThen the strategy \\(π = (π_1, \\dots, π_{T-1})\\) is optimal.\n\n\n\n\n\n\n\nRemark\n\n\n\nThe above model is one of the rare instances when the optimal strategy and the optimal strategy and value function of an MDP can be identified in closed form.\n\n\n\nTheorem 4.1 (Optimal gambling strategy) When \\(p \\le 0.5\\):\n\nthe optimal strategy is to not gamble, specifically \\(π_t(s) = 0\\);\nthe value function is \\(V_t(s) = \\log s\\).\n\nWhen \\(p > 0.5\\):\n\nthe optimal strategy is to bet a fraction of the current fortune, specifically \\(π_t(s) = (2p - 1)s\\);\nthe value function is \\(V_t(s) = \\log s + (T - t) C\\), where \\[ C = \\log 2 + p \\log p + (1-p) \\log (1-p).\\]\n\n\nThe constant \\(C\\) defined in Theorem 4.1 is equal to the capacity of a binary symmetric channel! In fact, the above model was introduced by Kelly (1956) to show a gambling interpretation of information rates.\nWe prove the two cases separately.\n\n\n\n\n\n\nProof when \\(p \\le 0.5\\)\n\n\n\n\n\nLet \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p \\le 0.5\\) implies that \\(p \\le 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V_{t+1}(s) = \\log s\\). Now consider\n\\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { - (q - p) s - a } {s^2 - a^2 }\n   \\\\\n   &< 0.\n  \\end{align*}   \n\\]\nThis implies that \\(Q_t(s,a)\\) is decreasing in \\(a\\). Therefore,\n\\[ π_t(s) = \\arg\\max_{a \\in [0, s]} Q_t(s,a) = 0. \\]\nMoreover, \\[ V_t(s) = Q_t(s, π_t(s)) = \\log s.\\]\nThis completes the induction step.\n\n\n\n\n\n\n\n\n\nProof when \\(p > 0.5\\)\n\n\n\n\n\nAs in the previous case, let \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p > 0.5\\) implies that \\(p > 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V_{t+1}(s) = \\log s + (T -t - 1)C\\). Now consider\n\\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { (p - q) s - a } {s^2 - a^2 }\n  \\end{align*}   \n\\]\nSetting \\(\\partial Q_t(s,a)/\\partial a = 0\\), we get that the optimal action is\n\\[ π_t(s) = (p-q) s. \\]\nNote that \\((p-q) \\in (0,1]\\)\n\\[\n  \\frac { \\partial^2 Q_t(s,a) } {\\partial a^2} =\n   - \\frac p { (s + a)^2 } - \\frac q { (s - a)^2 }\n  < 0;\n\\] hence the above action is indeed the maximizer. Moreover, \\[ \\begin{align*}\n  V_t(s) &= Q_t(s, π_t(s))  \\\\\n  &= p V_{t+1}(s + π_t(s)) + q V_{t+1}( s - π_t(s) )\\\\\n  &= \\log s + p \\log (1 + (p-q)) + q \\log (1 - (p-q)) + (T - t -1)C \\\\\n  &= \\log s + p \\log 2p + q \\log 2q + (T - t + 1)C \\\\\n  &= \\log s + (T - t) C\n  \\end{align*}   \n\\]\nThis completes the induction step."
  },
  {
    "objectID": "notes/mdps/gambling.html#generalized-model",
    "href": "notes/mdps/gambling.html#generalized-model",
    "title": "4  Optimal gambling",
    "section": "4.3 Generalized model",
    "text": "4.3 Generalized model\nSuppose that the terminal reward \\(r_T(s)\\) is monotone increasing2 in \\(s\\).2 I use the convention that increasing means weakly increasing. The alternative term non-decreasing implicitly assumes that we are talking about a totally ordered set.\n\nTheorem 4.2 For the generalized optimal gambling problem:\n\nFor each \\(t\\), the value function \\(V_t(s)\\) is monotone increasing in \\(s\\).\nFor each \\(s\\), the value function \\(V_t(s)\\) is monotone decreasing in \\(t\\).\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(s\\)\n\n\n\n\n\nWe proceed by backward induction. \\(V_T(s) = r_T(s)\\) which is monotone increasing in \\(s\\). Assume that \\(V_{t+1}(s)\\) is increasing in \\(s\\). Now, consider \\(V_t(s)\\). Consider \\(s_1, s_2 \\in \\reals_{\\ge 0}\\) such that \\(s_1 \\le s_2\\). Then for any \\(a \\le s_1\\), we have that\n\\[ \\begin{align*}\n    Q_t(s_1, a) &= p V_{t+1}(s_1+a) + q V_{t+1}(s_1-a) \\\\\n    & \\stackrel{(a)}{\\le} p V_{t+1}(s_2 + a) + q V_{t+1}(s_2  - a) \\\\\n    & = Q_t(s_2, a),\n  \\end{align*}\n\\] where \\((a)\\) uses the induction hypothesis. Now consider\n\\[ \\begin{align*}\n  V_t(s_1) &= \\max_{a \\in [0, s_1]} Q_t(s_1, a) \\\\\n  & \\stackrel{(b)}{\\le} \\max_{a \\in [0, s_1]} Q_t(s_2, a) \\\\\n  & \\le \\max_{a \\in [0, s_2]} Q_t(s_2, a) \\\\\n  &= V_t(s_2),\n  \\end{align*}\n\\] where \\((b)\\) uses monotonicity of \\(Q_t\\) in \\(s\\). This completes the induction step.\n\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(t\\)\n\n\n\n\n\nThis is a simple consequence of the following:\n\\[V_t(s) = \\max_{a \\in [0, s]} Q_t(s,a) \\ge Q_t(s,0) = V_{t+1}(s).\\]"
  },
  {
    "objectID": "notes/mdps/gambling.html#exercises",
    "href": "notes/mdps/gambling.html#exercises",
    "title": "4  Optimal gambling",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNote\n\n\n\nThe purpose of these series of exercises is to generalize the basic result to a model where the gambler can bet on many mutually exclusive outcomes (think of betting on multiple horses in a horse race).\n\n\n\nExercise 4.1 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log w_i\\] subject to:\n\n\\(w_i \\ge 0\\)\n\\(\\sum_{i=1}^n w_i \\le s\\).\n\nShow that the optimal solution is given by \\[ w_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 4.2 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log (s - a + na_i)\\] subject to:\n\n\\(a_i \\ge 0\\)\n\\(a = \\sum_{i=1}^n a_i \\le s\\).\n\nShow that the optimal solution is given by \\[ a_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 4.3 Consider an alternative of the optimal gambling problem where, at each time, the gambler can place bets on many mutually exclusive outcomes. Suppose there are \\(n\\) outcomes, with success probabilities \\((p_1, \\dots, p_n)\\). Let \\((A_{1,t}, \\dots, A_{n,t})\\) denote the amount that the gambler bets on each outcome. The total amount \\(A_t := \\sum_{i=1}^n A_{i,t}\\) must be less than the gambler’s fortune \\(S_t\\). If \\(W_t\\) denotes the winning outcome, then the gambler’s wealth evolves according to \\[ S_{t+1} = S_t - A_t + nU_{W_t, t}.\\] For example, if there are three outcomes, gambler’s current wealth is \\(s\\), the gambler bets \\((a_1, a_2, a_3)\\), and outcome 2 wins, then the gambler wins \\(3 a_2\\) and his fortune at the next time is \\[ s - (a_1 + a_2 + a_3) + 3 a_2. \\]\nThe gambler’s utility is \\(\\log S_T\\), the logarithm of his final wealth. Find the strategy that maximizes the gambler’s expected utility.\nHint: Argue that the value function is of the form \\(V_t(s) = \\log s + (T -t)C\\), where  \\[C = \\log n - H(p_1, \\dots, p_n)\\] where \\(H(p_1, \\dots, p_n) = - \\sum_{i=1}^n p_i \\log p_i\\) is the entropy of a random variable with pmf \\((p_1, \\dots, p_n)\\).The constant \\(C\\) is the capacity of a symmetric discrete memoryless with \\(n\\) outputs and for every input, the output probabilities are a permutation of \\((p_1, \\dots, p_n)\\)."
  },
  {
    "objectID": "notes/mdps/gambling.html#notes",
    "href": "notes/mdps/gambling.html#notes",
    "title": "4  Optimal gambling",
    "section": "Notes",
    "text": "Notes\nThe above model (including the model described in the exercise) was introduced by Kelly (1956). However, Kelly restricted attention to “bet a constant fraction of your fortune” betting strategy and found the optimal fraction. This strategy is sometimes referred to as :Kelly criteria. As far as I know, the dynamic programming treatment of the problem is due to Ross (1974). Ross also considered variations where the objective was to maximize the probability of reaching a preassigned fortune or maximizing the time until becoming broke.\nA generalization of the above model to general logarithmic and exponential utilities is presented in Ferguson and Gilstein (2004).\n\n\n\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004. Optimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nKelly, J.L., Jr. 1956. A new interpretation of information rate. Bell System Technical Journal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236."
  },
  {
    "objectID": "notes/mdps/power-delay-tradeoff.html#dynamic-program",
    "href": "notes/mdps/power-delay-tradeoff.html#dynamic-program",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "7.1 Dynamic program",
    "text": "7.1 Dynamic program\nWe can assume \\(Y_t = X_t - A_t\\) as a post-decision state in the above model and write the dynamic program as follows:\n\\[ V_{T+1}(x,s) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\\begin{align*}\n  H_t(y,s) &= \\lambda d(y) + \\EXP[ V_{t+1}(y + W_t, S_{t+1}) | S_t = s ], \\\\\n  V_t(x,s) &= \\min_{0 \\le a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\big\\}\n\\end{align*}\\]\n\n7.1.1 Monotonicity of value functions\n\nLemma 7.1 For all \\(t\\), \\(V_t(x,s)\\) and \\(H_t(y,s)\\) are increasing in both variables.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that the constraint set \\(\\ALPHABET A(x) = \\{0, \\dots, x\\}\\) satisfies the conditions that generalize the result of monotonicity to constrained actions.\nWe prove the two monotonicity properties by backward induction. First note that \\(V_{T+1}(x,s)\\) is trivially monotone. This forms the basis of induction. Now suppose \\(V_{t+1}(x,s)\\) is increasing in \\(x\\) and \\(s\\). Since \\(\\{S_t\\}_{t \\ge 1}\\) is stochastically monotone, \\[H_t(y,s) = \\lambda d(y) + \\EXP[ V_{t+1}(y + W_t, S_{t+1}) | S_t = s ]\\] is increasing in \\(s\\). Moreover, since both \\(d(y)\\) and \\(V_{t+1}(y + w, s)\\) are increasing in \\(y\\), so is \\(H_t(y,s)\\).\nNow, for every \\(a\\), \\(p(a) q(s)\\) and \\(H_t(x-a, s)\\) is increasing in \\(x\\) and \\(s\\). So, the pointwise minima over \\(a\\) is also increasing in \\(x\\) and \\(s\\).\n\n\n\n\n\n7.1.2 Convexity of value functions\n\nLemma 7.2 For all time \\(t\\) and channel state \\(s\\), \\(V_t(x,s)\\) and \\(H_t(y,s)\\) are convex in the first variable.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. First note that \\(V_{T+1}(x,s)\\) is trivially convex in \\(x\\). Now assume that \\(V_{t+1}(x,s)\\) is convex in \\(x\\). Then, \\(\\EXP[V_{t+1}(y + W_t, S_{t+1}) | S_t = s]\\) is weighted sum of convex functions and is, therefore, convex in \\(y\\). Therefore, \\(H_t(y,s)\\) is a sum of two convex functions and, therefore, convex in \\(y\\).\nWe cannot directly show the convexity of \\(V_t(x,s)\\) because the pointwise minimum of convex functions is not convex. So, we consider the following argument. Fix \\(s\\) and pick \\(x > 1\\). Let \\(\\underline a = π^*_t(x-1,s)\\) and \\(\\bar a = π^*_t(x+1,s)\\). Let \\(\\underline v = \\lfloor (\\underline a + \\bar a)/2 \\rfloor\\) and \\(\\bar v = \\lceil (\\underline a + \\bar a)/2 \\rceil\\). Note that both \\(\\underline v\\) and \\(\\bar v\\) are feasible at \\(x\\). Then, \\[ \\begin{align*}\n  \\hskip 2em & \\hskip -2em\n  V_t(x-1, s) + V_t(x+1, s)\n  \\\\\n  &=\n  [ p(\\underline a) + p(\\bar a) ] q(s) + H_t(x - 1 - \\underline a, s)\n  + H_t(x + 1 - \\bar a, s)\n  \\\\\n  &\\stackrel{(a)}\\ge [ p(\\underline v) + p(\\bar v)] q(s) +\n    H_t(x - \\underline v, s) + H_t(x - \\bar v, s) \\\\\n  &\\ge 2 \\min_{a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\\\\n  &= 2 V_t(x,s),\n\\end{align*} \\] where \\((a)\\) follows from convexity of \\(p(\\cdot)\\) and \\(H_t(\\cdot, s)\\). Thus, \\(V_t(x,s)\\) is convex in \\(x\\). This completes the induction step.\n\n\n\n\n\n7.1.3 Monotonicity of optimal policy in queue length\n\nTheorem 7.1 For all time \\(t\\) and channel state s\\(s\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is increasing in the queue length \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn the previous lemma, we have shown that \\(H_t(y,s)\\) is convex in \\(y\\). Therefore, \\(H_t(x-a, s)\\) is submodular in \\((x,a)\\).\n\nThus, for a fixed \\(s\\), \\(p(a)q(s) + H_t(x-a, s)\\) is submodular in \\((x,a)\\). Therefore, the optimal policy is increasing in \\(x\\).\n\n\n\nOne can show submodularity by finite difference, but for simplicity, we assume that \\(H_t(y,s)\\) is twice differentiable. Then, \\(\\partial^2 H_t(x - a, s)/ \\partial x \\partial a \\le 0\\) (by convexity of \\(H_t\\)).\n\n7.1.4 Monotonicity of optimal policy in channel state\nIt is natural to expect that for a fixed \\(x\\) the optimal policy is decreasing in \\(s\\). However, it is not possible to obtain the monotonicity of optimal policy in channel state in general. To see why this is difficult, let us impose a mild assumption on the arrival distribution.\n\nThe packet arrival distribution is weakly decreasing, i.e., for any \\(v,w \\in \\integers_{\\ge 0}\\) such that \\(v \\le w\\), we have that \\(P_W(v) \\ge P_W(w)\\).\n\nWe first start with a slight generalization of stochastic monotonicity result.\n\nLemma 7.3 Let \\(\\{p_i\\}_{i \\ge 0}\\) and \\(\\{q_i\\}_{i \\ge 0}\\) be real-valued non-negative sequences satisfying \\[ \\sum_{i \\le j} p_i \\le \\sum_{i \\le j} q_i, \\quad \\forall j.\\] Then, for any increasing sequence \\(\\{v_i\\}_{i \\ge 0}\\), we have \\[ \\sum_{i = 0}^\\infty p_i v_i \\ge \\sum_{i=0}^\\infty q_i v_i. \\]\n\nThe proof is similar to the proof for stochastic monotonicity.\n\nLemma 7.4 Under (asm-power-delay-density?), for all \\(t\\), \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nThe idea of the proof is similar to Lemma 1 of the notes on monotone MDPs.\nFix \\(y^+, y^- \\in \\integers_{\\ge 0}\\) and \\(s^+, s^- \\in \\ALPHABET S\\) such that \\(y^+ > y^-\\) and \\(s^+ > s^-\\). Now, for any \\(y' \\in \\integers_{\\ge 0}\\) and \\(s' \\in \\ALPHABET S\\) define \\[\\begin{align*}\n  π(y',s') = P_W(y' - y^+)P_S(s'|s^+) +\n             P_W(y' - y^-)P_S(s'|s^-),\n             \\\\\n  μ(y',s') = P_W(y' - y^-)P_S(s'|s^+) +\n             P_W(y' - y^+)P_S(s'|s^-).\n\\end{align*}\\]\nSince \\(P_S\\) is stochastically monotone, we have that for any \\(σ \\in \\ALPHABET S\\), \\[ \\sum_{s'=1}^{σ} P_S(s'|s^+) \\le \\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Moreover, due to (asm-power-delay-density?), we have that \\(P_W(y' - y^-) \\le P_W(y' - y^+)\\). Thus, \\[ [P_W(y' - y^+) - P_W(y' - y^-)] \\sum_{s'=1}^{σ} P_S(s'|s^+)\n\\le [P_W(y' - y^+) - P_W(y' - y^-)]\\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Rearranging terms, we get \\[ \\sum_{s'=1}^σ π(y',s') \\le \\sum_{s'=1}^σ μ(y',s'). \\] Thus, for any \\(y'\\), the sequence \\(π(y',s')\\) and \\(ν(y',s')\\) satisfy the condition of Lemma 7.3.\nNow, in Lemma 7.1, we have established that for any \\(y'\\), \\(V_{t+1}(y',s')\\) is increasing in \\(s'\\). Thus, from Lemma 7.3, we have \\[  \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Summing up over \\(y'\\), we get \\[  \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Or equivalently, \\[\\begin{align*}\n\\hskip 2em & \\hskip -2em\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^-) ]\n\\\\\n& \\ge\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^-) ] .\n\\end{align*}\\] Thus, \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\n\n\nEven under (asm-power-delay-density?), we cannot establish the monotonicity of \\(π^*_t(x,s)\\) is \\(s\\).\n\n\n\n\n\nNote that we have established that \\(H_t(y,s)\\) is supermodular in \\((y,s)\\). Thus, for any fixed \\(x\\), \\(H_t(x-a,s)\\) is submodular in \\((a,s)\\). Furthermore the function \\(p(a)q(s)\\) is increasing in both variables and therefore supermodular in \\((a,s)\\). Therefore, we cannot say anything specific about \\(p(a)q(s) + H_t(x-a, s)\\) which is a sum of submodular and supermodular functions."
  },
  {
    "objectID": "notes/mdps/power-delay-tradeoff.html#exercises",
    "href": "notes/mdps/power-delay-tradeoff.html#exercises",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 7.1 In this exercise, we provide sufficient conditions for the optimal policy to be monotone in the channel state. Suppose that the channel state \\(\\{S_t\\}_{t \\ge 1}\\) is an i.i.d. process. Then prove that for all time \\(t\\) and queue state \\(x\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is decreasing in channel state \\(s\\)."
  },
  {
    "objectID": "notes/mdps/power-delay-tradeoff.html#notes",
    "href": "notes/mdps/power-delay-tradeoff.html#notes",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "Notes",
    "text": "Notes\nThe mathematical model of power-delay trade-off is taken from Berry (2000), where the monotonicty results were proved using first principles. More detailed characterization of the optimal transmission strategy when the average power or the average delay goes to zero are provided in Berry and Gallager (2002) and Berry (2013). A related model is presented in Ding et al. (2016).\nFor a broader overview of power-delay trade offs in wireless communication, see Berry et al. (2012) and Yeh (2012).\nThe remark after Lemma 7.4 shows the difficulty in establishing monotonicity of optimal policies for a multi-dimensional state space. In fact, sometimes even when monotonicity appears to be intuitively obvious, it may not hold. See Sayedana and Mahajan (2020) for an example. For general discussions on monotonicity for multi-dimensional state spaces, see Topkis (1998) and Koole (2006). As an example of using such general conditions to establish monotonicity, see Sayedana et al. (2020).\n\n\n\n\nBerry, R.A. 2000. Power and delay trade-offs in fading channels. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay tradeoffs in fading channels—small-delay asymptotics. IEEE Transactions on Information Theory 59, 6, 3939–3952. DOI: 10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002. Communication over fading channels with delay constraints. IEEE Transactions on Information Theory 48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M. 2012. Energy-efficient scheduling under delay constraints for wireless networks. Synthesis Lectures on Communication Networks 5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A. 2016. On monotonicity of the optimal transmission policy in cross-layer adaptive \\(m\\) -QAM modulation. IEEE Transactions on Communications 64, 9, 3771–3785. DOI: 10.1109/TCOMM.2016.2590427.\n\n\nKoole, G. 2006. Monotonicity in markov reward and decision chains: Theory and applications. Foundations and Trends in Stochastic Systems 1, 1, 1–76. DOI: 10.1561/0900000002.\n\n\nSayedana, B. and Mahajan, A. 2020. Counterexamples on the monotonicity of delay optimal strategies for energy harvesting transmitters. IEEE Wireless Communications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E. 2020. Cross-layer communication over fading channels with adaptive decision feedback. International symposium on modeling and optimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press.\n\n\nYeh, E.M. 2012. Fundamental performance limits in cross-layer wireless optimization: Throughput, delay, and energy. Foundations and Trends in Communications and Information Theory 9, 1, 1–112. DOI: 10.1561/0100000014."
  },
  {
    "objectID": "linear-algebra/svd.html",
    "href": "linear-algebra/svd.html",
    "title": "13  Singular value decomposition",
    "section": "",
    "text": "14 Best rank-\\(k\\) approximations\nThere are two important matrix norms, the Frobenius norm which is defined as \\[\n  \\| A \\|_{F} = \\sqrt{ \\sum_{i,j} a_{ij}^2 }\n\\] and the induced norm which is defined as \\[\n  \\| A \\|_2 = \\max_{\\|x \\| = 1} \\| A x \\|.\n\\]\nNote that the Frobenius norm is equal to the square root of the sum of squares of the singular values and the \\(2\\)-norm is the largest singular value.\nLet \\(A\\) be an \\(n × d\\) matrix and think of \\(A\\) as the \\(n\\) points in \\(d\\)-dimensional space. The Frobenius norm of \\(A\\) is the square root of the sum of squared distance of the points to the origin. The induced norm is the square root of the sum of squared distances to the origin along the direction that maximizes this quantity."
  },
  {
    "objectID": "linear-algebra/svd.html#singular-values",
    "href": "linear-algebra/svd.html#singular-values",
    "title": "13  Singular value decomposition",
    "section": "13.1 Singular values",
    "text": "13.1 Singular values\nLet \\(A\\) be a \\(n × d\\) matrix. Then, the matrix \\(A^\\TRANS A\\) is a symmetric \\(d × d\\) matrix, so its eigenvalues are real. Moreover, \\(A^\\TRANS A\\) is positive semi-definite, so the eigen values are non-negative. Let \\(\\{ λ_1, \\dots, λ_d \\}\\) denote the eigenvalues of \\(A^\\TRANS A\\), with repetitions. Order then so that \\(λ_1 \\ge λ_2 \\ge \\dots \\ge λ_d \\ge 0\\). Let \\(σ_i = \\sqrt{λ_i}\\), so that \\(σ_1 \\ge σ_2 \\ge \\dots σ_d \\ge 0\\). These numbers are called the singular values of \\(A\\).\n\nProperties of singular values\n\nThe number of non-zero singular values of \\(A\\) equals to the rank of \\(A\\). In particular, if \\(A\\) is \\(n × d\\) where \\(n < d\\), then \\(A\\) has at most \\(n\\) nonzero singular values.\nIt can be shown that\n\\[ σ_1 = \\max_{\\|x\\| = 1}  \\| A x \\| . \\]\nLet \\(v_1\\) denote the arg-max of the above optimization. \\(v_1\\) is called the first singular vector of \\(A\\). Then,\n\\[ σ_2 = \\max_{ x \\perp v_1, \\|x \\| = 1}  \\| A x\\|. \\]\nLet \\(v_2\\) denote the arg-max of the above optimization. \\(v_2\\) is called the second singular vector of \\(A\\), and so on.\nLet \\(A\\) be a \\(n × d\\) matrix and \\(v_1, \\dots, v_r\\) be the singular vectors, where \\(r = \\text{rank}(A)\\). Then for any \\(k \\in \\{1, \\dots, r\\}\\), let \\(V_k\\) be the subspace spanned by \\(\\{v_1, \\dots, v_k\\}\\). Then, \\(V_k\\) is the best \\(k\\)-dimensional subspace for \\(A\\).\nFor any matrix \\(A\\), \\[ \\sum_{i =1}^r σ_i^2(A) = \\| A \\|_{F}^2\n   := \\sum_{j,k} a_{jk}^2. \\]\nAny vector \\(v\\) can be written as a linear combination of \\(v_1, \\dots, v_r\\) and a vector perpendicular to \\(V_r\\) (defined above). Now, \\(Av\\) can be written as the same linear combination of \\(Av_1, Av_2, \\dots, Av_r\\). So, \\(Av_1, \\dots, Av_r\\) form a fundamental set of vectors associated with \\(A\\). We normalize them to length one by \\[ u_i = \\frac{1}{σ_i(A)} A v_i. \\] The vectors \\(u_1, \\dots, u_r\\) are called the left singular vectors of \\(A\\). The \\(v_i\\) are called the right singular vectors.\nBoth the left and the right singular vectors are orthogonal.\n\n\n\n\n\n\n\nSingular value decomposition\n\n\n\nFor any matrix \\(A\\), \\[ A = \\sum_{i=1}^r σ_i u_i v_i^\\TRANS \\] where \\(u_i\\) and \\(v_i\\) are the left and right singular vectors, and \\(σ_i\\) are the singular values.\nEquivalently, in matrix notation: \\[ A = U D V^\\TRANS \\] where the columns of \\(U\\) and \\(V\\) consist of the left and right singular vectors, respectively, and \\(D\\) is a diagonal matrix whose diagonal entries are the singular values of \\(A\\).\n\n\nIf \\(A\\) is a positive definite square matrix, then the SVD and the eigen-decomposition coincide."
  },
  {
    "objectID": "linear-algebra/svd.html#notes",
    "href": "linear-algebra/svd.html#notes",
    "title": "13  Singular value decomposition",
    "section": "Notes",
    "text": "Notes\nThese chapter on SVD in Hopcroft and Kannan (2012) contains a nice intuitive explanation of SVD.\n\n\n\n\nHopcroft, J. and Kannan, R. 2012. Computer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf."
  },
  {
    "objectID": "linear-algebra/rkhs.html#review-of-linear-operators",
    "href": "linear-algebra/rkhs.html#review-of-linear-operators",
    "title": "14  Reproducing Kernel Hilbert Space",
    "section": "14.1 Review of Linear Operators",
    "text": "14.1 Review of Linear Operators\n\n\n\n\n\n\nLinear Operator\n\n\n\nLet \\(\\mathcal F\\) and \\(\\mathcal G\\) be normed vector spaces over \\(\\reals\\). A function \\(A \\colon \\mathcal F \\to \\mathcal G\\) is called a linear operator if it satisfies the following properties:\n\nHonogeneity: For any \\(α \\in \\reals\\) and \\(f \\in \\mathcal F\\), \\(A(αf) = α (Af)\\).\nAdditivity: For any \\(f,g \\in \\mathcal F\\), \\(A(f + g) = Af + Ag\\).\n\nThe operator norm of a linear operator is defined as \\[ \\NORM{A} = \\sup_{f \\in \\mathcal F} \\frac{ \\NORM{A f}_{\\mathcal G}}\n{\\NORM{f}}_{\\mathcal F}. \\]\nIf \\(\\NORM{A} < ∞\\), then the operator is said to be a bounded operator.\n\n\nAs an example, suppose \\(\\mathcal F\\) is an inner product space. For a \\(g \\in \\mathcal F\\), the operator \\(A_g \\colon \\mathcal F \\to \\reals\\) defined by \\(A_g(f) = \\langle f, g \\rangle\\) is a linear operator. Such scalar valued operators are called functionals on \\(\\mathcal F\\).\nLinear operators satisfy the following property.\n\nTheorem 14.1 If \\(A \\colon \\mathcal F \\to \\mathcal G\\) is a linear operator, then the following three conditions are equivalent:\n\n\\(A\\) is a bounded operator.\n\\(A\\) is continuous on \\(\\mathcal F\\).\n\\(A\\) is continious at one point of \\(\\mathcal F\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "href": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "title": "14  Reproducing Kernel Hilbert Space",
    "section": "14.2 Dual of a linear operator",
    "text": "14.2 Dual of a linear operator\nThere are two notions of dual of a linear operator: algebraic dual and topological dual. If \\(\\mathcal F\\) is a normed space, then the space of all linear functionals \\(A \\colon \\mathcal F \\to \\reals\\) is the algebraic dual space of \\(\\mathcal F\\); the space of all continuous linear functions \\(A \\colon \\mathcal F \\to \\reals\\) is the topological dual space of \\(\\mathcal F\\).\nIn finite-dimensional space, the two notions of dual spaces coincide (every linear operator on a normed, finite dimensional space is bounded). But this is not the case for infinite dimensional spaces.\n\nTheorem 14.2 (Riesz representation) In a Hilbert space \\(\\mathcal F\\), all continuous linear functionals are of the form \\(\\langle\\cdot, g\\rangle\\), for some \\(g \\in \\mathcal F\\).\n\nTwo Hilbert spaces \\(\\mathcal F\\) and \\(\\mathcal G\\) are said to be isometrically isomorphic if there is a linear bijective map \\(U \\colon \\mathcal F \\to \\mathcal G\\) which preserves the inner product, i.e., \\(\\langle f_1, f_2 \\rangle_{\\mathcal F} = \\langle U f_1, U f_2 \\rangle_{\\mathcal G}\\).\nNote that Riesz representation theorem gives a natural isometric isomorphism \\(\\psi \\colon g \\mapsto \\langle \\cdot, g \\rangle_{\\mathcal F}\\) between \\(\\mathcal F\\) and its topological dual \\(\\mathcal F'\\), whereby \\(\\NORM{ψ(g)}_{\\mathcal F'} = \\NORM{g}_{\\mathcal F}\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "href": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "title": "14  Reproducing Kernel Hilbert Space",
    "section": "14.3 Reproducing kernel Hilbert space",
    "text": "14.3 Reproducing kernel Hilbert space\nLet \\(\\mathcal H\\) be a Hilbert space of functions mapping from some non-empty set \\(\\ALPHABET X\\) to \\(\\reals\\). Note that for every \\(x \\in \\ALPHABET X\\), there is a very special functional on \\(\\mathcal H\\): the one that assigns to each \\(f \\in \\mathcal H\\), its value at \\(x\\). This is called the evaluation functional and denoted by \\(δ_x\\). In particular, \\(δ_x \\colon \\mathcal H \\to \\reals\\), where \\(δ_x \\colon f \\mapsto f(x)\\).\n\n\n\n\n\n\nReproducing kernel Hilbert space (RKHS)\n\n\n\nA Hilbert space \\(\\mathcal H\\) of functions \\(f \\colon \\ALPHABET X \\to \\reals\\) defined on a non-empty set \\(\\ALPHABET X\\) is said to be a RKHS if \\(δ_x\\) is continuous for all \\(x \\in \\ALPHABET X\\).\n\n\nIn view of Theorem 14.1, an equivalent definition is that a Hilbert space \\(\\mathcal H\\) is RKHS if the evaluation functionals \\(δ_x\\) are bounded, i.e., for every \\(x \\in \\ALPHABET X\\), there exists a \\(M_x\\) such that \\[ | δ_x | = | f(x) | \\le M_x \\| f \\|_{\\mathcal H}, \\quad \\forall f \\in \\mathcal H\\]\nAn immediate implication of the above property is that two functions which agree in RKHS norm agree at every point: \\[ | f(x) - g(x) | = | δ_x(f - g) | \\le M_x \\| f - g \\|_{\\mathcal H},\n   \\quad \\forall f,g \\in \\mathcal H. \\]\nFor example, the \\(L_2\\) space of square integrable functions i.e., \\(\\int_{\\reals^n} f(x)^2 dx < ∞\\) with inner product \\(\\int_{\\reals^n} f(x) g(x)dx\\) is a Hilbert space, but not an RKHS because the delta function, which has the reproducing property \\[ f(x) = \\int_{\\reals^n} δ(x - y) f(y) dy \\] is not bounded.\nRKHS are particularly well behaved. In particular, if we have a sequence of functions \\(\\{f_n\\}_{n \\ge 1}\\) which converges to a limit \\(f\\) in the Hilbert-space norm, i.e., \\(\\lim_{n \\to ∞} \\NORM{f_n - f}_{\\mathcal H} = 0\\), then they also converge pointwise, i.e., \\(\\lim_{n \\to ∞} f_n(x) = f(x)\\) for all \\(x \\in \\ALPHABET X\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-rhks",
    "href": "linear-algebra/rkhs.html#properties-of-rhks",
    "title": "14  Reproducing Kernel Hilbert Space",
    "section": "14.4 Properties of RHKS",
    "text": "14.4 Properties of RHKS\nRKHS has many useful properties:\n\nFor any RKHS, there exists a unique kernel \\(k \\colon \\ALPHABET X × \\ALPHABET X \\to \\reals\\) such that\n\nfor any \\(x \\in \\ALPHABET X\\), \\(k(\\cdot, x) \\in \\mathcal H\\),\nfor any \\(x \\in \\ALPHABET X\\) and \\(f \\in \\mathcal H\\), \\(\\langle f, k(\\cdot, x) \\rangle = f(x)\\) (the reproducing property).\n\nIn particular, for any \\(x,y \\in \\ALPHABET X\\), \\[ k(x,y) = \\langle k(\\cdot, x), k(\\cdot, y) \\rangle. \\] Thus, the kernel is a symmetric function.\nThe kernel is positive definite, i.e., for any \\(n \\ge 1\\), for all \\((a_1, \\dots, a_n) \\in \\reals^n\\) and \\((x_1, \\dots, x_n) \\in \\ALPHABET X^n\\), \\[ \\sum_{i=1}^n \\sum_{j=1}^n a_i a_i h(x_i, x_j) \\ge 0 \\]\nA conseuqence of positive definiteness is that \\[| k(x, y)|^2 \\le k(x, x) k(y, y). \\]\n(Moore-Aronszajn Theorem) For every positive definite kernel \\(K\\) on \\(\\ALPHABET X × \\ALPHABET X\\), there is a unique RKHS on \\(\\ALPHABET X\\) with \\(K\\) as its reproducing kernel."
  },
  {
    "objectID": "linear-algebra/rkhs.html#examples-of-kernels",
    "href": "linear-algebra/rkhs.html#examples-of-kernels",
    "title": "14  Reproducing Kernel Hilbert Space",
    "section": "14.5 Examples of kernels",
    "text": "14.5 Examples of kernels\nSome common examples of symmetric positive definite kernels for \\(\\ALPHABET X = \\reals^n\\) are as follows:\n\nLinear kernel \\[ k(x,y) = \\langle x, y \\rangle\\]\nGaussian kernel \\[ k(x,y) = \\exp\\biggl( - \\frac{\\| x - y \\|^2}{σ^2} \\biggr),\n   \\quad σ > 0. \\]\nPolynomail kernel \\[ k(x,y) = \\bigl( 1 + \\langle x, y \\rangle \\bigr)^d,\n   \\quad d \\in \\integers_{> 0}. \\]"
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-kernels",
    "href": "linear-algebra/rkhs.html#properties-of-kernels",
    "title": "14  Reproducing Kernel Hilbert Space",
    "section": "14.6 Properties of kernels",
    "text": "14.6 Properties of kernels\n\nSuppose \\(φ \\colon \\ALPHABET X \\to \\reals^n\\) is a feature map, then \\[ k(x,y) := \\langle φ(x), φ(y) \\rangle \\] is a kernel.\nNote that there are no conditions on \\(\\ALPHABET X\\) (e.g., \\(\\ALPHABET X\\) doesn’t need to be an inner product space).\nIf \\(k\\) is a kernel on \\(\\ALPHABET X\\), then for any \\(α > 0\\), \\(αk\\) is also a kernel.\nIf \\(k_1\\) and \\(k_2\\) are kernels on \\(\\ALPHABET X\\), then \\(k_1 + k_2\\) is also a kernel.\nIf \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be arbitrary sets and \\(A \\colon \\ALPHABET X \\to \\ALPHABET Y\\) is a map. Let \\(k\\) be a kernel on \\(\\ALPHABET Y\\). Then, \\(k(A(x_1), A(x_2))\\) is a kernel on \\(\\ALPHABET X\\).\nIf \\(k_1 \\colon \\ALPHABET X_1 × \\ALPHABET X_1 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_1\\) and \\(k_2 \\colon \\ALPHABET X_2 × \\ALPHABET X_2 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_2\\), then \\[ k( (x_1, x_2), (y_1, y_2) ) = k_1(x_1, y_1) k_2(x_2, y_2) \\] is a kernel on \\(\\ALPHABET X_1 × \\ALPHABET X_2\\).\n(Mercer-Hilber-Schmit theorems) If \\(k\\) is positive definite kernel (that is continous with finite trace), then there exists an infinite sequence of eiegenfunctions \\(\\{ φ_i \\colon \\ALPHABET X \\to \\reals \\}_{i \\ge 1}\\) and real eigenvalues \\(\\{λ_i\\}_{i \\ge 1}\\) such that we can write \\(k\\) as: \\[ k(x,y) = \\sum_{i=1}^∞ λ_i φ_i(x) φ_i(y). \\] This is analogous to the expression of a matrix in terms of its eigenvector and eigenvalues, except in this case we have functions and an infinity of them.\nUsing this property, we can define the inner product of RKHS in a simpler form. First, for any \\(f \\in \\mathcal H\\), define \\[ f_i = \\langle f, φ_i \\rangle.\\] Then, for any \\(f, g \\in \\mathcal H\\), \\[ \\langle f, g \\rangle = \\sum_{i=1}^∞ \\frac{ f_i g_i } { λ_i }. \\]"
  },
  {
    "objectID": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "href": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "title": "14  Reproducing Kernel Hilbert Space",
    "section": "14.7 Kernel ridge regression",
    "text": "14.7 Kernel ridge regression\nGiven labelled data \\(\\{ (x_i, y_i) \\}_{i=1}^n\\), and a feature map \\(φ \\colon \\ALPHABET X \\to \\ALPHABET Z\\), define the RKHS \\(\\ALPHABET H\\) of functions from \\(\\ALPHABET Z \\to \\reals\\) with the kernel \\(k(x,y) = \\langle φ(x), φ(y) \\rangle_{\\mathcal H}\\). Now, consider the problem of minimizing\n\\[f^* = \\arg \\min_{f \\in \\ALPHABET H}\n\\biggl(\n  \\sum_{i=1}^n \\bigl( y_i - \\langle f, φ(x_i) \\rangle_{\\mathcal{H}} \\bigr)^2 +\n  λ \\NORM{f}^2_{\\mathcal H}\n\\bigr).\\]\n\nTheorem 14.3 (The representer theoreom (simple version)) Given a loss function \\(\\ell \\colon \\ALPHABET Z^n \\to \\reals\\) and a penalty function \\(Ω \\colon \\reals \\to \\reals\\), there is as a solution of \\[ f^* = \\arg \\min_{f \\in \\mathcal H} \\ell(f(x_1), \\dots, f(x_n))\n        + Ω(\\NORM{f}^2_{\\mathcal H}). \\] that takes the the form \\[ f^* = \\sum_{i=1}^n α_i k(\\cdot, x_i).\\]\nIf \\(Ω\\) is strictly increasing, all solutions have this form.\n\nUsing the representer theorem, we know that the solution is of the form \\[ f = \\sum_{i=1}^n α_i φ(x_i). \\] Then, \\[\n\\sum_{i=1}^n \\bigl( y_i - \\langle f, φ_i(x_i) \\rangle_{\\mathcal H} \\bigr)^2\n  + λ \\NORM{f}_{\\mathcal H}^2\n= \\NORM{ y - K α}^2 + λ α^\\TRANS K α. \\]\nDifferentiating wrt \\(α\\) and setting this to zero, we get \\[\n  α^* = (K + λI_n)^{-1} y.\n\\]"
  },
  {
    "objectID": "notes/references.html",
    "href": "notes/references.html",
    "title": "References",
    "section": "",
    "text": "Arrow, K.J., Harris, T., and Marschak, J.\n1952. Optimal inventory policy. Econometrica 20, 1,\n250–272. DOI: 10.2307/1907830.\n\n\nBerry, R.A. 2000. Power and delay\ntrade-offs in fading channels. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay\ntradeoffs in fading channels—small-delay asymptotics. IEEE\nTransactions on Information Theory 59, 6, 3939–3952. DOI:\n10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002.\nCommunication over fading channels with delay constraints.\nIEEE Transactions on Information Theory\n48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M.\n2012. Energy-efficient scheduling under delay constraints for wireless\nnetworks. Synthesis Lectures on Communication Networks\n5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nBitar, E., Poolla, K., Khargonekar, P.,\nRajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind.\n2012 45th hawaii international conference on system sciences,\nIEEE, 1931–1937.\n\n\nBlackwell, D. 1964. Memoryless strategies\nin finite-stage dynamic programming. The Annals of Mathematical\nStatistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A.\n2016. On monotonicity of the optimal transmission policy in cross-layer\nadaptive m -QAM modulation.\nIEEE Transactions on Communications 64, 9, 3771–3785.\nDOI: 10.1109/TCOMM.2016.2590427.\n\n\nEdgeworth, F.Y. 1888. The mathematical\ntheory of banking. Journal of the Royal Statistical Society\n51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004.\nOptimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nHopcroft, J. and Kannan, R. 2012.\nComputer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf.\n\n\nKelly, J.L., Jr. 1956. A new\ninterpretation of information rate. Bell System Technical\nJournal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nKoole, G. 2006. Monotonicity in markov\nreward and decision chains: Theory and applications. Foundations and\nTrends in Stochastic Systems 1, 1, 1–76. DOI:\n10.1561/0900000002.\n\n\nMorse, P. and Kimball, G. 1951.\nMethods of operations research. Technology Press of MIT.\n\n\nPorteus, E.L. 2008. Building intuition:\nInsights from basic operations management models and principles. In: D.\nChhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nRoss, S.M. 1974. Dynamic programming and\ngambling models. Advances in Applied Probability 6, 3,\n593–606. DOI: 10.2307/1426236.\n\n\nSayedana, B. and Mahajan, A. 2020.\nCounterexamples on the monotonicity of delay optimal strategies for\nenergy harvesting transmitters. IEEE Wireless\nCommunications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E.\n2020. Cross-layer communication over fading channels with adaptive\ndecision feedback. International symposium on modeling and\noptimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nTopkis, D.M. 1998. Supermodularity\nand complementarity. Princeton University Press.\n\n\nWhitin, S. 1953. The theory of\ninventory management. Princeton University Press.\n\n\nWhittle, P. 1996. Optimal control:\nBasics and beyond. Wiley.\n\n\nWitsenhausen, H.S. 1979. On the structure\nof real-time source coders. Bell System Technical Journal\n58, 6, 1437–1451.\n\n\nYeh, E.M. 2012. Fundamental performance\nlimits in cross-layer wireless optimization: Throughput, delay, and\nenergy. Foundations and Trends in Communications and Information\nTheory 9, 1, 1–112. DOI: 10.1561/0100000014."
  },
  {
    "objectID": "506/01.html",
    "href": "506/01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Exercise 1.1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 1.2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 2.3 from the notes on the newsvendor problem. Provide an analytic solution to the problem, similar to the derivation of the analytic solution for the case of continuous demand and actions in the notes."
  }
]