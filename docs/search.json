[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Single- and Multi-Agent Decision Theory",
    "section": "",
    "text": "Outline\nThis website contains the course notes for two graduate courses that I teach: ECSE 506 (Stochastic Control and Decision Theory) and ECSE 508 (Multi-agent systems)."
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html",
    "href": "notes/stochastic-optimization/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Exercises\nTheorem 1.3 is due to Blackwell (1964) in a short 2.5 page paper. A similar result was used by Witsenhausen (1979) to show the structure of optimal coding strategies in real-time communication. Also see the blog post by Maxim Ragisnsky.\nExercise 1.3 is adaptive from Whittle (1996). It is a special instance of Bayesian hypothesis testing problem. We will study a generalization of this model later in sequential hypothesis testing"
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "href": "notes/stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "title": "1  Introduction",
    "section": "1.1 The stochastic optimization problem",
    "text": "1.1 The stochastic optimization problem\nNow consider the simplest stochastic optimization problem. A decision maker has to choose an action \\(a \\in \\ALPHABET A\\). Upon choosing the action \\(a\\), the decision maker incurs a cost \\(c(a,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable with known probability distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(a, W) ]\\), where the expectation is with respect to the random variable \\(W\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:stochastic}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(a, W) ].\n\\end{equation}\\]\nDefine \\(J(a) = \\EXP[ c(a, W) ]\\). Then Problem \\eqref{eq:stochastic} is conceptually the same as Problem \\eqref{eq:basic} with the cost function \\(J(a)\\). Numerically, Problem \\eqref{eq:stochastic} is more difficult because computing \\(J(a)\\) involves evaluating an expectation, but we ignore the computational complexity for the time being."
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html#key-simplifying-idea",
    "href": "notes/stochastic-optimization/intro.html#key-simplifying-idea",
    "title": "1  Introduction",
    "section": "1.2 Key simplifying idea",
    "text": "1.2 Key simplifying idea\nIn the stochastic optimization problems considered above, the decision maker does not observe any data before making a decision. In many situations, the decision maker does observe some data, which is captured by the following model. Suppose a decision maker observes a random variable \\(S \\in \\ALPHABET S\\) and then chooses an action \\(A \\in \\ALPHABET A\\) as a function of his observation according to a decision rule \\(π\\), i.e., \\[ A = π(S). \\]\nUpon choosing the action \\(A\\), the decision maker incurs a cost \\(c(S,A,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable. We assume that the primitive random variables \\((S,W)\\) are defined on a common probability space and have a known joint distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(S, π(S), W)]\\), where the expectation is taken with respect to the joint probability distribution of \\((S,W)\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:obs} \\tag{P1}\n  \\min_{π \\colon \\ALPHABET S \\to \\ALPHABET A} \\EXP[ c(S, π(S), W) ].\n\\end{equation}\\]\nDefine \\(J(π) = \\EXP[ c(S, π(S), W) ]\\). Then, Problem \\eqref{eq:obs} is conceptually the same as Problem \\eqref{eq:basic} with one difference: In Problem \\eqref{eq:basic}, the minimization is over a parameter \\(a\\), while in Problem \\eqref{eq:obs}, the minimization is over a function \\(π\\).\nWhen \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are finite sets, the optimal policy can be obtained by an exhaustive search over all policies as follows: for each policy \\(π\\) compute the performance \\(J(π)\\) and then pick the policy \\(π\\) with the smallest expected cost.\nSuch an exhaustive search is not satisfying for two reasons. First, it has a high computational cost. There are \\(| \\ALPHABET A |^{| \\ALPHABET S |}\\) policies and, for each policy, we have to evaluate an expectation, which can be expensive. Second, the above enumeration procedure does not work when \\(\\ALPHABET S\\) or \\(\\ALPHABET A\\) are continuous sets.\nThere is an alternative way of viewing the problem that simplifies it considerably. Instead of viewing the optimization problem before the system starts running (i.e., the ex ante view), imagine that the decision maker waits until they see the realization \\(s\\) of \\(S\\) (i.e., the interim view). they then asks what action \\(a\\) should they take to minimize the expected conditional cost \\(Q(s,a) := \\EXP[ c(s,a, W) | S = s]\\), i.e., they consider the problem\n\\[\\begin{equation} \\label{eq:cond-1} \\tag{P2}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a,W) | S = s], \\quad\n  \\forall s \\in \\ALPHABET S.\n\\end{equation}\\]\nThus, Problem \\eqref{eq:obs}, which is a functional optimization problem, has been reduced to a collection of parameter optimization problems (Problem \\eqref{eq:cond-1}), one for each possible of \\(s\\).\nNow define \\[ \\begin{equation} \\label{eq:cond} \\tag{P2-policy}\n  π^∘(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]\n\\end{equation} \\] where ties (in the minimization) are broken arbitrarily.\n\nTheorem 1.1 The decision rule \\(π^∘\\) defined in \\eqref{eq:cond} is optimal for Problem \\ref{eq:basic}.\n\n\n\n\n\n\n\nRemark\n\n\n\nWe restricted the proof finite \\(\\ALPHABET S\\), \\(\\ALPHABET A\\), \\(\\ALPHABET W\\). This is to avoid any measurability issues. If \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are continuous sets, we need to restrict to measurable \\(π\\) in Problem \\ref{eq:basic} (otherwise the expectation is not well defined; of course the cost \\(c\\) also has to be measurable). However, it is not immediately obvious that \\(π^∘\\) defined in \\eqref{eq:cond} is measurable. Conditions that ensure this are known as measurable selection theorems.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π\\) be any other decision rule. Then, \\[ \\begin{align*}\n  \\EXP[ c(S, π(S), W) ] &\\stackrel{(a)}= \\EXP[ \\EXP[c(S, π(S), W) | S ] ] \\\\\n  &\\stackrel{(b)}\\ge \\EXP[\\EXP[ c(S, π^∘(S), W) | S ] ] \\\\\n  &\\stackrel{(c)}= \\EXP[ c(S, π^∘(S), W) ],\n\\end{align*} \\] where \\((a)\\) and \\((c)\\) follow from the law of iterated expectations and \\((b)\\) follows from the definition of \\(π^∘\\) in \\eqref{eq:cond}.\n\n\n\nWe can also provide a partial converse of Theorem 1.1.\n\nTheorem 1.2 If \\(\\PR(S = s) > 0\\) for all \\(s\\), then any optimal policy \\(π^∘\\) for Problem \\(\\ref{eq:basic}\\) must satisfy \\(\\eqref{eq:cond}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove this by contradiction. Suppose \\(π^*\\) is an optimal policy that does not satisfy \\eqref{eq:cond}. By definition of \\(π^∘\\), it must be the case that for all states \\[\\begin{equation}\n   \\EXP[ c(s, π^∘(s), W) | S = s ]\n   \\le\n   \\EXP[ c(s, π^*(s), W) | S = s ] .\n   \\label{eq:ineq:1}\n\\end{equation}\\] Now, since \\(π^*\\) does not satisfy \\eqref{eq:cond}, there exists some state \\(s^∘ \\in \\ALPHABET S\\) such that \\[\\begin{equation}\n   \\EXP[ c(s^∘, π^*(s^∘), W) | S = s^∘ ]\n   >\n   \\EXP[ c(s^∘, π^∘(s^∘), W) | S = s^∘ ] .\n   \\label{eq:ineq:2}\n\\end{equation}\\] Therefore, \\[\\begin{align*}\n   \\EXP[ c(S, π^*(S), W) ]\n   &=\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^*(s), W) | S = s ] ]\n   \\\\\n   & \\stackrel{(a)}>\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^∘(s), W) | S = s ] ]\n   \\\\\n   &=\n   \\EXP[ c(S, π^∘(S), W) ]\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:ineq:1} and \\eqref{eq:ineq:2} and the inequality is strict becase \\(\\PR(S = s^∘) > 0\\). Thus, \\(J(π^*) > J(π^∘)\\) and, hence, \\(π^*\\) cannot be an optimal policy."
  },
  {
    "objectID": "notes/stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "href": "notes/stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "title": "1  Introduction",
    "section": "1.3 Blackwell’s principle of irrelevant information",
    "text": "1.3 Blackwell’s principle of irrelevant information\nIn many scenarios, the decision maker may observe data which is irrelevant for evaluating performance. In such instances, the decision maker may ignore such information without affecting performance. Formally, we have the following result, which is known as Blackwell’s principle of irrelevant information.\n\nTheorem 1.3 (Blackwell’s principle of irrelevant information) Let \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), and \\(\\ALPHABET A\\) be standard Borel spaces and \\(S \\in \\ALPHABET S\\), \\(Y \\in \\ALPHABET Y\\), \\(W \\in \\ALPHABET W\\) be random variables defined on a common probability space.\nA decision maker observes \\((S,Y)\\) and chooses \\(A = π(S,Y)\\) to minimize \\(\\EXP[c(S,A,W)]\\), where \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is a measurable function.\nThen, if \\(W\\) is conditionally independent of \\(Y\\) given \\(S\\), then there is no loss of optimality in choosing \\(A\\) only as a function of \\(S\\).\nFormally, there exists a \\(π^* \\colon \\ALPHABET S \\to \\ALPHABET A\\) such that for all \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), \\[ \\EXP[c(S, π^*(S), W)] \\le \\EXP[ c(S, π(S,Y), W) ]. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result for the case when \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), \\(\\ALPHABET A\\) are finite.\nDefine \\[π^*(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]. \\] Then, by construction, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), we have that \\[ \\EXP[ c(s, π^*(s), W ) | S = s]  \\le \\EXP[ c(s,a,W) | S = s]. \\] Hence, for any \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), and for any \\(s \\in \\ALPHABET S\\) and \\(y \\in \\ALPHABET Y\\), we have \\[ \\begin{equation} \\label{eq:opt}\n  \\EXP[ c(s, π^*(s), W) | S = s] \\le \\EXP[ c(s, π(s,y),W) | S = s].\n\\end{equation} \\] The result follows by taking the expectation of both sides of \\eqref{eq:opt}.\n\n\n\nThe above proof doesn’t work for general Borel spaces because \\(π^*\\) defined above may not exist (inf vs min) or may not be measurable. See Blackwell (1964) for a formal proof."
  },
  {
    "objectID": "notes/stochastic-optimization/newsvendor.html",
    "href": "notes/stochastic-optimization/newsvendor.html",
    "title": "2  The newsvendor problem",
    "section": "",
    "text": "Exercises\nPerhaps the earliest model of the newsvendor problem appeared in Edgeworth (1888) in the context of a bank setting the level of cash reserves to cover demands from its customers. The solution to the basic model presented above and some of its variants was provided in Morse and Kimball (1951); Arrow et al. (1952); Whitin (1953). See Porteus (2008) for an accessible introduction.\nThe property \\(F_1(w) \\le F_2(w)\\) used in Exercise 2 is called stochastic dominance. Later in the course, we will study how stochastic dominance is useful to establish monotonicity properties of general MDPs.\nThe example of selling random wind in Exercise 2.3 is taken from Bitar et al. (2012)."
  },
  {
    "objectID": "notes/stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "href": "notes/stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "title": "2  The newsvendor problem",
    "section": "2.1 Interlude with continuous version",
    "text": "2.1 Interlude with continuous version\nThe problem above has discrete action and discrete demand. To build intuition, we first consider the case where both the actions and demand are continuous. Let \\(f(w)\\) denote the probability density of the demand and \\(F(w)\\) denote the cumulative probability density. Then, the expected reward is \\[ \\begin{equation} \\label{eq:J}\nJ(a) = \\int_{0}^a [ q w - p a ] f(w) dw + \\int_{a}^\\infty [ q a - p a ] f(w) dw.\n\\end{equation}\\]\nTo fix ideas, we consider an example where \\(p = 0.5\\), \\(q = 1\\), and the demand is a Kumaraswamy distribution with parameters \\((a,b) = (2,5)\\) and support \\([0,100]\\). The performance of a function of action is shown below.\n\np = 0.5\nq = 1\nr = function(w,a){ if(w<=a) { return q*w - p*a } else { return q*a - p*a } }\n\na_opt = inverseCDF( (q-p)/q )\n\nconfig = ({\n  // Kumaraswamy Distribution: https://en.wikipedia.org/wiki/Kumaraswamy_distribution\n  a: 2,\n  b: 5,\n  max: 100\n})\n\npdf = {\n  const a = config.a\n  const b = config.b\n\n  return function(x) {\n    var normalized = x/config.max\n    return a*b*normalized**(a-1)*(1 - normalized**a)**(b-1)\n  }\n}\n\ninverseCDF= {\n  const a = config.a\n  const b = config.b\n  return function(y) {\n     // Closed form expression for inverse CDF of Kumaraswamy distribution \n     return config.max * (1 - (1-y)**(1/b))**(1/a)\n  }\n}\n\npoints = { \n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,action) }\n  }\n  return points\n}\n\ncost_values = {\n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\nJ = {\n  const a = config.a\n  const b = config.b\n\n  return function(action) {\n    const n = 1000\n    var cost = 0\n    var w = 0\n    for (var i = 0; i < n; i++) {\n      w = config.max*i/n\n      if (w <= action) {\n        cost += (q*w - p*action)*pdf(w)/n\n      } else {\n        cost += (q*action - p*action)*pdf(w)/n\n      }\n    }\n    return cost\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof action = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncost = Math.round(J(action)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJ = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [action, J(action)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_opt, J(a_opt)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_values, {x:\"x\", y:\"y\"})\n  ]\n})\n\nplotPDF = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [action,0], [action, pdf(action)] ], {stroke: \"blue\"}),\n    Plot.line(points,{x:\"x\", y:\"y\"}),\n    Plot.areaY(points.filter(pt => pt.x <= action),{x:\"x\", y:\"y\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt => pt.x > action),{x:\"x\", y:\"y\", fill: \"pink\"})\n  ]\n})\n\nplotReward = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {x:\"x\", y:\"reward\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 2.1: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn Figure 2.1(a), the plot of \\(J(a)\\) is concave. We can verify that this is true in general.\n\n\n\n\n\n\nVerify that \\(J(a)\\) is concave\n\n\n\n\n\nTo verify that the function \\(J(a)\\) is concave, we compute the second derivative: \\[\n  \\frac{d^2 J(a)}{da^2} = - p f(a) - (q - p) f(a) = -q f(a) \\le 0.\n\\]\n\n\n\nThis suggests that we can use calculus to find the optimal value. In particular, to find the optimal action, we need to compute the \\(a\\) such that \\(dJ(a)/da = 0\\).\n\nProposition 2.1 For the newsvendor problem with continuous demand, the optimal action is \\[\n    a = F^{-1}\\left( 1 - \\frac{p}{q} \\right).\n  \\] In the literature, the quantity \\(1 - (p/q)\\) is called the critical fractile.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nLeibniz integral rule\n\n\n\n\n\n\\[ \\dfrac{d}{dx} \\left( \\int_{p(x)}^{q(x)} f(x,t) dt \\right)\n   = f(x, q(x)) \\cdot \\dfrac {d}{dx} q(x)\n   - f(x, p(x)) \\cdot \\dfrac {d}{dx} p(x)\n   + \\int_{p(x)}^{q(x)} \\dfrac{\\partial}{\\partial x} f(x,t) dt.\n\\]\n\n\n\nUsing the Leibniz integral rule, the derivative of the first term of \\(\\eqref{eq:J}\\) is \\[ [q a - p a ] f(a) + \\int_{0}^a [ -p ] f(w) dw\n= [q a - p a ] f(a) - p F(a).\n\\]\nSimilarly, the derivative of the second term of \\(\\eqref{eq:J}\\) is \\[ - [q a - p a] f(a) + \\int_{a}^{\\infty} (q-p)f(w)dw\n= - [q a - p a] f(a) + (q -p)[ 1 - F(a)].\n\\]\nCombining the two, we get that \\[ \\dfrac{dJ(a)}{da} = - p F(a) + (q - p) [ 1 - F(a) ]. \\]\nEquating this to \\(0\\), we get \\[ F(a) = \\dfrac{ q - p }{ q}\n\\quad\\text{or}\\quad\na = F^{-1} \\left( 1 - \\dfrac{  p }{ q } \\right).\n\\]"
  },
  {
    "objectID": "notes/stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "href": "notes/stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "title": "2  The newsvendor problem",
    "section": "2.2 Back to discrete version",
    "text": "2.2 Back to discrete version\nNow, we come back to the problem with discrete actions and discrete demand. Suppose \\(W\\) takes the values \\(\\ALPHABET W = \\{ w_1, w_2, \\dots, w_k \\}\\) (where \\(w_1 < w_2 < \\cdots < w_k\\)) with probabilities \\(\\{ μ_1, μ_2, \\dots, μ_k \\}\\). It is ease to see that in this case the action \\(a\\) should be in the set \\(\\{ w_1, w_2, \\dots, w_k \\}\\).\nTo fix ideas, we repeat the above numerical example when \\(\\ALPHABET W = \\{0, 1, \\dots, 100\\}\\).\n\npointsD = { \n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,actionD) }\n  }\n  return points\n}\n\ncost_valuesD = {\n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\n\na_optD = Math.round(a_opt*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof actionD = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncostD = Math.round(J(actionD)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJD = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [actionD, J(actionD)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_optD, J(a_optD)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_valuesD, {x:\"x\", y:\"y\", curve: \"step-after\"})\n  ]\n})\n\nplotPDFD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [actionD,0], [actionD, pdf(actionD)] ], {stroke: \"blue\"}),\n    Plot.line(pointsD,{x:\"x\", y:\"y\", curve:\"step-after\"}),\n    Plot.areaY(points.filter(pt => pt.x <= actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt => pt.x > actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"pink\"})\n  ]\n})\n\nplotRewardD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(pointsD, {x:\"x\", y:\"reward\", curve: \"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 2.2: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn the discrete case, the brute force search is easier (because there are a finite rather than continuous number of values). We cannot directly use the ideas from calculus because functions over discrete domain are not differentiable. But we can use a very similar idea. Instead of checking if \\(dJ(a)/da = 0\\), we check the sign of \\(J(w_{i+1}) - J(w_i)\\).\n\nProposition 2.2 Let \\(\\{M_i\\}_{i \\ge 1}\\) denote the cumulative mass function of the demand. Then, the optimal action is the largest value of \\(w_i\\) such that \\[\n    M_i \\le 1 - \\frac{p}{q}.\n  \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe expected reward for choice \\(w_i\\) is \\[ \\begin{align*} J(w_i) &=\n\\sum_{j < i} μ_j [ q w_j - p w_i ] + \\sum_{j \\ge i} μ_j [q w_i - p w_i]\n\\\\\n&= -p w_i + q \\Bigl[ \\sum_{j < i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr].\n\\end{align*}\\]\nThus, \\[ \\begin{align*}\n  J(w_{i+1}) - J(w_i) &=\n  -p w_{i+1} + q \\Bigl[ \\sum_{j < i+1}  μ_j w_j + \\sum_{j \\ge i+1} μ_j w_{i+1} \\Bigr]\n  \\\\\n  &\\quad + p w_i - q \\Bigl[ \\sum_{j < i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr]\n  \\\\\n  &= -p (w_{i+1} - w_i) + q \\Bigl[ \\sum_{j \\ge i + 1} μ_j ( w_{i+1} - w_i) \\Bigr]\n  \\\\\n  &= \\big( - p + q [ 1 - M_i ] \\big) (w_{i+1} - w_i).\n\\end{align*}\\] Note that \\[\nM_i \\le \\dfrac{q-p}{q}\n\\iff\n-p + q [ 1 - M_i ] \\ge 0.\n\\] Thus, for all \\(i\\) such that \\(M_i \\le (q-p)/q\\), we have \\(J(w_{i+1}) \\ge J(w_i)\\). On the other hand, for all \\(i\\) such that \\(M_i > (q-p)/q)\\), we have \\(J(w_{i+1}) < J(w_i)\\). Thus, the optimal amount to order is the largest \\(w_i\\) such that \\(M_i \\le (q-p)/q\\).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that the structure of the optimal solution is the same for continuous and discrete demand distributions."
  },
  {
    "objectID": "notes/references.html",
    "href": "notes/references.html",
    "title": "References",
    "section": "",
    "text": "Arrow, K.J., Harris, T., and Marschak, J.\n1952. Optimal inventory policy. Econometrica 20, 1,\n250–272. DOI: 10.2307/1907830.\n\n\nBitar, E., Poolla, K., Khargonekar, P.,\nRajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind.\n2012 45th hawaii international conference on system sciences,\nIEEE, 1931–1937.\n\n\nBlackwell, D. 1964. Memoryless strategies\nin finite-stage dynamic programming. The Annals of Mathematical\nStatistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nEdgeworth, F.Y. 1888. The mathematical\ntheory of banking. Journal of the Royal Statistical Society\n51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nMorse, P. and Kimball, G. 1951.\nMethods of operations research. Technology Press of MIT.\n\n\nPorteus, E.L. 2008. Building intuition:\nInsights from basic operations management models and principles. In: D.\nChhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nWhitin, S. 1953. The theory of\ninventory management. Princeton University Press.\n\n\nWhittle, P. 1996. Optimal control:\nBasics and beyond. Wiley.\n\n\nWitsenhausen, H.S. 1979. On the structure\nof real-time source coders. Bell System Technical Journal\n58, 6, 1437–1451."
  },
  {
    "objectID": "506/01.html",
    "href": "506/01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Exercise 1.1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 1.2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 2.3 from the notes on the newsvendor problem. Provide an analytic solution to the problem, similar to the derivation of the analytic solution for the case of continuous demand and actions in the notes."
  }
]