[
  {
    "objectID": "stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "href": "stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "title": "2  Introduction",
    "section": "2.1 The stochastic optimization problem",
    "text": "2.1 The stochastic optimization problem\nNow consider the simplest stochastic optimization problem. A decision maker has to choose an action \\(a \\in \\ALPHABET A\\). Upon choosing the action \\(a\\), the decision maker incurs a cost \\(c(a,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable with known probability distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(a, W) ]\\), where the expectation is with respect to the random variable \\(W\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:stochastic}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(a, W) ].\n\\end{equation}\\]\nDefine \\(J(a) = \\EXP[ c(a, W) ]\\). Then Problem \\eqref{eq:stochastic} is conceptually the same as Problem \\eqref{eq:basic} with the cost function \\(J(a)\\). Numerically, Problem \\eqref{eq:stochastic} is more difficult because computing \\(J(a)\\) involves evaluating an expectation, but we ignore the computational complexity for the time being."
  },
  {
    "objectID": "stochastic-optimization/intro.html#key-simplifying-idea",
    "href": "stochastic-optimization/intro.html#key-simplifying-idea",
    "title": "2  Introduction",
    "section": "2.2 Key simplifying idea",
    "text": "2.2 Key simplifying idea\nIn the stochastic optimization problems considered above, the decision maker does not observe any data before making a decision. In many situations, the decision maker does observe some data, which is captured by the following model. Suppose a decision maker observes a random variable \\(S \\in \\ALPHABET S\\) and then chooses an action \\(A \\in \\ALPHABET A\\) as a function of his observation according to a decision rule \\(π\\), i.e., \\[ A = π(S). \\]\nUpon choosing the action \\(A\\), the decision maker incurs a cost \\(c(S,A,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable. We assume that the primitive random variables \\((S,W)\\) are defined on a common probability space and have a known joint distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(S, π(S), W)]\\), where the expectation is taken with respect to the joint probability distribution of \\((S,W)\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:obs} \\tag{P1}\n  \\min_{π \\colon \\ALPHABET S \\to \\ALPHABET A} \\EXP[ c(S, π(S), W) ].\n\\end{equation}\\]\nDefine \\(J(π) = \\EXP[ c(S, π(S), W) ]\\). Then, Problem \\eqref{eq:obs} is conceptually the same as Problem \\eqref{eq:basic} with one difference: In Problem \\eqref{eq:basic}, the minimization is over a parameter \\(a\\), while in Problem \\eqref{eq:obs}, the minimization is over a function \\(π\\).\nWhen \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are finite sets, the optimal policy can be obtained by an exhaustive search over all policies as follows: for each policy \\(π\\) compute the performance \\(J(π)\\) and then pick the policy \\(π\\) with the smallest expected cost.\nSuch an exhaustive search is not satisfying for two reasons. First, it has a high computational cost. There are \\(| \\ALPHABET A |^{| \\ALPHABET S |}\\) policies and, for each policy, we have to evaluate an expectation, which can be expensive. Second, the above enumeration procedure does not work when \\(\\ALPHABET S\\) or \\(\\ALPHABET A\\) are continuous sets.\nThere is an alternative way of viewing the problem that simplifies it considerably. Instead of viewing the optimization problem before the system starts running (i.e., the ex ante view), imagine that the decision maker waits until they see the realization \\(s\\) of \\(S\\) (i.e., the interim view). they then asks what action \\(a\\) should they take to minimize the expected conditional cost \\(Q(s,a) := \\EXP[ c(s,a, W) | S = s]\\), i.e., they consider the problem\n\\[\\begin{equation} \\label{eq:cond-1} \\tag{P2}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a,W) | S = s], \\quad\n  \\forall s \\in \\ALPHABET S.\n\\end{equation}\\]\nThus, Problem \\eqref{eq:obs}, which is a functional optimization problem, has been reduced to a collection of parameter optimization problems (Problem \\eqref{eq:cond-1}), one for each possible of \\(s\\).\nNow define \\[ \\begin{equation} \\label{eq:cond} \\tag{P2-policy}\n  π^∘(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]\n\\end{equation} \\] where ties (in the minimization) are broken arbitrarily.\n\nTheorem 2.1 The decision rule \\(π^∘\\) defined in \\eqref{eq:cond} is optimal for Problem \\ref{eq:basic}.\n\n\n\n\n\n\n\nRemark\n\n\n\nWe restricted the proof to finite \\(\\ALPHABET S\\), \\(\\ALPHABET A\\), \\(\\ALPHABET W\\). This is to avoid any measurability issues. If \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are continuous sets, we need to restrict to measurable \\(π\\) in Problem \\ref{eq:basic} (otherwise the expectation is not well defined; of course the cost \\(c\\) also has to be measurable). However, it is not immediately obvious that \\(π^∘\\) defined in \\eqref{eq:cond} is measurable. Conditions that ensure this are known as measurable selection theorems.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π\\) be any other decision rule. Then, \\[ \\begin{align*}\n  \\EXP[ c(S, π(S), W) ] &\\stackrel{(a)}= \\EXP[ \\EXP[c(S, π(S), W) | S ] ] \\\\\n  &\\stackrel{(b)}\\ge \\EXP[\\EXP[ c(S, π^∘(S), W) | S ] ] \\\\\n  &\\stackrel{(c)}= \\EXP[ c(S, π^∘(S), W) ],\n\\end{align*} \\] where \\((a)\\) and \\((c)\\) follow from the law of iterated expectations and \\((b)\\) follows from the definition of \\(π^∘\\) in \\eqref{eq:cond}.\n\n\n\nWe can also provide a partial converse of Theorem 2.1.\n\nTheorem 2.2 If \\(\\PR(S = s) > 0\\) for all \\(s\\), then any optimal policy \\(π^∘\\) for Problem \\(\\ref{eq:basic}\\) must satisfy \\(\\eqref{eq:cond}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove this by contradiction. Suppose \\(π^*\\) is an optimal policy that does not satisfy \\eqref{eq:cond}. By definition of \\(π^∘\\), it must be the case that for all states \\[\\begin{equation}\n   \\EXP[ c(s, π^∘(s), W) | S = s ]\n   \\le\n   \\EXP[ c(s, π^*(s), W) | S = s ] .\n   \\label{eq:ineq:1}\n\\end{equation}\\] Now, since \\(π^*\\) does not satisfy \\eqref{eq:cond}, there exists some state \\(s^∘ \\in \\ALPHABET S\\) such that \\[\\begin{equation}\n   \\EXP[ c(s^∘, π^*(s^∘), W) | S = s^∘ ]\n   >\n   \\EXP[ c(s^∘, π^∘(s^∘), W) | S = s^∘ ] .\n   \\label{eq:ineq:2}\n\\end{equation}\\] Therefore, \\[\\begin{align*}\n   \\EXP[ c(S, π^*(S), W) ]\n   &=\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^*(s), W) | S = s ] ]\n   \\\\\n   & \\stackrel{(a)}>\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^∘(s), W) | S = s ] ]\n   \\\\\n   &=\n   \\EXP[ c(S, π^∘(S), W) ]\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:ineq:1} and \\eqref{eq:ineq:2} and the inequality is strict becase \\(\\PR(S = s^∘) > 0\\). Thus, \\(J(π^*) > J(π^∘)\\) and, hence, \\(π^*\\) cannot be an optimal policy."
  },
  {
    "objectID": "stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "href": "stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "title": "2  Introduction",
    "section": "2.3 Blackwell’s principle of irrelevant information",
    "text": "2.3 Blackwell’s principle of irrelevant information\nIn many scenarios, the decision maker may observe data which is irrelevant for evaluating performance. In such instances, the decision maker may ignore such information without affecting performance. Formally, we have the following result, which is known as Blackwell’s principle of irrelevant information.\n\nTheorem 2.3 (Blackwell’s principle of irrelevant information) Let \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), and \\(\\ALPHABET A\\) be standard Borel spaces and \\(S \\in \\ALPHABET S\\), \\(Y \\in \\ALPHABET Y\\), \\(W \\in \\ALPHABET W\\) be random variables defined on a common probability space.\nA decision maker observes \\((S,Y)\\) and chooses \\(A = π(S,Y)\\) to minimize \\(\\EXP[c(S,A,W)]\\), where \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is a measurable function.\nThen, if \\(W\\) is conditionally independent of \\(Y\\) given \\(S\\), then there is no loss of optimality in choosing \\(A\\) only as a function of \\(S\\).\nFormally, there exists a \\(π^* \\colon \\ALPHABET S \\to \\ALPHABET A\\) such that for all \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), \\[ \\EXP[c(S, π^*(S), W)] \\le \\EXP[ c(S, π(S,Y), W) ]. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result for the case when \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), \\(\\ALPHABET A\\) are finite.\nDefine \\[π^*(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]. \\] Then, by construction, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), we have that \\[ \\EXP[ c(s, π^*(s), W ) | S = s]  \\le \\EXP[ c(s,a,W) | S = s]. \\] Hence, for any \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), and for any \\(s \\in \\ALPHABET S\\) and \\(y \\in \\ALPHABET Y\\), we have \\[ \\begin{equation} \\label{eq:opt}\n  \\EXP[ c(s, π^*(s), W) | S = s] \\le \\EXP[ c(s, π(s,y),W) | S = s].\n\\end{equation} \\] The result follows by taking the expectation of both sides of \\eqref{eq:opt}.\n\n\n\nThe above proof doesn’t work for general Borel spaces because \\(π^*\\) defined above may not exist (inf vs min) or may not be measurable. See Blackwell (1964) for a formal proof."
  },
  {
    "objectID": "stochastic-optimization/intro.html#exercises",
    "href": "stochastic-optimization/intro.html#exercises",
    "title": "2  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 2.1 (Computing optimal policies) Suppose \\(\\ALPHABET S = \\{1, 2 \\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET W\\) with joint distribution \\(P\\) shown below.\n\\[ P = \\MATRIX{ 0.25 & 0.15 & 0.05  \\\\ 0.30 & 0.10 & 0.15 } \\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S=2, W=1) = P_{21} = 0.30\\).\nThe cost function \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is shown below\n\\[\nc(\\cdot,\\cdot,1) = \\MATRIX{3 & 5 & 1 \\\\ 2 & 3 & 1 }, \\quad\nc(\\cdot,\\cdot,2) = \\MATRIX{4 & 3 & 1 \\\\ 1 & 2 & 8 }, \\quad\nc(\\cdot,\\cdot,3) = \\MATRIX{1 & 2 & 2 \\\\ 4 & 1 & 3 }.\n\\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(a\\). For example \\(c(s=1,a=2,w=1) = 5\\).\nFind the policy \\(π \\colon \\ALPHABET S \\to \\ALPHABET A\\) that minimizes \\(\\EXP[ c(S, π(S), W) ]\\).\n\n\nExercise 2.2 (Blackwell’s principle) Suppose \\(\\ALPHABET S = \\{1, 2\\}\\), \\(\\ALPHABET Y = \\{1, 2\\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,Y,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET Y × \\ALPHABET W\\), with joint distribution \\(Q\\) shown below. \\[\nQ_{Y = 1} = \\MATRIX{0.15 & 0.10 & 0.00 \\\\ 0.15 & 0.05 & 0.10}\n\\qquad\nQ_{Y = 2} = \\MATRIX{0.10 & 0.05 & 0.05 \\\\ 0.15 & 0.05 & 0.05}\n\\] For a fixed value of \\(y\\), the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S = 1, Y = 1, W = 3) = 0\\).\nThe cost function \\(c \\colon \\ALPHABET S × \\ALPHABET A × \\ALPHABET W \\to \\reals\\) is the same as the previous exercise.\n\nFind the policy \\(π \\colon \\ALPHABET S × \\ALPHABET Y \\to \\ALPHABET A\\) that minimizes \\(\\EXP[c(S, π(S,Y), W)]\\).\nCompare the solution with the solution of the previous exercise in view of Blackwell’s principle of irrelevant information. Clearly explain your observations.\n\n\n\n\nExercise 2.3 (Pollution monitoring) Consider the problem of monitoring the pollution level of a river. The river can have a high pollution level if there is a catastrophic failure of a factory upstream. There are then two “pollution states” indicating whether such a failure has not occured. We denote them by \\(S = 0\\) (indicating no failure) and \\(S = 1\\) (indicating catastrophic failure). Let \\([p, 1-p]\\) denote the prior probability mass function of \\(S\\).\nThe pollution monitoring system has a sensor which takes a measurement \\(y\\) of the pollution level. Let \\(f_s(y)\\) denote the probabiity density of the observation \\(y\\) conditional on the value of \\(s\\), \\(s \\in \\{0, 1\\}\\). Two actions are available at the monitoring system: raise an alarm or not raise an alarm. The cost of raising the alarm is \\(C_0\\) if the state \\(S\\) is \\(0\\) or zero if the state \\(S\\) is \\(1\\); the cost of not raising the alarm is zero if the state \\(S\\) is \\(0\\) or \\(C_1\\) if the state \\(S\\) is \\(1\\).\nShow that it is optimal to raise the alarm if \\[ p f_0(y) C_0 < (1 - p) f_1(y) C_1. \\] That is, it is optimal to raise the alarm if the likelihood ratio \\(f_1(y)/f_0(y)\\) exceeds the threshold value \\(p C_0/(1-p) C_1\\)."
  },
  {
    "objectID": "stochastic-optimization/intro.html#notes",
    "href": "stochastic-optimization/intro.html#notes",
    "title": "2  Introduction",
    "section": "Notes",
    "text": "Notes\nTheorem 2.3 is due to Blackwell (1964) in a short 2.5 page paper. A similar result was used by Witsenhausen (1979) to show the structure of optimal coding strategies in real-time communication. Also see the blog post by Maxim Ragisnsky.\nExercise 2.3 is adaptive from Whittle (1996). It is a special instance of Bayesian hypothesis testing problem. We will study a generalization of this model later in sequential hypothesis testing\n\n\n\n\n\nBlackwell, D. 1964. Memoryless strategies in finite-stage dynamic programming. The Annals of Mathematical Statistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nWhittle, P. 1996. Optimal control: Basics and beyond. Wiley.\n\n\nWitsenhausen, H.S. 1979. On the structure of real-time source coders. Bell System Technical Journal 58, 6, 1437–1451."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "href": "stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "title": "3  The newsvendor problem",
    "section": "3.1 Interlude with continuous version",
    "text": "3.1 Interlude with continuous version\nThe problem above has discrete action and discrete demand. To build intuition, we first consider the case where both the actions and demand are continuous. Let \\(f(w)\\) denote the probability density of the demand and \\(F(w)\\) denote the cumulative probability density. Then, the expected reward is \\[ \\begin{equation} \\label{eq:J}\nJ(a) = \\int_{0}^a [ q w - p a ] f(w) dw + \\int_{a}^\\infty [ q a - p a ] f(w) dw.\n\\end{equation}\\]\nTo fix ideas, we consider an example where \\(p = 0.5\\), \\(q = 1\\), and the demand is a :Kumaraswamy distribution with parameters \\((a,b) = (2,5)\\) and support \\([0,100]\\). The performance of a function of action is shown below.\n\np = 0.5\nq = 1\nr = function(w,a){ if(w<=a) { return q*w - p*a } else { return q*a - p*a } }\n\na_opt = inverseCDF( (q-p)/q )\n\nconfig = ({\n  // Kumaraswamy Distribution: https://en.wikipedia.org/wiki/Kumaraswamy_distribution\n  a: 2,\n  b: 5,\n  max: 100\n})\n\npdf = {\n  const a = config.a\n  const b = config.b\n\n  return function(x) {\n    var normalized = x/config.max\n    return a*b*normalized**(a-1)*(1 - normalized**a)**(b-1)\n  }\n}\n\ninverseCDF= {\n  const a = config.a\n  const b = config.b\n  return function(y) {\n     // Closed form expression for inverse CDF of Kumaraswamy distribution \n     return config.max * (1 - (1-y)**(1/b))**(1/a)\n  }\n}\n\npoints = { \n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,action) }\n  }\n  return points\n}\n\ncost_values = {\n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\nJ = {\n  const a = config.a\n  const b = config.b\n\n  return function(action) {\n    const n = 1000\n    var cost = 0\n    var w = 0\n    for (var i = 0; i < n; i++) {\n      w = config.max*i/n\n      if (w <= action) {\n        cost += (q*w - p*action)*pdf(w)/n\n      } else {\n        cost += (q*action - p*action)*pdf(w)/n\n      }\n    }\n    return cost\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof action = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncost = Math.round(J(action)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJ = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [action, J(action)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_opt, J(a_opt)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_values, {x:\"x\", y:\"y\"})\n  ]\n})\n\nplotPDF = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [action,0], [action, pdf(action)] ], {stroke: \"blue\"}),\n    Plot.line(points,{x:\"x\", y:\"y\"}),\n    Plot.areaY(points.filter(pt => pt.x <= action),{x:\"x\", y:\"y\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt => pt.x > action),{x:\"x\", y:\"y\", fill: \"pink\"})\n  ]\n})\n\nplotReward = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {x:\"x\", y:\"reward\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 3.1: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn Figure 3.1(a), the plot of \\(J(a)\\) is concave. We can verify that this is true in general.\n\n\n\n\n\n\nVerify that \\(J(a)\\) is concave\n\n\n\n\n\nTo verify that the function \\(J(a)\\) is concave, we compute the second derivative: \\[\n  \\frac{d^2 J(a)}{da^2} = - p f(a) - (q - p) f(a) = -q f(a) \\le 0.\n\\]\n\n\n\nThis suggests that we can use calculus to find the optimal value. In particular, to find the optimal action, we need to compute the \\(a\\) such that \\(dJ(a)/da = 0\\).\n\nProposition 3.1 For the newsvendor problem with continuous demand, the optimal action is \\[\n    a = F^{-1}\\left( 1 - \\frac{p}{q} \\right).\n  \\] In the literature, the quantity \\(1 - (p/q)\\) is called the critical fractile.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nLeibniz integral rule\n\n\n\n\n\n\\[ \\dfrac{d}{dx} \\left( \\int_{p(x)}^{q(x)} f(x,t) dt \\right)\n   = f(x, q(x)) \\cdot \\dfrac {d}{dx} q(x)\n   - f(x, p(x)) \\cdot \\dfrac {d}{dx} p(x)\n   + \\int_{p(x)}^{q(x)} \\dfrac{\\partial}{\\partial x} f(x,t) dt.\n\\]\n\n\n\nUsing the Leibniz integral rule, the derivative of the first term of \\(\\eqref{eq:J}\\) is \\[ [q a - p a ] f(a) + \\int_{0}^a [ -p ] f(w) dw\n= [q a - p a ] f(a) - p F(a).\n\\]\nSimilarly, the derivative of the second term of \\(\\eqref{eq:J}\\) is \\[ - [q a - p a] f(a) + \\int_{a}^{\\infty} (q-p)f(w)dw\n= - [q a - p a] f(a) + (q -p)[ 1 - F(a)].\n\\]\nCombining the two, we get that \\[ \\dfrac{dJ(a)}{da} = - p F(a) + (q - p) [ 1 - F(a) ]. \\]\nEquating this to \\(0\\), we get \\[ F(a) = \\dfrac{ q - p }{ q}\n\\quad\\text{or}\\quad\na = F^{-1} \\left( 1 - \\dfrac{  p }{ q } \\right).\n\\]"
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "href": "stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "title": "3  The newsvendor problem",
    "section": "3.2 Back to discrete version",
    "text": "3.2 Back to discrete version\nNow, we come back to the problem with discrete actions and discrete demand. Suppose \\(W\\) takes the values \\(\\ALPHABET W = \\{ w_1, w_2, \\dots, w_k \\}\\) (where \\(w_1 < w_2 < \\cdots < w_k\\)) with probabilities \\(\\{ μ_1, μ_2, \\dots, μ_k \\}\\). It is ease to see that in this case the action \\(a\\) should be in the set \\(\\{ w_1, w_2, \\dots, w_k \\}\\).\nTo fix ideas, we repeat the above numerical example when \\(\\ALPHABET W = \\{0, 1, \\dots, 100\\}\\).\n\npointsD = { \n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,actionD) }\n  }\n  return points\n}\n\ncost_valuesD = {\n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\n\na_optD = Math.round(a_opt*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof actionD = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncostD = Math.round(J(actionD)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJD = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [actionD, J(actionD)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_optD, J(a_optD)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_valuesD, {x:\"x\", y:\"y\", curve: \"step-after\"})\n  ]\n})\n\nplotPDFD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [actionD,0], [actionD, pdf(actionD)] ], {stroke: \"blue\"}),\n    Plot.line(pointsD,{x:\"x\", y:\"y\", curve:\"step-after\"}),\n    Plot.areaY(points.filter(pt => pt.x <= actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt => pt.x > actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"pink\"})\n  ]\n})\n\nplotRewardD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(pointsD, {x:\"x\", y:\"reward\", curve: \"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 3.2: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn the discrete case, the brute force search is easier (because there are a finite rather than continuous number of values). We cannot directly use the ideas from calculus because functions over discrete domain are not differentiable. But we can use a very similar idea. Instead of checking if \\(dJ(a)/da = 0\\), we check the sign of \\(J(w_{i+1}) - J(w_i)\\).\n\nProposition 3.2 Let \\(\\{M_i\\}_{i \\ge 1}\\) denote the cumulative mass function of the demand. Then, the optimal action is the largest value of \\(w_i\\) such that \\[\n    M_i \\le 1 - \\frac{p}{q}.\n  \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe expected reward for choice \\(w_i\\) is \\[ \\begin{align*} J(w_i) &=\n\\sum_{j < i} μ_j [ q w_j - p w_i ] + \\sum_{j \\ge i} μ_j [q w_i - p w_i]\n\\\\\n&= -p w_i + q \\Bigl[ \\sum_{j < i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr].\n\\end{align*}\\]\nThus, \\[ \\begin{align*}\n  J(w_{i+1}) - J(w_i) &=\n  -p w_{i+1} + q \\Bigl[ \\sum_{j < i+1}  μ_j w_j + \\sum_{j \\ge i+1} μ_j w_{i+1} \\Bigr]\n  \\\\\n  &\\quad + p w_i - q \\Bigl[ \\sum_{j < i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr]\n  \\\\\n  &= -p (w_{i+1} - w_i) + q \\Bigl[ \\sum_{j \\ge i + 1} μ_j ( w_{i+1} - w_i) \\Bigr]\n  \\\\\n  &= \\big( - p + q [ 1 - M_i ] \\big) (w_{i+1} - w_i).\n\\end{align*}\\] Note that \\[\nM_i \\le \\dfrac{q-p}{q}\n\\iff\n-p + q [ 1 - M_i ] \\ge 0.\n\\] Thus, for all \\(i\\) such that \\(M_i \\le (q-p)/q\\), we have \\(J(w_{i+1}) \\ge J(w_i)\\). On the other hand, for all \\(i\\) such that \\(M_i > (q-p)/q)\\), we have \\(J(w_{i+1}) < J(w_i)\\). Thus, the optimal amount to order is the largest \\(w_i\\) such that \\(M_i \\le (q-p)/q\\).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that the structure of the optimal solution is the same for continuous and discrete demand distributions."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#exercises",
    "href": "stochastic-optimization/newsvendor.html#exercises",
    "title": "3  The newsvendor problem",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 3.1 (Qualitative properties of optimal solution) Intuitively, we expect that if the purchase price of the newspaper increases but the selling price remains the same, then the newsvendor should buy less newspapers. Formally prove this statement.\nHint: The CDF of a distribution is a weakly increasing function.\n\n\nExercise 3.2 (Monotonicity of optimal action) Consider two scenarios for the case with continuous demand and actions. In scenario 1, the demand is distributed according to PDF \\(f_1\\). In scenario 2, it is distributed according to PDF \\(f_2\\). Suppose \\(F_1(w) \\le F_2(w)\\) for all \\(w\\). Show that the optimal action \\(a_1\\) for scenario 1 is greater than the optimal action \\(a_2\\) for scenario 2.\nHint: Plot the two CDFs and try to interpret the optimal decision rule graphically.\n\n\nExercise 3.3 (Selling random wind) The amount \\(W\\) of power generated by the wind turbine is a positive real-valued random variable with probability density function \\(f\\). The operator of the wind turbine has to commit to provide a certain amount of power in the day-ahead market. The price of power is \\(\\$p\\) per MW.\nIf the operator commits to provide \\(a\\) MW of power and the wind generation \\(W\\) is less than \\(a\\), then he has to buy the balance \\(a - W\\) from a reserves market at the cost of \\(\\$ q\\) per unit, where \\(q > p\\). Thus, the reward of the operator is \\(r(a,W)\\) where \\[ r(a, w) = \\begin{cases}\n  p a, & \\text{if } w > a \\\\\n  p a - q (a  - w), & \\text{if } w < a.\n\\end{cases}\\]\nFind the value of commitment \\(a\\) that maximizes the expected reward."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#notes",
    "href": "stochastic-optimization/newsvendor.html#notes",
    "title": "3  The newsvendor problem",
    "section": "Notes",
    "text": "Notes\nPerhaps the earliest model of the newsvendor problem appeared in Edgeworth (1888) in the context of a bank setting the level of cash reserves to cover demands from its customers. The solution to the basic model presented above and some of its variants was provided in Morse and Kimball (1951); Arrow et al. (1952); Whitin (1953). See Porteus (2008) for an accessible introduction.\nThe property \\(F_1(w) \\le F_2(w)\\) used in Exercise 2 is called stochastic dominance. Later in the course, we will study how stochastic dominance is useful to establish monotonicity properties of general MDPs.\n\nThe example of selling random wind in Exercise 3.3 is taken from Bitar et al. (2012).\n\n\n\n\n\nArrow, K.J., Harris, T., and Marschak, J. 1952. Optimal inventory policy. Econometrica 20, 1, 250–272. DOI: 10.2307/1907830.\n\n\nBitar, E., Poolla, K., Khargonekar, P., Rajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind. 2012 45th hawaii international conference on system sciences, IEEE, 1931–1937.\n\n\nEdgeworth, F.Y. 1888. The mathematical theory of banking. Journal of the Royal Statistical Society 51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nMorse, P. and Kimball, G. 1951. Methods of operations research. Technology Press of MIT.\n\n\nPorteus, E.L. 2008. Building intuition: Insights from basic operations management models and principles. In: D. Chhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nWhitin, S. 1953. The theory of inventory management. Princeton University Press."
  },
  {
    "objectID": "mdps/gambling.html#computational-experiment",
    "href": "mdps/gambling.html#computational-experiment",
    "title": "5  Optimal gambling",
    "section": "5.1 Computational experiment",
    "text": "5.1 Computational experiment\nTo fix ideas, let’s try to find the optimal policy on our own. An example strategy is given below.\n\nviewof code = Inputs.textarea({label: \"\", height:800, rows:11, width: 800, submit: true,\n   value: `// function bet(t, states, outcomes) {\n// t: current time\n// states: Array of states\n// outcomes: Array of outcomes\n// \n// modify the (javascript) code between the lines:\n// ===============================\n     // As an illustration, we implement the policy to bet\n     //  half of the wealth as long as one is winning. \n     if(t == 0) { \n        return 0.5*states[t] \n     } else { \n        return outcomes[t-1] == 1 ? 0.5*states[t] : 0\n     }\n// ================================\n//}`\n                              })\nviewof strategy = Inputs.radio([\"user code\", \"optimal\"], {value: \"user code\", label: \"Select strategy\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT = 100\nn = 25\nS1 = 100\n\nBernoulli = function(p) { return Math.random() <= p ? 1 : -1 }\n\nuser_strategy = new Function('t', 'states', 'outcomes', code)\n\noptimal_strategy = function(t,states,outcomes) {\n  return p < 0.5 ? 0 : (2*p - 1)*states[t]\n}\n\nbet = function(t, states, outcomes) {\n  return strategy == \"optimal\" ? optimal_strategy(t, states, outcomes) : user_strategy(t, states, outcomes) \n}\n\ndata = { \n  run;\n  var states = new Array(T+1)\n  var outcomes = new Array(T+1)\n  var trajectory = new Array(T+1)\n  var sum = 0\n\n  const initial = 100\n  var idx = 0\n\n  for (var i = 0; i < n; i++) {\n      // Initialize the array to NaN values.\n      for (var t = 0; t < T+1; t++) {\n        states[t] = NaN\n        outcomes[t] = Bernoulli(p)\n      }\n    \n      states[0] = initial\n      var action = 0\n    \n      for (var t = 0; t < T; t++, idx++) {\n        action = bet(t, states, outcomes)\n        states[t+1] = states[t] + outcomes[t] * action\n        trajectory[idx] = { \n          time: t+1, \n          state: states[t],\n          action: action, \n          outcome: outcomes[t],\n          reward: Math.log10(states[t]),\n          sample: i,\n        }\n      }\n      sum += Math.log10(states[T])\n  }\n  return { trajectories: trajectory, mean: sum/n }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming that $S_1 = $ , we plot the performance of this policy below. Choosing “optimal” in the radio button above gives the performance of the optimal policy (derived below).\n\nviewof p = Inputs.range([0, 1], {value: 0.6, label: \"p\", step: 0.01})\n\nviewof run = Inputs.button(\"Re-run simulation\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrewardPlot = Plot.plot({\n  grid: true,\n  marginRight: 40,\n  marks: [\n    // Data\n    Plot.line(data.trajectories, {x: \"time\", y: \"reward\", z: \"sample\", stroke: \"gray\", curve: \"step-after\"}),\n    Plot.line(data.trajectories, Plot.groupX({y: \"mean\"}, {x:\"time\", y: \"reward\", stroke: \"red\", strokeWidth: 2, curve: \"step-after\"})),\n\n    // Final value\n    Plot.dot([ [T,data.mean] ], { fill: \"red\"}),\n    Plot.text([ [T,data.mean] ], { text: Math.round(data.mean*100)/100, dx:18, fill:\"red\", fontWeight:\"bold\" }),\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n  ]\n})\n\n\n\n\n\nFigure 5.1: Plot of the performance of the strategy for a horizon of \\(T=\\) . The curves in gray show the performance over $n = $  difference sample paths and the red curve shows its mean. For ease of visualization, we are plotting the utility at each stage (i.e., \\(\\log s_t\\)), even though the reward is only received at the terminal time step. The red line shows the mean performance over the \\(n\\) sample paths. The final mean value of the reward is shown in red. You can toggle the select strategy button to see how the optimal strategy performs (and how close you came to it).\n\n\n\nAs we can see, most intuitive policies do not do so well. We will now see how to compute the optimal policy using dynamic programming."
  },
  {
    "objectID": "mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "href": "mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "title": "5  Optimal gambling",
    "section": "5.2 Optimal gambling strategy and value functions",
    "text": "5.2 Optimal gambling strategy and value functions\nThe above model of optimal gambling is a Markov decision process. Therefore, the optimal solution is given by dynamic programming.\n\nProposition 5.1 (Dynamic programming decomposition) Define the following value function \\(V_t \\colon \\reals_{\\ge 0} \\to \\reals\\) \\[ V_T(s) = \\log s \\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[ \\begin{align*}\nQ_t(s,a) &= \\EXP[ r_t(s,a) + V_{t+1}(S_{t+1}) \\,|\\, S_t = s, A_t = a] \\\\\n&= p V_{t+1}(s+a) + (1-p) V_{t+1}(s-a),\n\\end{align*}\n\\] and \\[ \\begin{align*}\nV_t(s) &=  \\max_{a \\in [0, s]} Q_t(s,a), \\\\\nπ_t(s) &= \\arg \\max_{a \\in [0, s]} Q_t(s,a). \\\\\n\\end{align*}\n\\]\nThen the strategy \\(π = (π_1, \\dots, π_{T-1})\\) is optimal.\n\n\n\n\n\n\n\nRemark\n\n\n\nThe above model is one of the rare instances when the optimal strategy and the optimal strategy and value function of an MDP can be identified in closed form.\n\n\n\nTheorem 5.1 (Optimal gambling strategy) When \\(p \\le 0.5\\):\n\nthe optimal strategy is to not gamble, specifically \\(π_t(s) = 0\\);\nthe value function is \\(V_t(s) = \\log s\\).\n\nWhen \\(p > 0.5\\):\n\nthe optimal strategy is to bet a fraction of the current fortune, specifically \\(π_t(s) = (2p - 1)s\\);\nthe value function is \\(V_t(s) = \\log s + (T - t) C\\), where \\[ C = \\log 2 + p \\log p + (1-p) \\log (1-p).\\]\n\n\nThe constant \\(C\\) defined in Theorem 5.1 is equal to the capacity of a binary symmetric channel! In fact, the above model was introduced by Kelly (1956) to show a gambling interpretation of information rates.\nWe prove the two cases separately.\n\n\n\n\n\n\nProof when \\(p \\le 0.5\\)\n\n\n\n\n\nLet \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p \\le 0.5\\) implies that \\(p \\le 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V_{t+1}(s) = \\log s\\). Now consider\n\\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { - (q - p) s - a } {s^2 - a^2 }\n   \\\\\n   &< 0.\n  \\end{align*}   \n\\]\nThis implies that \\(Q_t(s,a)\\) is decreasing in \\(a\\). Therefore,\n\\[ π_t(s) = \\arg\\max_{a \\in [0, s]} Q_t(s,a) = 0. \\]\nMoreover, \\[ V_t(s) = Q_t(s, π_t(s)) = \\log s.\\]\nThis completes the induction step.\n\n\n\n\n\n\n\n\n\nProof when \\(p > 0.5\\)\n\n\n\n\n\nAs in the previous case, let \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p > 0.5\\) implies that \\(p > 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V_{t+1}(s) = \\log s + (T -t - 1)C\\). Now consider\n\\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { (p - q) s - a } {s^2 - a^2 }\n  \\end{align*}   \n\\]\nSetting \\(\\partial Q_t(s,a)/\\partial a = 0\\), we get that the optimal action is\n\\[ π_t(s) = (p-q) s. \\]\nNote that \\((p-q) \\in (0,1]\\)\n\\[\n  \\frac { \\partial^2 Q_t(s,a) } {\\partial a^2} =\n   - \\frac p { (s + a)^2 } - \\frac q { (s - a)^2 }\n  < 0;\n\\] hence the above action is indeed the maximizer. Moreover, \\[ \\begin{align*}\n  V_t(s) &= Q_t(s, π_t(s))  \\\\\n  &= p V_{t+1}(s + π_t(s)) + q V_{t+1}( s - π_t(s) )\\\\\n  &= \\log s + p \\log (1 + (p-q)) + q \\log (1 - (p-q)) + (T - t -1)C \\\\\n  &= \\log s + p \\log 2p + q \\log 2q + (T - t + 1)C \\\\\n  &= \\log s + (T - t) C\n  \\end{align*}   \n\\]\nThis completes the induction step."
  },
  {
    "objectID": "mdps/gambling.html#generalized-model",
    "href": "mdps/gambling.html#generalized-model",
    "title": "5  Optimal gambling",
    "section": "5.3 Generalized model",
    "text": "5.3 Generalized model\nSuppose that the terminal reward \\(r_T(s)\\) is monotone increasing2 in \\(s\\).2 I use the convention that increasing means weakly increasing. The alternative term non-decreasing implicitly assumes that we are talking about a totally ordered set.\n\nTheorem 5.2 For the generalized optimal gambling problem:\n\nFor each \\(t\\), the value function \\(V_t(s)\\) is monotone increasing in \\(s\\).\nFor each \\(s\\), the value function \\(V_t(s)\\) is monotone decreasing in \\(t\\).\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(s\\)\n\n\n\n\n\nWe proceed by backward induction. \\(V_T(s) = r_T(s)\\) which is monotone increasing in \\(s\\). Assume that \\(V_{t+1}(s)\\) is increasing in \\(s\\). Now, consider \\(V_t(s)\\). Consider \\(s_1, s_2 \\in \\reals_{\\ge 0}\\) such that \\(s_1 \\le s_2\\). Then for any \\(a \\le s_1\\), we have that\n\\[ \\begin{align*}\n    Q_t(s_1, a) &= p V_{t+1}(s_1+a) + q V_{t+1}(s_1-a) \\\\\n    & \\stackrel{(a)}{\\le} p V_{t+1}(s_2 + a) + q V_{t+1}(s_2  - a) \\\\\n    & = Q_t(s_2, a),\n  \\end{align*}\n\\] where \\((a)\\) uses the induction hypothesis. Now consider\n\\[ \\begin{align*}\n  V_t(s_1) &= \\max_{a \\in [0, s_1]} Q_t(s_1, a) \\\\\n  & \\stackrel{(b)}{\\le} \\max_{a \\in [0, s_1]} Q_t(s_2, a) \\\\\n  & \\le \\max_{a \\in [0, s_2]} Q_t(s_2, a) \\\\\n  &= V_t(s_2),\n  \\end{align*}\n\\] where \\((b)\\) uses monotonicity of \\(Q_t\\) in \\(s\\). This completes the induction step.\n\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(t\\)\n\n\n\n\n\nThis is a simple consequence of the following:\n\\[V_t(s) = \\max_{a \\in [0, s]} Q_t(s,a) \\ge Q_t(s,0) = V_{t+1}(s).\\]"
  },
  {
    "objectID": "mdps/gambling.html#exercises",
    "href": "mdps/gambling.html#exercises",
    "title": "5  Optimal gambling",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNote\n\n\n\nThe purpose of these series of exercises is to generalize the basic result to a model where the gambler can bet on many mutually exclusive outcomes (think of betting on multiple horses in a horse race).\n\n\n\nExercise 5.1 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log w_i\\] subject to:\n\n\\(w_i \\ge 0\\)\n\\(\\sum_{i=1}^n w_i \\le s\\).\n\nShow that the optimal solution is given by \\[ w_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 5.2 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log (s - a + na_i)\\] subject to:\n\n\\(a_i \\ge 0\\)\n\\(a = \\sum_{i=1}^n a_i \\le s\\).\n\nShow that the optimal solution is given by \\[ a_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 5.3 Consider an alternative of the optimal gambling problem where, at each time, the gambler can place bets on many mutually exclusive outcomes. Suppose there are \\(n\\) outcomes, with success probabilities \\((p_1, \\dots, p_n)\\). Let \\((A_{1,t}, \\dots, A_{n,t})\\) denote the amount that the gambler bets on each outcome. The total amount \\(A_t := \\sum_{i=1}^n A_{i,t}\\) must be less than the gambler’s fortune \\(S_t\\). If \\(W_t\\) denotes the winning outcome, then the gambler’s wealth evolves according to \\[ S_{t+1} = S_t - A_t + nU_{W_t, t}.\\] For example, if there are three outcomes, gambler’s current wealth is \\(s\\), the gambler bets \\((a_1, a_2, a_3)\\), and outcome 2 wins, then the gambler wins \\(3 a_2\\) and his fortune at the next time is \\[ s - (a_1 + a_2 + a_3) + 3 a_2. \\]\nThe gambler’s utility is \\(\\log S_T\\), the logarithm of his final wealth. Find the strategy that maximizes the gambler’s expected utility.\nHint: Argue that the value function is of the form \\(V_t(s) = \\log s + (T -t)C\\), where  \\[C = \\log n - H(p_1, \\dots, p_n)\\] where \\(H(p_1, \\dots, p_n) = - \\sum_{i=1}^n p_i \\log p_i\\) is the entropy of a random variable with pmf \\((p_1, \\dots, p_n)\\).The constant \\(C\\) is the capacity of a symmetric discrete memoryless with \\(n\\) outputs and for every input, the output probabilities are a permutation of \\((p_1, \\dots, p_n)\\)."
  },
  {
    "objectID": "mdps/gambling.html#notes",
    "href": "mdps/gambling.html#notes",
    "title": "5  Optimal gambling",
    "section": "Notes",
    "text": "Notes\nThe above model (including the model described in the exercise) was introduced by Kelly (1956). However, Kelly restricted attention to “bet a constant fraction of your fortune” betting strategy and found the optimal fraction. This strategy is sometimes referred to as :Kelly criteria. As far as I know, the dynamic programming treatment of the problem is due to Ross (1974). Ross also considered variations where the objective was to maximize the probability of reaching a preassigned fortune or maximizing the time until becoming broke.\nA generalization of the above model to general logarithmic and exponential utilities is presented in Ferguson and Gilstein (2004).\n\n\n\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004. Optimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nKelly, J.L., Jr. 1956. A new interpretation of information rate. Bell System Technical Journal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236."
  },
  {
    "objectID": "mdps/inventory-management.html#dynamic-programming-decomposition",
    "href": "mdps/inventory-management.html#dynamic-programming-decomposition",
    "title": "6  Inventory Management",
    "section": "6.1 Dynamic programming decomposition",
    "text": "6.1 Dynamic programming decomposition\n\\(\\def\\S{\\mathbb{S}}\\)\nThe above model is a Markov decision process.1 Therefore, the optimal solution is given by dynamic programming.1 Part of the per-step cost depends on the future state \\(S_{t+1}\\). It is easy to show that the standard MDP model works even when the per-step cost is a function of \\((S_t, A_t, S_{t+1})\\)\nInstead of \\(\\integers\\), we use \\(\\S\\) to denote the possible values of states. The reason is that we will later consider the case when the state space is the set of reals, and we can still use the same equations.\n\nProposition 6.1 (Dynamic programming) Define the following value functions \\(V_t \\colon \\S \\to \\reals\\) \\[V_{T+1}(s) = 0\\] and for \\(t \\in \\{T, \\dots, 1\\}\\) \\[ Q_t(s, a) = p a + \\EXP[ h(s + a - W_t) + V_{t+1}( s + a - W_t ) ]\\] and \\[ \\begin{align*}\n  V_t(s) &= \\min_{a \\in \\S_{\\ge 0}} Q_t(s,a) \\\\\n  π_t(s) &= \\arg \\min_{a \\in \\S_{\\ge 0}} Q_t(s,a)\n  \\end{align*}\n\\] Then the strategy \\(π = (π_1, \\dots, π_T)\\) is optimal.\n\nIt is possible to simplify the above dynamic program by exploiting a feature of the model. Notice that the dynamics can be split into two parts: \\[ \\begin{align*}\n    Z_t &= S_t + A_t,  \\\\\n    S_{t+1} &= Z_t - W_t.\n   \\end{align*}\n\\] The first part, \\(Z_t\\), depends only on the current state and action. The second part depends only on \\(Z_t\\) and a primitive random variable. In this particular model, \\(Z_t\\) is a deterministic function of \\(S_t\\) and \\(A_t\\); but, in general, it could be stochastic as well; what is important is that the second part should only depend on \\(Z_t\\) and a primitive random variable. The variable \\(Z_t\\) is sometimes called the post-decision state.\nNow write the dynamic program in terms of the post-decision state as follows. Define \\[ H_t(z) = \\EXP[ h(z - W) + V_{t+1}(z-W) ].\\] Then the value function and optimal policy at time \\(t\\) can be written as: \\[ \\begin{align*}\n  V_t(s) &= \\min_{a \\in \\S_{\\ge 0}} \\bigl\\{ pa + H_t(s + a) \\bigr\\}, \\\\\n  π_t(s) &= \\arg \\min_{a \\in \\S_{\\ge 0}} \\bigl\\{ pa + H_t(s + a) \\bigr\\}.\n\\end{align*} \\]\nNote that the problem at each step is similar to the newsvendor problem. So, similar to that model, we try to see if we can establish qualitative properties of the optimal solution.\nTo fix ideas, let’s solve this dynamic program for a specific instance. We consider \\(p = 5\\), \\(c_h = 4\\), \\(c_s = 2\\), and assume that the demand is distributed according to a Binomial(10,0.4) distribution, as shown in Figure 1.\n\n\n\n\n\n\n\nFigure 1 Demand distribution\n\n\nWe consider a horizon \\(T = 15\\), and solve the dynamic program shown above. The optimal value function and policy are shown below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue function\n\n\nOptimal policy\n\n\n\n\nFigure 2 The value function and the optimal policy for the example shown above\n\n\nThe plots above suggest that the optimal policy has a structure. Play around with the value of the purchase cost to see if that structure is retained.\n\n\n\n\nWe will now see how to prove the structure of optimal policy."
  },
  {
    "objectID": "mdps/inventory-management.html#structure-of-optimal-solution",
    "href": "mdps/inventory-management.html#structure-of-optimal-solution",
    "title": "6  Inventory Management",
    "section": "6.2 Structure of optimal solution",
    "text": "6.2 Structure of optimal solution\nFor ease of exposition, we assume that the state space \\(\\S\\) is equal to \\(\\reals\\) (instead of \\(\\integers\\)). See exercise 1 at the end to extend the argument to \\(\\integers\\).\nFor this setting, the optimal policy is then characterized as follows.\n\nTheorem 6.1 Define \\[ s^*_t = \\arg \\min_{z \\in \\reals} \\bigl\\{ p z + H_t(z) \\bigr\\} . \\] Then, \\[\\begin{equation} \\label{eq:V}\nV_t(s) = \\begin{cases}\n  H_t(s_t) + p (s_t - s), &\\text{if } s \\le s^*_t \\\\\n  H_t(s)   , & \\text{otherwise }\n\\end{cases}\n\\end{equation}\\] and \\[\\begin{equation}\\label{eq:pi}\n  π_t(s) = \\begin{cases}\n  s^*_t - s, &\\text{if } s \\le s^*_t \\\\\n  0, & \\text{otherwise }\n\\end{cases}\\end{equation}\\]\nFurthermore, for all \\(t\\), \\(H_t(z)\\) and \\(V_t(s)\\) are convex in \\(z\\) and \\(s\\), respectively.\n\n\n\n\n\n\n\nBase-stock policy\n\n\n\n\n\nThe optimal policy given by \\eqref{eq:pi} is called a base-stock policy. It states that there is a base-stock level \\(\\{s^*_t\\}_{t \\ge 1}\\) for every time step. If, at the beginning of time \\(t\\), the value of the current stock is below the base stock level \\(s^*_t\\), then the optimal decision is to order more goods so that the level of the stock equals the base stock level.\n\n\n\nWe first establish some preliminary results.\n\n\nFor any convex function \\(f \\colon \\reals \\to \\reals\\), \\(F(s) = \\EXP[ f(s - W) ]\\) is convex.\nProof For any realization of \\(W\\), \\(f(s - w)\\) is convex in \\(s\\). The expectation w.r.t. \\(W\\) is simply the sum of convex functions and is, therefore, convex.\n\n\n\n\n\nFigure 3 An example showing that the average of convex functions is convex.\n\n\nFor any convex function \\(f \\colon \\reals \\to \\reals\\), let \\(s^* = \\arg \\min_{s \\in \\reals} f(s)\\). Then, \\[\\arg \\min_{a \\in \\reals_{\\ge 0}} f(s + a) = \\begin{cases}\n0, & \\text{if } s > s^*, \\\\\ns^* - s, & \\text{if } s \\le s^*\n\\end{cases}\\] and \\[F(s) = \\min_{a \\in \\reals_{\\ge 0}} f(s+a) = \\begin{cases}\nf(s), & \\text{if } s > s^* \\\\\nf(s^*), & \\text{if } s \\le s^*\n\\end{cases}\\] and \\(F(s)\\) is convex in \\(s\\).\nWe first see an illustration of \\(F(s) = \\min\\{ f(s), f(s+1), f(s+2) \\}\\). Note that the resulting function is not convex because \\(a\\) takes only discrete values. But the plot shows that the minimum will look like when we allow \\(a\\) to take continuous values.\n\n\n\n\n\nFigure 4 An example showing that the minimum of \\(f(s)\\), \\(f(s+1)\\), \\(f(s+2)\\).\n\n\nIf there were no constraint on \\(a\\), then the minimizer will be \\(a = s^* -  s\\). If \\(s \\le s^*\\), then \\(a = s^* -s \\in \\reals_{\\ge 0}\\) is the minimizer for the constrained problem as well. On the other hand, if \\(s \\ge s^*\\), then the function \\(f(s + a)\\) is increasing as a function of \\(a\\). Hence, the minimizer for the constrained problem is \\(a = 0\\). See the figures below.\n\n\n\n\n\n\n\nProof of the structural result\n\n\n\n\n\nNow to prove the result, we define \\[ f_t(z) = py + H_t(z). \\] Then, \\[ V_t(s) = \\min_{a \\in \\reals_{\\ge 0}} \\bigl\\{ p(s + a) + H_t(s + a)\n\\bigr\\} - p s\n= \\min_{a \\in \\reals_{\\ge 0}} f_t(s+a) - p s.\n\\] As usual, we prove the result by backward induction. For \\(t=T\\), \\[\\bar Q_T(z) = \\EXP[ h(z - W) ] \\] which is convex because \\(h(z)\\) is convex. \\(f_T(z) = p z + Q_T(z)\\) is the sum of a linear function and convex function and is, therefore, convex. Then, by fact 2 above, \\[π_T(s) = \\arg \\min_{a \\in \\reals_{\\ge 0}} f_T(s+a) = \\max(s^*_T - s, 0)\n\\] and \\[V_T(s) = \\min_{a \\in \\reals_{\\ge 0}} f_T(s + a) - px =\n  \\begin{cases}\n    f_T(s) - p s, & \\text{if } s > s^*_T \\\\\n    f_T(s^*_T) - px, & \\text{if } s \\le s^*_T.\n  \\end{cases}\n\\] Substituting \\(f_t(z) = p z + H_t(z)\\), we get that both \\(V_T\\) and \\(π_T\\) have the desired form and \\(V_T\\) is convex. This forms the basis of induction.\nNow assume that \\(V_{t+1}(s)\\) is convex and of the form \\eqref{eq:V}. Now note that, by fact 1, \\[ H_t(z) = \\EXP[ h(z - W) + V_{t+1}(z - W) ]\\] is convex. Hence, \\(f_t(z)\\) is convex. Therefore, by fact 2 above, \\[ π_t(s) = \\max(s^*_t - s, 0)\\] and \\(V_t(s)\\) is of the desired form and convex.\nThus, the result is holds by induction."
  },
  {
    "objectID": "mdps/inventory-management.html#exercises",
    "href": "mdps/inventory-management.html#exercises",
    "title": "6  Inventory Management",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 6.1 Consider the setting when \\(\\S = \\integers\\). Show that there exists a sequence \\(\\{s_t\\}_{t \\ge 1}\\) of numbres such that policy given by \\[ π_t(s) = \\begin{cases}\n   n, & \\text{if } s_t - n \\le s \\le s_t - n + 1, \\\\\n   0, & \\text{if } s_t \\ge s_t.\n  \\end{cases} \\] is optimal."
  },
  {
    "objectID": "mdps/inventory-management.html#notes",
    "href": "mdps/inventory-management.html#notes",
    "title": "6  Inventory Management",
    "section": "Notes",
    "text": "Notes\nInventory management models with deterministic demand were introduced by Harris (1913). The mathematical model of inventory management considered here was originally proposed by Arrow et al. (1952). The optimality of base-stock policy was established by Bellman et al. (1955). See the notes on infinite horizon version of this model to see how to find the threshold in closed form.\nThe problem for Exercise 1 is from Veinott (1965). See Tsitsiklis (1984) for a partial characterization of the optimal policy with non-zero ordering costs.\n\n\n\n\nArrow, K.J., Harris, T., and Marschak, J. 1952. Optimal inventory policy. Econometrica 20, 1, 250–272. DOI: 10.2307/1907830.\n\n\nBellman, R., Glicksberg, I., and Gross, O. 1955. On the optimal inventory equation. Management Science 2, 1, 83–104. DOI: 10.1287/mnsc.2.1.83.\n\n\nHarris, F.W. 1913. How many parts to make at once. The magazine of management 10, 2, 135–152. DOI: 10.1287/opre.38.6.947.\n\n\nTsitsiklis, J.N. 1984. Periodic review inventory systems with continuous demand and discrete order sizes. Management Science 30, 10, 1250–1254. DOI: 10.1287/mnsc.30.10.1250.\n\n\nVeinott, A.F. 1965. The optimal inventory policy for batch ordering. Operations Research 13, 3, 424–432. DOI: 10.1287/opre.13.3.424."
  },
  {
    "objectID": "mdps/monotone-mdps.html#stochastic-dominance",
    "href": "mdps/monotone-mdps.html#stochastic-dominance",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "7.1 Stochastic dominance",
    "text": "7.1 Stochastic dominance\n Let \\(\\ALPHABET S\\) be a totally ordered finite set, say \\(\\{1, \\dots, n\\}\\).Stochastic dominance is a partial order on random variables defined on totally ordered sets\n\n\n\n\n\n\n(First order) stochastic dominance\n\n\n\nSuppose \\(S^1\\) and \\(S^2\\) are \\(\\ALPHABET S\\) valued random variables where \\(S^1 \\sim \\mu^1\\) and \\(S^2 \\sim \\mu^2\\). We say \\(S^1\\) stochastically dominates \\(S^2\\) if for any \\(s \\in \\ALPHABET S\\), \\[\\begin{equation}\\label{eq:inc-prob}\n  \\PR(S^1 \\ge s) \\ge \\PR(S^2 \\ge s).\n\\end{equation}\\]\nStochastic domination is denoted by \\(S^1 \\succeq_s S^2\\) or \\(\\mu^1 \\succeq_s \\mu^2\\).\n\n\nLet \\({\\rm M}^1\\) and \\({\\rm M}^2\\) denote the CDF of \\(\\mu^1\\) and \\(\\mu^2\\). Then \\eqref{eq:inc-prob} is equivalent to the following: \\[\\begin{equation}\\label{eq:cdf}\n  {\\rm M}^1_s \\le {\\rm M}^2_s, \\quad \\forall s \\in \\ALPHABET S.\n\\end{equation}\\] Thus, visually, \\(S^1 \\succeq_s S^2\\) means that the CDF of \\(S^1\\) lies below the CDF of \\(S^2\\).\n\n\n\n\n\n\nExample\n\n\n\n\\(\\left[0, \\frac 14, \\frac 14, \\frac 12\\right] \\succeq_s \\left[\\frac 14, 0, \\frac 14, \\frac 12 \\right] \\succeq_s \\left[\\frac 14, \\frac 14, \\frac 14, \\frac 14 \\right].\\)\n\n\nStochastic dominance is important due to the following property.\n\nTheorem 7.1 Let \\(f \\colon \\ALPHABET S \\to \\reals\\) be a (weakly) increasing function and \\(S^1 \\sim \\mu^1\\) and \\(S^2 \\sim \\mu^2\\) are random variables defined on \\(\\ALPHABET S\\). Then \\(S^1 \\succeq_s S^2\\) if and only if \\[\\begin{equation}\\label{eq:inc-fun}\n  \\EXP[f(S^1)] \\ge \\EXP[f(S^2)].\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof (stochastic dominance implies monotone expectations)\n\n\n\n\n\nFor the ease of notation, let \\(f_i\\) to denote \\(f(i)\\) and define \\({\\rm M}^1_0 = {\\rm M}^2_0 = 0\\). Consider the following: \\[\\begin{align*}\n    \\sum_{i=1}^n f_i \\mu^1_i\n    &= \\sum_{i=1}^n f_i ({\\rm M}^1_i - {\\rm M}^1_{i-1})\n    \\\\\n    &= \\sum_{i=1}^n {\\rm M}^1_{i-1} (f_{i-1} - f_{i}) + f_n {\\rm M}^1_n\n    \\\\\n    &\\stackrel{(a)}{\\ge}\n    \\sum_{i=1}^n {\\rm M}^2_{i-1} (f_{i-1} - f_{i}) + f_n {\\rm M}^2_n\n    \\\\\n    &= \\sum_{i=1}^n f_i ({\\rm M}^2_i - {\\rm M}^2_{i-1})\n    \\\\\n    &= \\sum_{i=1}^n f_i \\mu_i,\n\\end{align*}\\] which completes the proof. In the above equations, \\((a)\\) uses the following facts:\n\nFor any \\(i\\), \\({\\rm M}^1_{i-1} \\le {\\rm M}^2_{i-1}\\) (because of \\eqref{eq:cdf}) and \\(f_{i-1} - f_{i} < 0\\) (because \\(f\\) is increasing function). Thus, \\[{\\rm M}^1_{i-1}(f_{i-1} - f_i) \\ge {\\rm M}^2_{i-1}(f_{i-1} - f_i). \\]\n\\({\\rm M}^1_n = {\\rm M}^2_n = 1\\).\n\n\n\n\n\n\n\n\n\n\nProof (monotone expectations implies stochastic monotonicity)\n\n\n\n\n\nSuppose for any increasing function \\(f\\), \\eqref{eq:inc-fun} holds. Given any \\(i \\in \\{1, \\dots, n\\}\\), define the function \\(f_i(k) = \\IND\\{k > i\\}\\), which is an increasing function of \\(k\\). Then, \\[ \\EXP[f_i(S)] = \\sum_{k=1}^n f_i(k) \\mu^1_k = \\sum_{k > i} \\mu^1_k = 1 - {\\rm M}^1_i.\n\\] By a similar argument, we have \\[ \\EXP[f_i(S^2)] = 1 - {\\rm M}^2_i. \\] Since \\(\\EXP[f_i(S)] \\ge \\EXP[f_i(S^2)]\\), we have that \\({\\rm M}^1_i \\le {\\rm M}^2_i\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#stochastic-monotonicity",
    "href": "mdps/monotone-mdps.html#stochastic-monotonicity",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "7.2 Stochastic monotonicity",
    "text": "7.2 Stochastic monotonicity\nStochastic monotonicity extends the notion of stochastic dominance to Markov chains. Suppose \\(\\ALPHABET S\\) is a totally ordered set and \\(\\{S_t\\}_{t \\ge 1}\\) is a time-homogeneous Markov chain on \\(\\ALPHABET S\\) with transition probability matrix \\(P\\). Let \\(P_i\\) denote the \\(i\\)-th row of \\(P\\). Note that \\(P_i\\) is a PMF.\n\n\n\n\n\n\nStochastic monotonicity\n\n\n\nA Markov chain with transition matrix \\(P\\) is stochastically monotone if \\[ P_i \\succeq_s P_j, \\quad \\forall i > j. \\]\n\n\nAn immediate implication is the following.\n\nTheorem 7.2 Let \\(\\{S_t\\}_{t \\ge 1}\\) be a Markov chain with transition matrix \\(P\\) and \\(f \\colon \\ALPHABET S \\to \\reals\\) is a weakly increasing function. Then, for any \\(s^1, s^2 \\in \\ALPHABET S\\) such that \\(s^1 > s^2\\), \\[ \\EXP[f(S_{t+1}) | S_t = s^1] \\ge \\EXP[ f(S_{t+1}) | S_t = s^2], \\] if and only if \\(P\\) is stochatically monotone."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotonicity-of-value-functions",
    "href": "mdps/monotone-mdps.html#monotonicity-of-value-functions",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "7.3 Monotonicity of value functions",
    "text": "7.3 Monotonicity of value functions\n\nTheorem 7.3 Consider an MDP where the state space \\(\\ALPHABET S\\) is totally ordered. Suppose the following conditions are satisfied.\nC1. For every \\(a \\in \\ALPHABET A\\), the per-step cost \\(c_t(s,a)\\) is weakly inceasing in \\(s\\).\nC2. For every \\(a \\in \\ALPHABET A\\), the transition matrix \\(P(a)\\) is stochastically monotone.\nThen, the value function \\(V_t(s)\\) is weakly increasing in \\(s\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe result above also applies to models with continuous (and totally ordered) state space provided the measurable selection conditions hold so that the arg min at each step of the dynamic program is attained.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. By definition, \\(V_{T+1}(s) = 0\\), which is weakly increasing. This forms the basis of induction. Assume that \\(V_{t+1}(s)\\) is weakly increasing. Now consider, \\[Q_t(s,a) = c_t(s,a) + \\EXP[V_{t+1}(S_{t+1}) | S_t = s, A_t = a].\\] For any \\(a \\in \\ALPHABET A\\), \\(Q_t(s,a)\\) is a sum of two weakly increasing functions in \\(s\\); hence \\(Q_t(s,a)\\) is weakly increasing in \\(s\\).\nNow consider \\(s_1, s_2 \\in \\ALPHABET S\\) such that \\(s_1 > s_2\\). Suppose \\(a_1^*\\) is the optimal action at state \\(s_1\\). Then \\[\n  V_t(s^1) = Q_t(s^1, a_1^*) \\stackrel{(a)}\\ge Q_t(s^2,a_1^*) \\stackrel{(b)}\\ge V_t(s_2),\n\\] where \\((a)\\) follows because \\(Q_t(\\cdot, u^*)\\) is weakly increasing and \\((b)\\) follows from the definition of the value function."
  },
  {
    "objectID": "mdps/monotone-mdps.html#submodularity",
    "href": "mdps/monotone-mdps.html#submodularity",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "7.4 Submodularity",
    "text": "7.4 Submodularity\n\n\n\n\n\n\nSubmodularity\n\n\n\nLet \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be partially ordered sets. A function \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) is called submodular if for any \\(x^+ \\ge x^-\\) and \\(y^+ \\ge y^-\\), we have \\[\\begin{equation}\\label{eq:submodular}\n  f(x^+, y^+) + f(x^-, y^-) \\le f(x^+, y^-) + f(x^-, y^+).\n\\end{equation}\\]\nThe function is called supermodular if the inequality in \\eqref{eq:submodular} is reversed.\n\n\nA continuous and differentiable function on \\(\\reals^2\\) is submodular iff \\[ \\frac{ \\partial^2 f(x,y) }{ \\partial x \\partial y } \\le 0,\n  \\quad \\forall x,y.\n\\] If the inequality is reversed, then the function is supermodular.\nSubmodularity is a useful property because it implies monotonicity of the arg min.\n\nTheorem 7.4 Let \\(\\ALPHABET X\\) be a partially ordered set, \\(\\ALPHABET Y\\) be a totally ordered set, and \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) be a submodular function. Suppose that for all \\(x\\), \\(\\arg \\min_{y \\in \\ALPHABET Y} f(x,y)\\) exists. Then, \\[\n  π(x) := \\max \\{ y^* \\in \\arg \\min_{y \\in \\ALPHABET Y} f(x,y) \\}\n\\] is weakly increasing in \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(x^+, x^- \\in \\ALPHABET X\\) such that \\(x^+ \\ge x^-\\). Since \\(f\\) is submodular, for any \\(y \\le π(x^-)\\), we have \\[\\begin{equation}\\label{eq:1}\n  f(x^+, π(x^-)) - f(x^+, y) \\le f(x^-, π(x^-)) - f(x^-, y) \\le 0,\n\\end{equation}\\] where the last inequality follows because \\(π(x^-)\\) is the arg min of \\(f(x^-, y)\\). Eq. \\eqref{eq:1} implies that for all \\(y \\le π(x^-)\\), \\[\n  f(x^+, π(x^-)) \\le f(x^+, y).\n\\] Thus, \\(π(x^+) \\ge π(x^-)\\).\n\n\n\nThe analogue of Theorem 7.4 for supermodular functions is as follows.\n\nTheorem 7.5 Let \\(\\ALPHABET X\\) be a partially ordered set, \\(\\ALPHABET Y\\) be a totally ordered set, and \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) be a supermodular function. Suppose that for all \\(x\\), \\(\\arg \\min_{y \\in \\ALPHABET Y} f(x,y)\\) exists. Then, \\[\n  π(x) := \\min \\{ y^* \\in \\arg \\min_{y \\in \\ALPHABET Y} f(x,y) \\}\n\\] is weakly decreasing in \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is similar to Theorem 7.4.\nConsider \\(x^+, x^- \\in \\ALPHABET X\\) such that \\(x^+ \\ge x^-\\). Since \\(f\\) is supermodular, for any \\(y \\ge π(x^-)\\), we have \\[\\begin{equation}\\label{eq:2}\n  f(x^+, y) - f(x^+, π(x^-)) \\ge f(x^-, y) - f(x^-, π(x^-)) \\ge 0,\n\\end{equation}\\] where the last inequality follows because \\(π(x^-)\\) is the arg min of \\(f(x^-, y)\\). Eq. \\eqref{eq:2} implies that for all \\(y \\ge π(x^-)\\), \\[\n  f(x^+, y) \\ge f(x^+, π(x^-)).\n\\] Thus, \\(π(x^+) \\le π(x^-)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotonicity-of-optimal-policy",
    "href": "mdps/monotone-mdps.html#monotonicity-of-optimal-policy",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "7.5 Monotonicity of optimal policy",
    "text": "7.5 Monotonicity of optimal policy\n\nTheorem 7.6 Consider an MDP where the state space \\(\\ALPHABET S\\) and the action space \\(\\ALPHABET A\\) are totally ordered. Suppose that, in addition to (C1) and (C2), the following condition is satisfied.\nC3. For any weakly increasing function \\(v\\), \\[ c_t(s,a) + \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\] is submodular in \\((s,a)\\).\nLet \\(π^*_t(s) = \\max\\{ a^* \\in \\arg \\min_{a \\in \\ALPHABET A} Q_t(s,a) \\}\\). Then, \\(π^*(s)\\) is weakly increasing in \\(s\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConditions (C1) and (C2) imply that the value function \\(V_{t+1}(s)\\) is weakly increasing. Therefore, condition (C3) implies that \\(Q_t(s,a)\\) is submodular in \\((s,a)\\). Therefore, the arg min is weakly increasing in \\(x\\)\n\n\n\nIt is difficult to verify condition (C3). The following conditions are sufficient for (C3).\n\nLemma 7.1 Consider an MDP with totally ordered state and action spaces. Suppose\n\n\\(c_t(s,a)\\) is submodular in \\((s,a)\\).\nFor all \\(s' \\in \\ALPHABET S\\), \\(H(s' | s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a)\\) is submodular in \\((s,a)\\).\n\nThe condition (C3) of the previous theorem holds.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s^+, s^- \\in \\ALPHABET S\\) and \\(a^+, a^- \\in \\ALPHABET A\\) such that \\(s^+ > s^-\\) and \\(a^+ > a^-\\). Define\n\\[\\begin{align*}\n  μ_1(s) &= \\tfrac 12 P_{s^- s}(a^-) + \\tfrac 12 P_{s^+ s}(a^+), \\\\\n  μ_2(s) &= \\tfrac 12 P_{s^- s}(a^+) + \\tfrac 12 P_{s^+ s}(a^-).\n\\end{align*}\\] Since \\(H(s' | s,a)\\) is submodular, we have \\[ H(s' | s^+, a^+) + H(s' | s^-, a^-) \\le H(s' | s^+, a^-) + H(s' | s^-, a^+) \\] or equivalently, \\[\\sum_{z \\le s'} \\big[ P_{s^+ z}(a^+) + P_{s^- z}(a^-) \\big]\n  \\ge\n  \\sum_{z \\le s'} \\big[ P_{s^+ z}(a^-) + P_{s^- z}(a^+) \\big]. \\] which implies \\[ M_1(s') \\ge M_2(s')\\] where \\(M_1\\) and \\(M_2\\) are the CDFs of \\(μ_1\\) and \\(μ_2\\). Thus, \\(μ_1 \\preceq_s μ_2\\).\nHence, for any weakly increasing function \\(v \\colon \\ALPHABET S \\to \\reals\\), \\[ \\sum_{s' \\in \\ALPHABET S} μ_1(s') v(s') \\le\n   \\sum_{s' \\in \\ALPHABET S} μ_2(s') v(s').\\] Or, equivalently, \\[H(s^+, a^+) + H(s^-, a^-) \\le H(s^-, a^+) + H(s^+, a^-)\\] where \\(H(s,a) = \\EXP[ v(X_{t+1}) | X_t = s, U_t = a]\\).\nTherefore, \\(c_t(s,a) + H_t(s,a)\\) is submodular in \\((s,a)\\).\n\n\n\nThe analogue of Theorem 7.6 for supermodular functions is as follows.\n\nTheorem 7.7 Consider an MDP where the state space \\(\\ALPHABET S\\) and the action space \\(\\ALPHABET A\\) are totally ordered. Suppose that, in addition to (C1) and (C2), the following condition is satisfied.\nC4. For any weakly increasing function \\(v\\), \\[ c_t(s,a) + \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\] is supermodular in \\((s,a)\\).\nLet \\(π^*_t(s) = \\min\\{ a^* \\in \\arg \\min_{a \\in \\ALPHABET S} Q_t(s,a) \\}\\). Then, \\(π^*(s)\\) is weakly decreasing in \\(s\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConditions (C1) and (C2) imply that the value function \\(V_{t+1}(s)\\) is weakly increasing. Therefore, condition (C4) implies that \\(Q_t(s,a)\\) is supermodular in \\((s,a)\\). Therefore, the arg min is decreasing in \\(s\\)\n\n\n\nIt is difficult to verify condition (C4). The following conditions are sufficient for (C4).\n\nLemma 7.2 Consider an MDP with totally ordered state and action spaces. Suppose\n\n\\(c_t(s,a)\\) is supermodular in \\((s,a)\\).\nFor all \\(s' \\in \\ALPHABET S\\), \\(H(s' | s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a)\\) is supermodular in \\((s,a)\\).\n\nThe condition (C4) of the previous theorem holds.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s^+, s^- \\in \\ALPHABET S\\) and \\(a^+, a^- \\in \\ALPHABET A\\) such that \\(s^+ > s^-\\) and \\(a^+ > a^-\\). Define\n\\[\\begin{align*}\n  μ_1(s) &= \\tfrac 12 P_{s^- s}(a^-) + \\tfrac 12 P_{s^+ s}(a^+), \\\\\n  μ_2(s) &= \\tfrac 12 P_{s^- s}(a^+) + \\tfrac 12 P_{s^+ s}(a^-).\n\\end{align*}\\] Since \\(H(s' | s,a)\\) is supermodular, we have \\[ H(s' | s^+, a^+) + H(s' | s^-, a^-) \\ge H(s' | s^+, a^-) + H(s' | s^-, a^+) \\] or equivalently, \\[\\sum_{s' \\le s'} \\big[ P_{s^+ s'}(a^+) + P_{s^- s'}(a^-) \\big]\n  \\le\n  \\sum_{s' \\le s'} \\big[ P_{s^+ s'}(a^-) + P_{s^- s'}(a^+) \\big]. \\] which implies \\[ M_1(s') \\le M_2(s')\\] where \\(M_1\\) and \\(M_2\\) are the CDFs of \\(μ_1\\) and \\(μ_2\\). Thus, \\(μ_1 \\succeq_s μ_2\\).\nHence, for any weakly increasing function \\(v \\colon \\ALPHABET S \\to \\reals\\), \\[ \\sum_{s' \\in \\ALPHABET S} μ_1(s') v(s') \\ge\n   \\sum_{s' \\in \\ALPHABET S} μ_2(s') v(s').\\] Or, equivalently, \\[H(s^+, a^+) + H(s^-, a^-) \\ge H(s^-, a^+) + H(s^+, a^-)\\] where \\(H(s,a) = \\EXP[ v(X_{t+1}) | X_t = s, U_t = a]\\).\nTherefore, \\(c_t(s,a) + H_t(s,a)\\) is supermodular in \\((s,a)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#constraints-on-actions",
    "href": "mdps/monotone-mdps.html#constraints-on-actions",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "7.6 Constraints on actions",
    "text": "7.6 Constraints on actions\nIn the results above, we have assumed that the action set \\(\\ALPHABET A\\) is the same for all states. The results also extend to the case when the action at state \\(s\\) must belong to some set \\(\\ALPHABET A(s)\\) provided the following conditions are satisfied:\n\nFor any \\(s \\ge s'\\), \\(\\ALPHABET A(s) \\supseteq \\ALPHABET A(s')\\)\nFor any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A(s)\\), \\(a' < a\\) implies that \\(a' \\in \\ALPHABET A(s)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotone-dynamic-programming",
    "href": "mdps/monotone-mdps.html#monotone-dynamic-programming",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "7.7 Monotone dynamic programming",
    "text": "7.7 Monotone dynamic programming\nIf we can establish that the optimal policy is monontone, then we can use this structure to implement the dynamic program more efficient. Suppose \\(\\ALPHABET S = \\{1, \\dots, n\\}\\) and \\(\\ALPHABET A = \\{1, \\dots. m\\}\\). The main idea is as follows. Suppose \\(V_{t+1}(\\cdot)\\) has been caclulated. Insead of computing \\(Q_t(s,a)\\) and \\(V_t(s)\\), proceed as follows:\n\nSet \\(s = 1\\) and \\(α = 1\\).\nFor all \\(u \\in \\{α, \\dots, m\\}\\), compute \\(Q_t(s,a)\\) as usual.\nCompute\n\\[V_t(s) = \\min_{ α \\le a \\le m } Q_t(s,a)\\]\nand set\n\\[π_t^*(s) = \\max \\{ a \\in \\{α, \\dots, m\\} : V_t(s) = Q_t(s,a) \\}.\\]\nIf \\(s = n\\), then stop. Otherwise, set \\(α = π_t^*(s)\\) and \\(s = s+1\\) and go to step 2."
  },
  {
    "objectID": "mdps/monotone-mdps.html#example-a-machine-replacement-model",
    "href": "mdps/monotone-mdps.html#example-a-machine-replacement-model",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "7.8 Example: A machine replacement model",
    "text": "7.8 Example: A machine replacement model\nLet’s revisit the machine replacement problem presented at the beginning of this section. For simplicity, we’ll assume that \\(n = ∞\\), i.e., the state space is countable. In this case, the transition matrices are given by \\[ P_{sz}(0) = \\begin{cases}\n  0, & z < s \\\\\n  μ_{z - s}, & z \\ge s\n\\end{cases}\n\\quad\\text{and}\\quad\nP_sz(1) = μ_z.\n\\] where \\(μ\\) is the PMF of \\(W\\).\n\nProposition 7.1 For the machine replacement problem, there exist a series of thresholds \\(\\{s^*_t\\}_{t = 1}^T\\) such that the optimal policy at time \\(t\\) is a threshold policy with threshold \\(s_t\\), i.e., \\[\n  π_t(s) = \\begin{cases}\n  0 & \\text{if $s < s_t^*$} \\\\\n  1 & \\text{otherwise}\n\\end{cases}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by verifying conditions (C1)–(C4) to establish that the optimal policy is monotone.\nC1. For \\(a = 0\\), \\(c(s,0) = h(s)\\), which is weakly increasing by assumption. For \\(a = 1\\), \\(c(s,1) = K\\), which is trivially weakly increasing.\nC2. For \\(a = 0\\), \\(P(0)\\) is stochastically monotone (because the CDF of \\(P(\\cdot | s, 0)\\) lies above the CDF of \\(P(\\cdot | s+1, 0)\\)). For \\(a = 1\\), all rows of \\(P(1)\\) are the same; therefore \\(P(1)\\) is stochastically monotone.\nSince (C1) and (C2) are satisfied, by Theorem 7.3, we can assert that the value function is weakly increasing.\nC3. \\(c(s,1) - c(s,0) = K - h(s)\\), which is weakly decreasing in \\(s\\). Therefore, \\(c(s,a)\\) is submodular in \\((s,a)\\).\nC4. Recall that \\(H(s'|s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a).\\) Therefore,\n\\[H(s'|s,0) = 1 - \\sum_{z = s}^{s'} μ_{z -s} = 1 - \\sum_{k = 0}^{s' - s} μ_k\n= 1 - M_{s' - s},\\] where \\(M\\) is the CMF of \\(μ\\), and \\[H(s'|s,1) = 1 - \\sum_{z \\le s'} μ_z = 1 - M_{s'},\\]\nTherefore, \\(H(s'|s,1) - H(s'|s,0) = M_{s'-s} - M_{s'}\\). For any fixed \\(s'\\), \\(H(s'|s,1) - H(s'|s,0)\\) is weakly decreasing in \\(s\\). There \\(H(s'|s,a)\\) is submodular in \\((s,a)\\).\nSince (C1)–(C4) are satisfied, the optimal policy is weakly increasing in~\\(s\\). Since there are only two actions, it means that for every time, there exists a state \\(s^*_t\\) with the property that if \\(s\\) exceeds \\(s^*_t\\), the optimal decision is to replace the machine; and if \\(s \\le s^*_t\\), then the optimal decision is to operate the machine for another period."
  },
  {
    "objectID": "mdps/monotone-mdps.html#exercises",
    "href": "mdps/monotone-mdps.html#exercises",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 7.1 Let \\(T\\) denote a upper triangular matrix with 1’s on or below the diagonal and 0’s above the diagonal. Then \\[ T^{-1}_{ij} = \\begin{cases}\n  1, & \\text{if } i = j, \\\\\n-1, & \\text{if } i = j + 1, \\\\\n  0, & \\text{otherwise}.\n\\end{cases}\\]\nFor example, for a \\(4 \\times 4\\) matrix \\[\n  T = \\MATRIX{1 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 1},\n  \\quad\n  T^{-1} = \\MATRIX{1 & -1 & 0 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 1 & -1 \\\\\n  0 & 0 & 0 & 1 }.\n\\]\nShow the following:\n\nFor any two PMFs \\(μ^1\\) and \\(μ^2\\), \\(\\mu^1 \\succeq_s \\mu^2\\) iff \\(\\mu^1 T \\ge \\mu^2 T\\).\nA Markov transition matrix \\(P\\) is stochastic monotone iff \\(T^{-1} P T \\ge 0\\).\n\n\n\nExercise 7.2 Show that the following are equivalent:\n\nA transition matrix \\(P\\) is stochastically monotone\nFor any two PMFs \\(μ^1\\) and \\(μ^2\\), if \\(\\mu^1 \\succeq_s \\mu^2\\) then \\(\\mu^1P \\succeq_s \\mu^2P\\).\n\n\n\nExercise 7.3 Show that if two transition matrices \\(P\\) and \\(Q\\) have the same dimensions and are stochastically monotone, then so are:\n\n\\(\\lambda P + (1 - \\lambda) Q\\), where \\(\\lambda \\in (0,1)\\).\n\\(P Q\\)\n\\(P^k\\), for \\(k \\in \\integers_{> 0}\\).\n\n\n\nExercise 7.4 Let \\(\\mu_t\\) denote the distribution of a Markov chain at time \\(t\\). Suppose \\(\\mu_0 \\succeq_s \\mu_1\\). Then \\(\\mu_t \\succeq_s \\mu_{t+1}\\).\n\n\n\nExercise 7.5 Consider the example of machine repair presented in notes on matrix formulation of MDPs. Prove that the optimal policy for that model is weakly increasing.\n\n\nExercise 7.6 Suppose the state space \\(\\ALPHABET S\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L-1, L\\}\\) and the action space \\(\\ALPHABET A\\) is discrete. Let \\(\\ALPHABET X_{\\ge 0}\\) denote the set \\(\\{0, \\dots, L\\}\\).\nLet \\(P(a)\\) denote the controlled transition matrix and \\(c_t(s,a)\\) denote the per-step cost. To avoid ambiguity, we define the optimal policy as \\[\nπ^*_t(s) = \\begin{cases}\n    \\max\\bigl\\{ a' \\in \\arg\\min_{a \\in \\ALPHABET A} Q_t(s,a) \\bigr\\},\n    & \\text{if } s \\ge 0 \\\\\n    \\min\\bigl\\{ a' \\in \\arg\\min_{a \\in \\ALPHABET A} Q_t(s,a) \\bigr\\},\n    & \\text{if } s < 0\n\\end{cases}\\] The purpose of this exercise is to identify conditions under which the value function and the optimal policy are even and :quasi-convex. We do so using the following steps.\n\nWe say that the transition probability matrix \\(P(a)\\) is even if for all \\(s, s' \\in \\ALPHABET S\\), \\(P(s'|s,a) = P(-s'|-s,a)\\). Prove the following result.\n\n\nProposition 7.2 Suppose the MDP satisfies the following properties:\n(A1) For every \\(t\\) and \\(a \\in \\ALPHABET A\\), \\(c_t(s,a)\\) is even function of \\(s\\).\n(A2) For every \\(a \\in \\ALPHABET A\\), \\(P(a)\\) is even.\nThen, for all \\(t\\), \\(V_t\\) and \\(π_t\\) are even functions.\n\n\nGiven any probability mass function \\(μ\\) on \\(\\ALPHABET S\\), define the folded probability mass function \\(\\tilde μ\\) on \\(\\ALPHABET X_{\\ge 0}\\) as follows: \\[ \\tilde μ(s) = \\begin{cases}\n   μ(0), & \\text{if } s = 0 \\\\\n   μ(s) + μ(-s), & \\text{if } s > 0.\n\\end{cases} \\]\n\nFor ease of notation, we use \\(\\tilde μ = \\mathcal F μ\\) to denote this folding operation. Note that an immediate consequence of the definition is the following (you don’t have to prove this).\n\nLemma 7.3 If \\(f \\colon \\ALPHABET S \\to \\reals\\) is even, then for any probability mass function \\(μ\\) on \\(\\ALPHABET S\\) and \\(\\tilde μ = \\mathcal F μ\\), we have \\[\n  \\sum_{s \\in \\ALPHABET S} f(s) μ(s) =\n  \\sum_{s \\in \\ALPHABET X_{\\ge 0}} f(s) \\tilde μ(s). \\]\n\nThus, the expectation of the function \\(f \\colon \\ALPHABET S \\to \\reals\\) with respect to the PMF \\(μ\\) is equal to the expectation of the function \\(f \\colon \\ALPHABET X_{\\ge 0} \\to \\reals\\) with respect to the PMF \\(\\tilde μ = \\mathcal F μ\\).\nNow given any probability transition matrix \\(P\\) on \\(\\ALPHABET S\\), we can define a probability transition matrix \\(\\tilde P\\) on \\(\\ALPHABET X_{\\ge 0}\\) as follows: for any \\(s \\in \\ALPHABET S\\), \\(\\tilde P_s = \\mathcal F P_s\\), where \\(P_s\\) denotes the \\(s\\)-th row of \\(P\\). For ease of notation, we use \\(\\tilde P = \\mathcal F P\\) to denote this relationship.\nNow prove the following:\n\nProposition 7.3 Given the MDP \\((\\ALPHABET S, \\ALPHABET A, P, \\{c_t\\})\\), define the folded MDP as \\((\\ALPHABET S_{\\ge 0}, \\ALPHABET A, \\tilde P, \\{c_t\\})\\), where \\(\\tilde P(a) = \\mathcal F P(a)\\) for all \\(a \\in \\ALPHABET A\\). Let \\(\\tilde Q_t \\colon \\ALPHABET S_{\\ge 0} \\times \\ALPHABET A \\to \\reals\\), \\(\\tilde V_t \\colon \\ALPHABET S_{\\ge 0} \\to \\reals\\) and \\(\\tilde π_t^* \\colon \\ALPHABET S_{\\ge 0} \\to \\ALPHABET A\\) denote the action-value function, value function and the policy of the folded MDP. Then, if the original MDP satisfies conditions (A1) and (A2) then, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), \\[ Q_t(s,a) = \\tilde Q_t(|s|, a),\n\\quad\n  V_t(s) = \\tilde V_t(|s|),\n\\quad\n  π_t^*(s) = \\tilde π_t^*(|s|).\n\\]\n\n\nThe result of the previous part implies that if the value function \\(\\tilde V_t\\) and the policy \\(\\tilde π^*_t\\) are monotone increasing, then the value function \\(V_t\\) and the policy \\(π^*_t\\) are even and quasi-convex. This gives us a method to verify if the value function and optimal policy are even and quasi-convex.\nNow, recall the model of the Internet of Things presented in Q2 of Assignment 3. The numerical experiments that you did in Assignment 3 suggest that the value function and the optimal policy are even and quasi-convex. Prove that this is indeed the case.\nNow suppose the distribution of \\(W_t\\) is not Gaussian but is some general probability density \\(\\varphi(\\cdot)\\) and the cost function is \\[ c(e,a) = \\lambda a + (1 - a) d(e). \\] Find conditions on \\(\\varphi\\) and \\(d\\) such that the value function and optimal policy are even and quasi-convex."
  },
  {
    "objectID": "mdps/monotone-mdps.html#notes",
    "href": "mdps/monotone-mdps.html#notes",
    "title": "7  Monotonicity of value function and optimal policies",
    "section": "Notes",
    "text": "Notes\nStochastic dominance has been employed in various areas of economics, finance, and statistics since the 1930s. See Levy (1992) and Levy (2015) for detailed overviews. The notion of stochastic monotonicity for Markov chains is due to Daley (1968). For a generalization of stochastic monotonicity to continuous state spaces, see Serfozo (1976). The characterization of stochastic monotonicity in Exercise 7.1–Exercise 7.4 are due to Keilson and Kester (1977).\nRoss (1974) has an early treatment of monotonicity of optimal policies. The general theory was developed by Topkis (1998). The presentation here follows Puterman (2014). Exercise 7.6 is from Chakravorty and Mahajan (2018).\n\n\n\n\nChakravorty, J. and Mahajan, A. 2018. Sufficient conditions for the value function and optimal strategy to be even and quasi-convex. IEEE Transactions on Automatic Control 63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nDaley, D.J. 1968. Stochastically monotone markov chains. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 10, 4, 305–317. DOI: 10.1007/BF00531852.\n\n\nKeilson, J. and Kester, A. 1977. Monotone matrices and monotone markov processes. Stochastic Processes and their Applications 5, 3, 231–241.\n\n\nLevy, H. 1992. Stochastic dominance and expected utility: Survey and analysis. Management Science 38, 4, 555–593. DOI: 10.1287/mnsc.38.4.555.\n\n\nLevy, H. 2015. Stochastic dominance: Investment decision making under uncertainty. Springer. DOI: 10.1007/978-3-319-21708-6.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236.\n\n\nSerfozo, R.F. 1976. Monotone optimal policies for markov decision processes. In: Mathematical programming studies. Springer Berlin Heidelberg, 202–215. DOI: 10.1007/bfb0120752.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#dynamic-program",
    "href": "mdps/power-delay-tradeoff.html#dynamic-program",
    "title": "8  Power-delay tradeoff in wireless communication",
    "section": "8.1 Dynamic program",
    "text": "8.1 Dynamic program\nWe can assume \\(Y_t = X_t - A_t\\) as a post-decision state in the above model and write the dynamic program as follows:\n\\[ V_{T+1}(x,s) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\\begin{align*}\n  H_t(y,s) &= \\lambda d(y) + \\EXP[ V_{t+1}(y + W_t, S_{t+1}) | S_t = s ], \\\\\n  V_t(x,s) &= \\min_{0 \\le a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\big\\}\n\\end{align*}\\]\n\n8.1.1 Monotonicity of value functions\n\nLemma 8.1 For all \\(t\\), \\(V_t(x,s)\\) and \\(H_t(y,s)\\) are increasing in both variables.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that the constraint set \\(\\ALPHABET A(x) = \\{0, \\dots, x\\}\\) satisfies the conditions that generalize the result of monotonicity to constrained actions.\nWe prove the two monotonicity properties by backward induction. First note that \\(V_{T+1}(x,s)\\) is trivially monotone. This forms the basis of induction. Now suppose \\(V_{t+1}(x,s)\\) is increasing in \\(x\\) and \\(s\\). Since \\(\\{S_t\\}_{t \\ge 1}\\) is stochastically monotone, \\[H_t(y,s) = \\lambda d(y) + \\EXP[ V_{t+1}(y + W_t, S_{t+1}) | S_t = s ]\\] is increasing in \\(s\\). Moreover, since both \\(d(y)\\) and \\(V_{t+1}(y + w, s)\\) are increasing in \\(y\\), so is \\(H_t(y,s)\\).\nNow, for every \\(a\\), \\(p(a) q(s)\\) and \\(H_t(x-a, s)\\) is increasing in \\(x\\) and \\(s\\). So, the pointwise minima over \\(a\\) is also increasing in \\(x\\) and \\(s\\).\n\n\n\n\n\n8.1.2 Convexity of value functions\n\nLemma 8.2 For all time \\(t\\) and channel state \\(s\\), \\(V_t(x,s)\\) and \\(H_t(y,s)\\) are convex in the first variable.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. First note that \\(V_{T+1}(x,s)\\) is trivially convex in \\(x\\). Now assume that \\(V_{t+1}(x,s)\\) is convex in \\(x\\). Then, \\(\\EXP[V_{t+1}(y + W_t, S_{t+1}) | S_t = s]\\) is weighted sum of convex functions and is, therefore, convex in \\(y\\). Therefore, \\(H_t(y,s)\\) is a sum of two convex functions and, therefore, convex in \\(y\\).\nWe cannot directly show the convexity of \\(V_t(x,s)\\) because the pointwise minimum of convex functions is not convex. So, we consider the following argument. Fix \\(s\\) and pick \\(x > 1\\). Let \\(\\underline a = π^*_t(x-1,s)\\) and \\(\\bar a = π^*_t(x+1,s)\\). Let \\(\\underline v = \\lfloor (\\underline a + \\bar a)/2 \\rfloor\\) and \\(\\bar v = \\lceil (\\underline a + \\bar a)/2 \\rceil\\). Note that both \\(\\underline v\\) and \\(\\bar v\\) are feasible at \\(x\\). Then, \\[ \\begin{align*}\n  \\hskip 2em & \\hskip -2em\n  V_t(x-1, s) + V_t(x+1, s)\n  \\\\\n  &=\n  [ p(\\underline a) + p(\\bar a) ] q(s) + H_t(x - 1 - \\underline a, s)\n  + H_t(x + 1 - \\bar a, s)\n  \\\\\n  &\\stackrel{(a)}\\ge [ p(\\underline v) + p(\\bar v)] q(s) +\n    H_t(x - \\underline v, s) + H_t(x - \\bar v, s) \\\\\n  &\\ge 2 \\min_{a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\\\\n  &= 2 V_t(x,s),\n\\end{align*} \\] where \\((a)\\) follows from convexity of \\(p(\\cdot)\\) and \\(H_t(\\cdot, s)\\). Thus, \\(V_t(x,s)\\) is convex in \\(x\\). This completes the induction step.\n\n\n\n\n\n8.1.3 Monotonicity of optimal policy in queue length\n\nTheorem 8.1 For all time \\(t\\) and channel state s\\(s\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is increasing in the queue length \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn the previous lemma, we have shown that \\(H_t(y,s)\\) is convex in \\(y\\). Therefore, \\(H_t(x-a, s)\\) is submodular in \\((x,a)\\).\n\nThus, for a fixed \\(s\\), \\(p(a)q(s) + H_t(x-a, s)\\) is submodular in \\((x,a)\\). Therefore, the optimal policy is increasing in \\(x\\).\n\n\n\nOne can show submodularity by finite difference, but for simplicity, we assume that \\(H_t(y,s)\\) is twice differentiable. Then, \\(\\partial^2 H_t(x - a, s)/ \\partial x \\partial a \\le 0\\) (by convexity of \\(H_t\\)).\n\n8.1.4 Monotonicity of optimal policy in channel state\nIt is natural to expect that for a fixed \\(x\\) the optimal policy is decreasing in \\(s\\). However, it is not possible to obtain the monotonicity of optimal policy in channel state in general. To see why this is difficult, let us impose a mild assumption on the arrival distribution.\n\nThe packet arrival distribution is weakly decreasing, i.e., for any \\(v,w \\in \\integers_{\\ge 0}\\) such that \\(v \\le w\\), we have that \\(P_W(v) \\ge P_W(w)\\).\n\nWe first start with a slight generalization of stochastic monotonicity result.\n\nLemma 8.3 Let \\(\\{p_i\\}_{i \\ge 0}\\) and \\(\\{q_i\\}_{i \\ge 0}\\) be real-valued non-negative sequences satisfying \\[ \\sum_{i \\le j} p_i \\le \\sum_{i \\le j} q_i, \\quad \\forall j.\\] Then, for any increasing sequence \\(\\{v_i\\}_{i \\ge 0}\\), we have \\[ \\sum_{i = 0}^\\infty p_i v_i \\ge \\sum_{i=0}^\\infty q_i v_i. \\]\n\nThe proof is similar to the proof for stochastic monotonicity.\n\nLemma 8.4 Under (asm-power-delay-density?), for all \\(t\\), \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nThe idea of the proof is similar to Lemma 1 of the notes on monotone MDPs.\nFix \\(y^+, y^- \\in \\integers_{\\ge 0}\\) and \\(s^+, s^- \\in \\ALPHABET S\\) such that \\(y^+ > y^-\\) and \\(s^+ > s^-\\). Now, for any \\(y' \\in \\integers_{\\ge 0}\\) and \\(s' \\in \\ALPHABET S\\) define \\[\\begin{align*}\n  π(y',s') = P_W(y' - y^+)P_S(s'|s^+) +\n             P_W(y' - y^-)P_S(s'|s^-),\n             \\\\\n  μ(y',s') = P_W(y' - y^-)P_S(s'|s^+) +\n             P_W(y' - y^+)P_S(s'|s^-).\n\\end{align*}\\]\nSince \\(P_S\\) is stochastically monotone, we have that for any \\(σ \\in \\ALPHABET S\\), \\[ \\sum_{s'=1}^{σ} P_S(s'|s^+) \\le \\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Moreover, due to (asm-power-delay-density?), we have that \\(P_W(y' - y^-) \\le P_W(y' - y^+)\\). Thus, \\[ [P_W(y' - y^+) - P_W(y' - y^-)] \\sum_{s'=1}^{σ} P_S(s'|s^+)\n\\le [P_W(y' - y^+) - P_W(y' - y^-)]\\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Rearranging terms, we get \\[ \\sum_{s'=1}^σ π(y',s') \\le \\sum_{s'=1}^σ μ(y',s'). \\] Thus, for any \\(y'\\), the sequence \\(π(y',s')\\) and \\(ν(y',s')\\) satisfy the condition of Lemma 8.3.\nNow, in Lemma 8.1, we have established that for any \\(y'\\), \\(V_{t+1}(y',s')\\) is increasing in \\(s'\\). Thus, from Lemma 8.3, we have \\[  \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Summing up over \\(y'\\), we get \\[  \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Or equivalently, \\[\\begin{align*}\n\\hskip 2em & \\hskip -2em\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^-) ]\n\\\\\n& \\ge\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^-) ] .\n\\end{align*}\\] Thus, \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\n\n\nEven under (asm-power-delay-density?), we cannot establish the monotonicity of \\(π^*_t(x,s)\\) is \\(s\\).\n\n\n\nNote that we have established that \\(H_t(y,s)\\) is supermodular in \\((y,s)\\). Thus, for any fixed \\(x\\), \\(H_t(x-a,s)\\) is submodular in \\((a,s)\\). Furthermore the function \\(p(a)q(s)\\) is increasing in both variables and therefore supermodular in \\((a,s)\\). Therefore, we cannot say anything specific about \\(p(a)q(s) + H_t(x-a, s)\\) which is a sum of submodular and supermodular functions."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#exercises",
    "href": "mdps/power-delay-tradeoff.html#exercises",
    "title": "8  Power-delay tradeoff in wireless communication",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 8.1 In this exercise, we provide sufficient conditions for the optimal policy to be monotone in the channel state. Suppose that the channel state \\(\\{S_t\\}_{t \\ge 1}\\) is an i.i.d. process. Then prove that for all time \\(t\\) and queue state \\(x\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is decreasing in channel state \\(s\\)."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#notes",
    "href": "mdps/power-delay-tradeoff.html#notes",
    "title": "8  Power-delay tradeoff in wireless communication",
    "section": "Notes",
    "text": "Notes\nThe mathematical model of power-delay trade-off is taken from Berry (2000), where the monotonicty results were proved using first principles. More detailed characterization of the optimal transmission strategy when the average power or the average delay goes to zero are provided in Berry and Gallager (2002) and Berry (2013). A related model is presented in Ding et al. (2016).\nFor a broader overview of power-delay trade offs in wireless communication, see Berry et al. (2012) and Yeh (2012).\nThe remark after Lemma 8.4 shows the difficulty in establishing monotonicity of optimal policies for a multi-dimensional state space. In fact, sometimes even when monotonicity appears to be intuitively obvious, it may not hold. See Sayedana and Mahajan (2020) for an example. For general discussions on monotonicity for multi-dimensional state spaces, see Topkis (1998) and Koole (2006). As an example of using such general conditions to establish monotonicity, see Sayedana et al. (2020).\n\n\n\n\nBerry, R.A. 2000. Power and delay trade-offs in fading channels. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay tradeoffs in fading channels—small-delay asymptotics. IEEE Transactions on Information Theory 59, 6, 3939–3952. DOI: 10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002. Communication over fading channels with delay constraints. IEEE Transactions on Information Theory 48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M. 2012. Energy-efficient scheduling under delay constraints for wireless networks. Synthesis Lectures on Communication Networks 5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A. 2016. On monotonicity of the optimal transmission policy in cross-layer adaptive \\(m\\) -QAM modulation. IEEE Transactions on Communications 64, 9, 3771–3785. DOI: 10.1109/TCOMM.2016.2590427.\n\n\nKoole, G. 2006. Monotonicity in markov reward and decision chains: Theory and applications. Foundations and Trends in Stochastic Systems 1, 1, 1–76. DOI: 10.1561/0900000002.\n\n\nSayedana, B. and Mahajan, A. 2020. Counterexamples on the monotonicity of delay optimal strategies for energy harvesting transmitters. IEEE Wireless Communications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E. 2020. Cross-layer communication over fading channels with adaptive decision feedback. International symposium on modeling and optimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press.\n\n\nYeh, E.M. 2012. Fundamental performance limits in cross-layer wireless optimization: Throughput, delay, and energy. Foundations and Trends in Communications and Information Theory 9, 1, 1–112. DOI: 10.1561/0100000014."
  },
  {
    "objectID": "mdps/reward-shaping.html",
    "href": "mdps/reward-shaping.html",
    "title": "9  Reward Shaping",
    "section": "",
    "text": "Notes\nThe idea of reward shaping was proposed by Skinner (1938) to synthesize complex behavior by guiding animals to perform simple functions (see :Skinner’s Box Experiment). The formal description of reward shaping comes from Porteus (1975), who established a result similar to Ng et al. (1999), and called it the transformation method. Porteus (1975) also describes transformations of the dynamics which preserve the optimal policy.\nCorollary 9.2 was also re-established by Ng et al. (1999), who aslo provided a partial converse. The results of Porteus (1975) and Ng et al. (1999) were restricted to time-homogeneous potential functions. The generalization to time-varying potential functions was presented in Devlin and Kudenko (2012).\nThe partial converse of Corollary 9.1 established by Ng et al. (1999) states that the shaping presented in Theorem 9.1 is the only additive cost transformation that that preserves the set of optimal policy. However, this converse was derived under the assumption that the transition dynamics are complete (see Ng et al. (1999)). A similar converse under a weaker set of assumptions on the transition dynamics is established in Jenner et al. (2022).\nFor a discussion on practical considerations in using reward shaping in reinforcement learning, see Grzes and Kudenko (2009) and Devlin (2014). As a counter-point, Wiewiora (2003) shows that the advantages of reward shaping can also be achieved by simply adding the potential function to the \\(Q\\)-function initialization."
  },
  {
    "objectID": "mdps/reward-shaping.html#generalization-to-discounted-models",
    "href": "mdps/reward-shaping.html#generalization-to-discounted-models",
    "title": "9  Reward Shaping",
    "section": "9.1 Generalization to discounted models",
    "text": "9.1 Generalization to discounted models\nNow consider a finite horizon discounted cost problem, where the performance of a policy \\(π\\) is given by \\[\nJ(π) = \\EXP\\Bigl[ \\sum_{t=1}^{T-1} γ^{t-1} c_t(S_t, A_t) + γ^T c_T(S_T)\n       \\Bigr].\n\\] As argued in the introduction to discounted models, the dynamic prgram for this case is given by\n\\[ V_{T}(s) = c_T(s) \\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[ \\begin{align*}\n  Q_t(s,a) &= c(s,a) + γ \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\\\\n  V_t(s) &= \\min_{a \\in \\ALPHABET A} Q_t(s,a).\n\\end{align*} \\]\nFor such models, we have the following.\n\nCorollary 9.2 For discounted cost models, the results of Theorem 9.1 and Corollary 9.1 continue to hold if condition 2 is replaced by\n\nFor \\(t \\in \\{1, \\dots, T-1\\}\\),\n\\[ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) + γ Φ_{t+1}(s_{+}) - Φ_t(s). \\]\n\n\n\n\n\n\n\n\nInfinite horizon models\n\n\n\nIf the cost function is time homogeneous, Corollary 9.2 extends naturally to infinite horizon models with a time-homogeneous potential function. A remarkable feature is that if the potential function is chosen as the value function, i.e., \\(Φ(s) = V(s)\\), then the value function of the modified cost \\(\\tilde c(s,a,s_{+})\\) is zero!"
  },
  {
    "objectID": "mdps/reward-shaping.html#examples",
    "href": "mdps/reward-shaping.html#examples",
    "title": "9  Reward Shaping",
    "section": "9.2 Examples",
    "text": "9.2 Examples\nAs an example of reward shaping, see the notes on inventory management. Also see the notes on martingale approach to stochastic control for an iteresting relationship between reward shaping and martingales."
  },
  {
    "objectID": "mdps/inf-horizon.html#performance-of-a-time-homogeneous-markov-policy",
    "href": "mdps/inf-horizon.html#performance-of-a-time-homogeneous-markov-policy",
    "title": "10  Infinite horizon MDPs",
    "section": "10.1 Performance of a time-homogeneous Markov policy",
    "text": "10.1 Performance of a time-homogeneous Markov policy\nFor any \\(π \\colon \\ALPHABET S \\to \\ALPHABET A\\), consider the time homogeneous policy \\((π, π, \\dots)\\). For ease of notation, we denote this policy simply by \\(π\\). The expected discounted cost under this policy is given by \\[ V_π(s) = \\EXP^π\\bigg[ \\sum_{t=1}^∞ γ^{t-1} c(S_t, A_t) \\biggm| S_1 = s\n\\bigg].\\]\nTo get a compact expression for this, define a \\(n × 1\\) vector \\(c_π\\) and a \\(n × n\\) matrix \\(P_π\\) as follows: \\[ [c_π]_s = c(s, π(s))\n   \\quad\\text{and}\\quad\n   [P_π]_{ss'} = P_{ss'}(π(s)).\n\\] Then the dynamics under policy \\(π\\) are Markovian with transition probability matrix \\(P_π\\) and a cost function \\(c_π\\). Then \\[ \\begin{align*}\n\\EXP^π\\big[ c(S_t, π(S_t)) \\bigm| S_1 = s \\big]\n  &= \\sum_{s' \\in \\ALPHABET S} \\PR^π(S_t = s' | S_1 = s) c(s', π(s'))\n  \\\\\n  &= \\sum_{s' \\in \\ALPHABET S} [P_π^{t-1}]_{ss'} [c_π]_y\n  \\\\\n  &= δ_s P_π^{t-1} c_π.\n\\end{align*} \\]\nLet \\(V_π\\) denote the \\(n × 1\\) vector given by \\([V_π]_s = V_π(s)\\). Then, \\[ \\begin{align*}\nV_π &= c_π + γ P_π c_π + γ^2 P_π^2 c_π + \\cdots \\\\\n    &= c_π + γ P_π \\big( c_π + γ P_π c_π + \\cdots \\big) \\\\\n    &= c_π + γ P_π V_π,\n\\end{align*} \\] which can be rewritten as \\[ (I - γ P_π) V_π = c_π. \\]\nThe :spectral radius \\(ρ(γ P_d)\\) of a matrix is upper bounded by its :spectral norm \\(\\lVert γ P_d \\rVert = γ < 1\\). Therefore, the matrix \\((I - γ P_π)\\) has an inverse and by left multiplying both sides by \\((I - γ P_π)^{-1}\\), we get \\[ V_π = (I - γP_π)^{-1} c_π. \\]\nThe equation \\[ V_π = c_π + γ P_π V_π \\] is sometimes also written as \\[ V_π = \\mathcal B_π V_π \\] where the operator \\(\\mathcal B_π\\), which is called the Bellman operator, is an operator from \\(\\reals^n\\) to \\(\\reals^n\\) given by \\[ \\mathcal B_π v = c_π + γ P_π v.\\]"
  },
  {
    "objectID": "mdps/inf-horizon.html#bellman-operators",
    "href": "mdps/inf-horizon.html#bellman-operators",
    "title": "10  Infinite horizon MDPs",
    "section": "10.2 Bellman operators",
    "text": "10.2 Bellman operators\n\nDefinition 10.1 Define the Bellman operator \\(\\mathcal B : \\reals^n \\to \\reals^n\\) as follows: for any \\(v \\in \\reals^n\\) \\[ [\\mathcal B v]_s = \\min_{a \\in \\ALPHABET A}\n\\Big\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S} P_{ss'}(a) v_y \\Big\\}.\n\\]\n\nNote that the above may also be written as1 \\[ \\mathcal B v = \\min_{π \\in \\Pi} \\mathcal B_π v, \\] where \\(\\Pi\\) denotes the set of all deterministic Markov policies.1 This is true for general models only when the arg min at each state exists.\n\nProposition 10.1 For any \\(v \\in \\reals^n\\), define the norm \\(\\NORM{V} := \\sup_{s \\in \\ALPHABET S} \\ABS{V_s}\\). Then, the Bellman operator is a contraction, i.e., for any \\(v, w \\in \\reals^n\\), \\[ \\NORM{\\mathcal B v - \\mathcal B w} \\le γ \\NORM{v - w}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix a state \\(s \\in \\ALPHABET S\\) and consider \\([\\mathcal B v](s) - [\\mathcal B w](s)\\). In particular, let \\(a^*\\) be the optimal action in the right hand side of \\([\\mathcal B w](s)\\). Then, \\[\\begin{align*}\n  [\\mathcal B v - \\mathcal B w](s) &=\n  \\min_{a \\in \\ALPHABET A}\\bigl\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S}\n  P_{ss'}(a) v(s') \\bigr\\} -\n  \\min_{a \\in \\ALPHABET A}\\bigl\\{ c(s,a) + γ \\sum_{s' \\in \\ALPHABET S}\n  P_{ss'}(a) w(s') \\bigr\\}\n  \\\\\n  &\\le c(s,a^*) + γ \\sum_{s'\\in \\ALPHABET S} P_{ss'}(a^*) v(s') -\n       c(s,a^*) - γ \\sum_{s'\\in \\ALPHABET S} P_{ss'}(a^*) w(s')\n  \\\\\n  &\\le γ \\sum_{s' \\in \\ALPHABET S} P_{ss'}(a^*) \\| v - w \\|\n  \\\\\n  &= γ \\| v - w \\|.\n\\end{align*} \\]\nBy a similar argument, we can show that \\([\\mathcal B w - \\mathcal B v](s) \\le γ \\| v - w \\|\\), which proves the other side of the inequality.\n\n\n\nAn immediate consequence of the contraction property is the following.\n\nTheorem 10.1 There is a unique bounded \\(V^* \\in \\reals^n\\) that satisfies the Bellman equation \\[ V = \\mathcal B V \\]\nMoreover, if we start from any \\(V_0 \\in \\reals^n\\) and recursively define \\[ V_n = \\mathcal B V_{n-1} \\] then \\[ \\lim_{n \\to ∞} V_n = V^*. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis follows immediately from the Banach fixed point theorem."
  },
  {
    "objectID": "mdps/inf-horizon.html#optimal-time-homogeneous-policy",
    "href": "mdps/inf-horizon.html#optimal-time-homogeneous-policy",
    "title": "10  Infinite horizon MDPs",
    "section": "10.3 Optimal time-homogeneous policy",
    "text": "10.3 Optimal time-homogeneous policy\n\n\nProposition 10.2 Define \\[ V^{\\text{opt}}_∞(s) := \\min_{π} \\EXP^π \\bigg[ \\sum_{t=1}^∞ γ^{t-1} c(S_t, A_t)\n\\biggm| S_1 = s \\bigg], \\] where the minimum is over all (possibly randomized) history dependent policies. Then, \\[ V^{\\text{opt}}_∞ = V^*, \\] where \\(V^*\\) is the solution of the Bellman equation.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the state and action space are finite, without loss of optimality, we can assume that \\(0 \\le c(s,a) \\le M\\).\nConsider the finite horizon truncation \\[ V^{\\text{opt}}_T(s) =  \\min_{π} \\EXP^π\\bigg[ \\sum_{t=1}^T γ^{t-1} c(S_t, A_t) | S_1 = s \\bigg].\n\\] From the results for finite horizon MDP, we have that \\[ V^{\\text{opt}}_T = \\mathcal B^T V_0 \\] where \\(V_0\\) is the all zeros vector.\nNow by construction, \\[V^{\\text{opt}}_∞(s) \\ge V^{\\text{opt}}_T(s) = [\\mathcal B^T V_0](s). \\] Taking limit as \\(T \\to ∞\\), we get that \\[\\begin{equation}\\label{eq:1}\n  V^{\\text{opt}}_∞(s) \\ge \\lim_{T \\to ∞} [\\mathcal B^T V_0](s) = V^*(s).\n\\end{equation}\\]\nSince \\(0 \\le c(s,a) \\le M\\), for any \\(T\\), \\[ \\begin{align*}\nV^{\\text{opt}}_∞(s) &\\le \\min_π \\EXP^π \\bigg[ \\sum_{t=1}^T γ^{t-1} c(S_t, A_t)\n\\biggm| S_1 = s \\bigg] + \\sum_{t=T+1}^∞ γ^{t-1} M \\\\\n&= V^{\\text{opt}}_T(s) + γ^T M / (1 - γ) \\\\\n&= [\\mathcal B^T V_0](s) + γ^T M / (1-γ).\n\\end{align*} \\] Taking limit as \\(T \\to ∞\\), we get that \\[\\begin{equation}\\label{eq:2}\n  V^{\\text{opt}}_∞(s) \\le \\lim_{T \\to ∞}\n  \\big\\{ [\\mathcal B^T V_0](s) + γ^T M / (1-γ) \\big\\} = V^*(s).\n\\end{equation}\\]\nFrom \\eqref{eq:1} and \\eqref{eq:2}, we get that \\(V^{\\text{opt}}_∞ = V^*\\)."
  },
  {
    "objectID": "mdps/inf-horizon.html#properties-of-bellman-operator",
    "href": "mdps/inf-horizon.html#properties-of-bellman-operator",
    "title": "10  Infinite horizon MDPs",
    "section": "10.4 Properties of Bellman operator",
    "text": "10.4 Properties of Bellman operator\n\nProposition 10.3 The Bellman operator satisfies the following properties\n\nMonotonicity. For any \\(v, w \\in \\reals^n\\), if \\(v \\le w\\), then \\(\\mathcal B_π v \\le \\mathcal B_π w\\) and \\(\\mathcal B v \\le \\mathcal B w\\).\nDiscounting. For any \\(v \\in \\reals^n\\) and \\(m \\in \\reals\\), \\(\\mathcal B_π (v + m \\ONES) = \\mathcal B_π v + γ m \\ONES\\) and \\(\\mathcal B (v + m \\ONES) = \\mathcal B v + γ m \\ONES\\).\n\n\n\n\n\n\n\n\nProof of monotonicity property\n\n\n\n\n\nRecall that \\[ \\mathcal B_π v = c_π + γ P_π v. \\] So, monotonicity of \\(\\mathcal B_π\\) follows immediately from monotonicity of matrix multiplication for positive matrices.\nLet \\(μ\\) be such that \\(\\mathcal B w = \\mathcal B_μ w\\). Then, \\[ \\mathcal B v \\le \\mathcal B_μ v\n\\stackrel{(a)} \\le \\mathcal B_μ w = \\mathcal B w,\n\\] where \\((a)\\) uses the monotonicity of \\(\\mathcal B_μ\\).\n\n\n\n\n\n\n\n\n\nProof of discounting property\n\n\n\n\n\nRecall that \\[ \\mathcal B_π v = c_π + γ P_π v. \\] Thus, \\[ \\mathcal B_π(v+m \\ONES) = c_π + γ P_π (v+m \\ONES) = c_π + γ P_π v + γ m\n\\ONES = \\mathcal B_π\nv + γ m \\ONES.\\] Thus, \\(\\mathcal B_π\\) is discounting. Now consider \\[ \\mathcal B (v + m \\ONES ) = \\min_{π} \\mathcal B_π (v+m \\ONES)\n= \\min_π \\mathcal (B_π v + γ m \\ONES) = \\mathcal B v + γ m \\ONES.\\] Thus, \\(\\mathcal B\\) is discounting.\n\n\n\n\nProposition 10.4 For any \\(V \\in \\reals^n\\),\n\nIf \\(V \\ge \\mathcal B V\\), then \\(V \\ge V^*\\);\nIf \\(V \\le \\mathcal B V\\), then \\(V \\le V^*\\);\nIf \\(V = \\mathcal B V\\), then \\(V\\) is the only vector with this property and \\(V = V^*\\).\n\nThe same bounds are true when \\((\\mathcal B, V^*)\\) is replaced with \\((\\mathcal B_π, V_π)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the first part. The proof of the other parts is similar.\nWe are given that \\[V \\ge \\mathcal B V.\\] Then, by monotonicity of the Bellman operator, \\[ \\mathcal B V \\ge \\mathcal B^2 V.\\] Continuing this way, we get \\[ \\mathcal B^k V \\ge \\mathcal B^{k+1} V.\\] Adding the above equations, we get \\[ V \\ge \\mathcal B^{k+1} V.\\] Taking limit as \\(k \\to ∞\\), we get \\[V \\ge V^*.\\]\n\n\n\n\nProposition 10.5 For any \\(V \\in \\reals^n\\) and \\(m \\in \\reals\\),\n\nIf \\(V + m \\ONES \\ge \\mathcal B V\\), then \\(V + m \\ONES/(1-γ) \\ge V^*\\);\nIf \\(V + m \\ONES \\le \\mathcal B V\\), then \\(V + m \\ONES/(1-γ) \\le V^*\\);\n\nThe same bounds are true when \\((\\mathcal B, V^*)\\) is replaced with \\((\\mathcal B_π, V_π)\\).\n\n\n\n\n\n\n\nRemark\n\n\n\nThe above result can also be stated as follows:\n\n\\(\\displaystyle \\| V_π - V \\| \\le \\frac{1}{1-γ}\\| \\mathcal B_π V - V \\|\\).\n\\(\\displaystyle \\| V^* - V \\| \\le \\frac{1}{1-γ}\\| \\mathcal B V - V \\|\\).\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAgain, we only prove the first part. The proof of the second part is the same. We have that \\[ V + m \\ONES \\ge \\mathcal B V. \\] From discounting and monotonicity properties, we get \\[ \\mathcal B V + γ m \\ONES \\ge \\mathcal B^2 V. \\] Again, from discounting and monotonitiy properties, we get \\[ \\mathcal B^2 V + γ^2 m \\ONES \\ge \\mathcal B^3 V. \\] Continuing this way, we get \\[ \\mathcal B^k V + γ^k m \\ONES \\ge \\mathcal B^{k+1} V. \\] Adding all the above equations, we get \\[ V + \\sum_{\\ell = 0}^k γ^\\ell m \\ONES \\ge \\mathcal B^{k+1} V. \\] Taking the limit as \\(k \\to ∞\\), we get \\[ V + m \\ONES/(1-γ) \\ge V^*. \\]"
  },
  {
    "objectID": "mdps/inf-horizon.html#exercises",
    "href": "mdps/inf-horizon.html#exercises",
    "title": "10  Infinite horizon MDPs",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 10.1 (One-step look-ahead error bounds.) Given any \\(V \\in \\reals^n\\), let \\(π\\) be such that \\(\\mathcal B V = \\mathcal B_π V\\). Moreover, let \\(V^*\\) denote the unique fixed point of \\(\\mathcal B\\) and \\(V_π\\) denote the unique fixed point of \\(\\mathcal B_π\\). Then, show that\n\n\\[ \\| V^* - V \\| \\le \\frac{1}{1-γ} \\| \\mathcal B V - V \\|. \\]\n\\[ \\| V^* - \\mathcal B V \\| \\le \\frac{γ}{1-γ} \\| \\mathcal B V - V \\|. \\]\n\\[ \\| V_π - V \\| \\le \\frac{1}{1-γ} \\| \\mathcal B_π V - V \\|. \\]\n\\[ \\| V_π - \\mathcal B_π V \\| \\le \\frac{γ}{1-γ} \\| \\mathcal B_π V - V \\|. \\]\n\\[ \\| V_π - V^* \\| \\le \\frac{2}{1-γ} \\| \\mathcal B V - V \\|. \\]\n\\[ \\| V_π - V^* \\| \\le \\frac{2γ}{1 - γ} \\| V - V^* \\|. \\]"
  },
  {
    "objectID": "mdps/inf-horizon.html#notes",
    "href": "mdps/inf-horizon.html#notes",
    "title": "10  Infinite horizon MDPs",
    "section": "Notes",
    "text": "Notes\nThe material included here is referenced from different sources. Perhaps the best sources to study this material are the books by Puterman (2014), Whittle (1982), and Bertsekas (2011).\n\n\n\n\nBertsekas, D.P. 2011. Dynamic programming and optimal control. Athena Scientific. Available at: http://www.athenasc.com/dpbook.html.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.\n\n\nShwartz, A. 2001. Death and discounting. IEEE Transactions on Automatic Control 46, 4, 644–647. DOI: 10.1109/9.917668.\n\n\nWhittle, P. 1982. Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2. Wiley."
  },
  {
    "objectID": "mdps/mdp-algorithms.html",
    "href": "mdps/mdp-algorithms.html",
    "title": "11  MDP algorithms",
    "section": "",
    "text": "References\nThe techniques for value iteration and policy improvement were formalized by Howard (1960). The equivalence of policy improvement and the Newton-Raphson algorithm was demonstrated in the LQ case by Whittle and Komarova (1988), for which it holds in a tighter sense."
  },
  {
    "objectID": "mdps/mdp-algorithms.html#value-iteration-algorithm",
    "href": "mdps/mdp-algorithms.html#value-iteration-algorithm",
    "title": "11  MDP algorithms",
    "section": "11.1 Value Iteration Algorithm",
    "text": "11.1 Value Iteration Algorithm\n\n\n\n\n\n\n Value Iteration Algorithm\n\n\n\n\nStart with any \\(V_0 \\in \\reals^n\\).\nRecursively compute \\(V_{k+1} = \\mathcal B V_k = \\mathcal B_{π_k} V_k.\\)\nDefine \\[ \\begin{align*}\n   \\underline δ_k &= \\frac{γ}{1-γ} \\min_s \\{ V_k(s) - V_{k-1}(s) \\}, \\\\\n   \\bar δ_k &=       \\frac{γ}{1-γ} \\max_s \\{ V_k(s) - V_{k-1}(s) \\}.\n\\end{align*} \\]\n\nThen, for all \\(k\\)\n\n\\(V_k + \\underline δ_k \\ONES \\le V^* \\le V_k + \\bar δ_k \\ONES\\).\n\\((\\underline δ_k - \\bar δ_k) \\ONES \\le V_{π_k} - V^* \\le (\\bar δ_k - \\underline δ_k) \\ONES.\\)\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy construction, \\[ \\begin{align*}\n   \\mathcal B V_k - V_k &= \\mathcal B V_k - \\mathcal B V_{k-1} \\\\\n   & \\le \\mathcal B_{π_{k-1}} V_k - \\mathcal B_{π_{k-1}} V_{k-1}\\\\\n   & \\le γ P_{π_{k-1}}[ V_k - V_{k-1} ] \\\\\n   &= (1-γ) \\bar δ_k \\ONES.\n\\end{align*} \\] Thus, by Proposition 10.5, we have \\[\\begin{equation} \\label{eq:VI-1}\n  V^* \\le V_k + \\bar δ_k \\ONES.\n\\end{equation}\\] Note that \\(\\mathcal B V_k = \\mathcal B_{π_k} V_k\\). So, we have also show that \\(\\mathcal B_{π_k} V_k - V_k \\le (1-γ) \\bar δ_k \\ONES\\). Thus, again by Proposition 10.5, we have \\[\\begin{equation}\\label{eq:VI-2}\n  V_{π_k} \\le V_k + \\bar δ_k \\ONES.\n\\end{equation}\\]\nBy a similar argument, we can show \\[\\begin{equation}\\label{eq:VI-3}\n  V^* \\ge V_k + \\underline δ_k \\ONES\n\\quad\\text{and}\\quad\nV_{π_k} \\ge V_k + \\underline δ_k \\ONES.\n\\end{equation}\\]\nEq. \\eqref{eq:VI-1} and \\eqref{eq:VI-3} imply the first relationship of the result. To establish the second relationship, note that the triangle inequality \\[ \\max\\{ V_{π_k} - V^* \\} \\le\n   \\max\\{ V_{π_k} - V_k \\} + \\max\\{ V_{k} - V^* \\}\n   \\le (\\bar δ_k - \\underline δ_k) \\ONES.\n\\] Similarly, \\[\n  \\max\\{ V^* - V_{π_k} \\} \\le\n   \\max \\{ V^* - V_k \\} + \\max\\{ V_k - V_{π_k} \\}\n   \\le (\\bar δ_k - \\underline δ_k) \\ONES.\n\\] Combining the above two equation, we get the second relationship of the result."
  },
  {
    "objectID": "mdps/mdp-algorithms.html#policy-iteration-algorithm",
    "href": "mdps/mdp-algorithms.html#policy-iteration-algorithm",
    "title": "11  MDP algorithms",
    "section": "11.2 Policy Iteration Algorithm",
    "text": "11.2 Policy Iteration Algorithm\n\nLemma 11.1 (Policy improvement) Suppose \\(V_π\\) is the fixed point of \\(\\mathcal B_π\\) and \\[\\mathcal B_{μ} V_π = \\mathcal B V_π. \\] Then, \\[V_{μ}(s) \\le V_π(s), \\quad \\forall s \\in \\ALPHABET S. \\] Moreover, if \\(π\\) is not optimal, then at least one inequality is strict.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[ V_π = \\mathcal B_π V_π \\ge \\mathcal B V_π = \\mathcal B_{μ} V_π.\\] Thus, \\[ V_π \\ge V_{μ}. \\]\nFinally, suppose \\(V_μ = V_π\\). Then, \\[ V_μ = \\mathcal B_μ V_μ = \\mathcal B_μ V_π = \\mathcal B V_π = \\mathcal B\nV_μ. \\] Thus, \\(V_μ\\) (and \\(V_π\\)) is the unique fixed point of \\(\\mathcal B\\). Hence \\(μ\\) and \\(π\\) are optimal.\n\n\n\n\n\n\n\n\n\n Policy Iteration Algorithm\n\n\n\n\nStart with an arbitrary policy \\(π_0\\). Compute \\(V_0 = \\mathcal B_{π_0} V_0\\).\nRecursively compute a policy \\(π_k\\) such that \\[\\mathcal B V_{k-1} = \\mathcal B_{π_k} V_{k-1}\\] and compute the performance of the policy using \\[ V_k = \\mathcal B_{π_k} V_k.\\]\nStop if \\(V_k = V_{k-1}\\) (or \\(π_k = π_{k-1}\\)).\n\n\n\nThe policy improvement lemma (Lemma 11.1) implies that \\(V_{k-1} \\ge V_k\\). Since the state and action spaces are finite, there are only a finite number of policies. The value function improves at each step. So the process must converge in finite number of iterations. At convergence, \\(V_k = V_{k-1}\\) and the policy improvement lemma implies that the corresponding policies \\(π_k\\) or \\(π_{k-1}\\) are optimal.\n\n11.2.1 Policy iteration as Newton-Raphson algoritm\n\nRecall that the main idea behind Newton-Raphson is as follows. Suppose we want to solve a fixed point equation \\(V = \\mathcal B V\\) and we have an approximate solution \\(V_k\\). Then we can search for an improved soluiton \\(V_{k+1} = V_k + Δ_k\\) by setting \\[\\begin{equation} \\label{eq:NR}\nV_k + Δ_k = \\mathcal{B}( V_k + Δ_k ),\n\\end{equation} \\] expanding the right-hand side as far as first-order terms in \\(Δ_k\\) and solving the consequent linear equation for \\(Δ_k\\).\nNow, let’s try to apply this idea to find the fixed point of the Bellman equation. Suppose we have identified a guess \\(V_k\\) and \\(\\mathcal B V_k = \\mathcal B_{π_{k+1}} V_k\\). Because the choice of control action \\(a\\) is optimization out in \\(\\mathcal B\\), the varation of \\(a\\) induced by the variation \\(Δ_k\\) of \\(V_k\\) has no first-order effect on the value of \\(\\mathcal B(V_k + Δ_k)\\). Therefore, \\[\n  \\mathcal{B}(V_k + Δ_k) = \\mathcal B_{π_{k+1}}(V_k + Δ_k) + o(Δ_k).\n\\] It follows that the linearized version of \\eqref{eq:NR} is just \\[\n  V_{k+1} = \\mathcal B_{π_{k+1}} V_{k+1}.\n\\] That is, \\(V_{k+1}\\) is just the value function for the policy \\(π_{k+1}\\), where \\(π_{k+1}\\) was deduced from the value function \\(V_k\\) exactly by the policy improvement procedure. Therefore, we can conclude the following.\n\nTheorem 11.1 The policy improvement algorithm is equivalent to the application of Newton-Raphson algorithm to the fixed point equation \\(V = \\mathcal B V\\) of dynamic programming.\n\nThe equivalence between policy iteration and Newton-Raphson partily explains why policy iteration approaches converge in few iterations."
  },
  {
    "objectID": "mdps/mdp-algorithms.html#optimistic-policy-iteration",
    "href": "mdps/mdp-algorithms.html#optimistic-policy-iteration",
    "title": "11  MDP algorithms",
    "section": "11.3 Optimistic Policy Iteration",
    "text": "11.3 Optimistic Policy Iteration\n\n\n\n\n\n\n Optimistic Policy Iteration Algorithm\n\n\n\n\nFix a sequence of integers \\(\\{\\ell_k\\}_{k \\in \\integers_{\\ge 0}}\\).\nStart with an initial guess \\(V_0 \\in \\reals^n\\).\nFor \\(k=0, 1, 2, \\dots\\), recursively compute a policy \\(π_k\\) such that \\[ \\mathcal B_{π_k} V_k = \\mathcal B V_k \\] and then update the value function \\[ V_{k+1} = \\mathcal B_{π_k}^{\\ell_k} V_k. \\]\n\n\n\nNote that if \\(\\ell_k = 1\\), the optimistic policy iteration is equivalent to value iteration and if \\(\\ell_k = \\infty\\), then optimistic policy iteration is equal to policy iteration.\nIn the remainder of this section, we state the modifications of the main results to establish the convergence bounds for optimistic policy iteration.\n\nProposition 11.1 For any \\(V \\in \\reals^n\\) and \\(m \\ONES \\in \\reals_{\\ge 0}\\)\n\nIf \\(V + m \\ONES \\ge \\mathcal B V = \\mathcal B_π V\\), then for any \\(\\ell \\in \\integers_{> 0}\\), \\[ \\mathcal B V + \\frac{γ}{1 - γ} m \\ONES \\ge \\mathcal B_π^\\ell V \\] and \\[ \\mathcal B_π^\\ell V + γ^\\ell m \\ONES \\ge \\mathcal B( \\mathcal B_π^\\ell V). \\]\n\n\nThe proof is left as an exercise.\n\nProposition 11.2 Let \\(\\{(V_k, π_k)\\}_{k \\ge 0}\\) be generated as per the optimistic policy iteration algorithm. Define \\[ \\alpha_k = \\begin{cases}\n  1, & \\text{if } k = 0 \\\\\n  γ^{\\ell_0 + \\ell_1 + \\dots + \\ell_{k-1}}, & \\text{if } k > 0.\n\\end{cases}\\] Suppose there exists an \\(m \\in \\reals\\) such that \\[ \\| V_0 - \\mathcal B V_0 \\| \\le m. \\] Then, for all \\(k \\ge 0\\), \\[ \\mathcal B V_{k+1} - \\alpha_{k+1} m \\le V_{k+1} \\le\n\\mathcal B V_k + \\frac{γ}{1-γ} \\alpha_k m.\n\\] Moreover, \\[ V_{k} - \\frac{(k+1) γ^k}{1-γ} m \\le\n    V^* \\le\n    V_k + \\frac{\\alpha_k}{1 - γ} m \\le\n    V_k + \\frac{γ^k}{1 - γ} m. \\]"
  },
  {
    "objectID": "mdps/mdp-algorithms.html#exercises",
    "href": "mdps/mdp-algorithms.html#exercises",
    "title": "11  MDP algorithms",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 11.1 Show that the error bound for value iteration is monotone with the number of iterations, i.e, \\[ V_k + \\underline δ_k \\ONES \\le V_{k+1} + \\underline δ_{k+1} \\ONES\n\\le V^*\n\\le V_{k+1} + \\bar δ_{k+1} \\ONES \\le V_k + \\bar δ_k \\ONES. \\]"
  },
  {
    "objectID": "mdps/inventory-management-revisited.html#exercises",
    "href": "mdps/inventory-management-revisited.html#exercises",
    "title": "12  Inventory management (revisted)",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 12.1 Suppose that the arrival process is exponential with rate \\(1/\\mu\\), i.e., the density of \\(W\\) is given by \\(e^{-s/\\mu}/\\mu\\). Show that the optimal threshold is given by \\[ s^* = \\mu \\log \\left[ \\frac{ c_h + c_s} { c_h + p (1-γ)/γ} \\right]. \\]\nHint: Recall that the CDF the exponential distribution is \\(F(s) = 1 - e^{-s/μ}\\)."
  },
  {
    "objectID": "mdps/inventory-management-revisited.html#notes",
    "href": "mdps/inventory-management-revisited.html#notes",
    "title": "12  Inventory management (revisted)",
    "section": "Notes",
    "text": "Notes\nThe idea of using reward shaping to derive a closed form expression for inventory management is taken from Whittle (1982). It is interesting to note that Whittle (1982) uses the idea of reward shaping more than 17 years before the paper by Ng et al. (1999) on reward shaping. It is possible that Whittle was using the results of Porteus (1975).\n\n\n\n\nNg, A.Y., Harada, D., and Russell, S. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. ICML, 278–287. Available at: http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf.\n\n\nPorteus, E.L. 1975. Bounds and transformations for discounted finite markov decision chains. Operations Research 23, 4, 761–784. DOI: 10.1287/opre.23.4.761.\n\n\nWhittle, P. 1982. Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2. Wiley."
  },
  {
    "objectID": "mdps/mobile-edge-computing.html#structure-of-the-optimal-policy",
    "href": "mdps/mobile-edge-computing.html#structure-of-the-optimal-policy",
    "title": "13  Service Migration in Mobile edge computing",
    "section": "13.1 Structure of the optimal policy",
    "text": "13.1 Structure of the optimal policy\nWe provide a basic characterization of the optimal policy.\n\nProposition 13.1 Let \\(π^*\\) denote the optimal policy. Then for any \\((x,s) \\in \\ALPHABET X × \\ALPHABET S\\), we have \\[ \\| x - π^*(x,s) \\| \\le \\| x - s \\|. \\]\n\nProposition 13.1 states that the optimal policy always migrates the user to a server which is closer than the one already serving the user.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result using an interchange argument. Suppose we are given a service migration policy \\(π\\) such that the service is migrated to a location farther away from the user, i.e., \\(\\|x - a\\| > \\| x - s \\|\\). We will show that for an arbitrary sample path of the user locations \\(\\{ x_t\\}_{t \\ge 1}\\), we can find a (possibly history dependent) policy \\(μ\\) that does not migrate to locations further away from the user in any time slot and performs no worse than policy \\(π\\).\nGiven a arbitrary sample path of user locations \\(\\{x_t\\}_{t \\ge 1}\\) let \\(t_0\\) denote the first timeslot in which the service is migrated to somewhere farther away from the user when following policy \\(π\\). The state of \\(t_0\\) is \\((x_{t_0}, s_{t_0})\\) and the policy \\(π\\) moves the service to server \\(a_{t_0} = π(x_{t_0}, s_{t_0})\\), where \\(\\|x_{t_0} - a_{t_0}\\| > \\| x_{t_0} - s_{t_0} \\|\\). Let \\(\\{a^π_t\\}_{t \\ge t_0}\\) denote the subsequent locations of the server (after migration) under policy \\(π\\).\nNow, we define a policy \\(μ\\) such that the following conditions are satisfied for the given sample path \\(\\{x_t\\}_{t \\ge 1}\\) of the user locations as follows. The policy \\(μ\\) chooses the same migration actions as policy \\(π\\) in timeslots \\(t < t_0\\).\nNow, suppose \\[\\begin{equation} \\label{eq:1}\n  \\| x_{t} - s^π_{t_0} \\| \\le \\| x_{t} - a^π_{t} \\|, \\quad \\forall t > t_0.\n\\end{equation}\\] Then, the policy \\(μ\\) does not choose any migrations from time \\(t_0\\) onwards. Hence, \\(a^h_t = s^π_{t_0}\\) for all \\(t \\ge t_0\\). Note that from time \\(t_0\\) onwards, policy \\(μ\\) doesn’t incur any migration cost and always incurs a transmission cost which is less than \\(π\\). Hence, policy \\(μ\\) performs at least as well as policy \\(π\\).\nNow suppose \\eqref{eq:1} does not hold. Then define \\(t_m\\) to be the first timeslot after \\(t_0\\) such that \\[\n  \\| x_{t_m} - s^π_{t_0} \\| > \\| x_{t_m} - a^π_{t_m} \\|.\n\\]\nNow, we define policy \\(μ\\) as a policy which does not specify any migrations for time \\(t \\in [t_0, t_m - 1]\\), migrates to location \\(a^π_{t_m}\\) at timeslot \\(t_m\\), and follows policy \\(π\\) from \\(t_m\\) onwards.\nNote that policies \\(π\\) and \\(μ\\) agree on \\([1, t_0 -1]\\) and \\([t_m + 1, ∞)\\). In the interval \\([t_0, t_m]\\), \\[\n  \\| x_{t} - a^h_t \\| \\le \\| x_t - a^π_t \\|.\n\\] Thus, the transmission cost of policy \\(μ\\) is no more than the transmission cost of policy \\(π\\).\nNow, the migration cost incurred by policy \\(π\\) in the interval \\([t_0, t_m]\\) can be lower bounded by the migration cost incurred by policy \\(μ\\) as follows: \\[\\begin{align*}\n  \\hskip 2em & \\hskip -2em\n  γ^{t_0 - 1}  b(\\| s^π_{t_0} - a^π_{t_0} \\|) +\n  γ^{t_0 } b(\\| a^π_{t_0} - a^π_{t_0 + 1} \\|) + \\cdots  +\n  γ^{t_m - 1} b(\\| a^π_{t_m -1} - a^π_{t_m} \\|) \\\\\n  &\\ge\n  γ^{t_m - 1}\\bigl[\n    b(\\| s^π_{t_0} - a^π_{t_0} \\|) +\n   b(\\| a^π_{t_0} - a^π_{t_0 + 1} \\|) + \\cdots  +\n   b(\\| a^π_{t_m -1} - a^π_{t_m} \\|)  \\bigr]\n  \\\\\n  &\\ge\n  γ^{t_m - 1} b(\\| s^π_{t_0} - a^π_{t_m} \\|),\n\\end{align*}\\] where the first inequality follows because \\(γ < 1\\) and the second follows from the triangle inequality.\nHence, policy \\(μ\\) performs at least as well as policy \\(π\\). The above procedure can be repeated so that all the mitigation actions to a location farther away from the user can be removed without increasing the overall cost. \nNote that the policy \\(μ\\) constructed above is a history dependent policy. From the result for infinite horizon MDP, we know that a history dependent policy cannot outperform Markovian policies. Therefore, there exists a Markovian policy that does not migrate to a location farther away from the user, which does not perform worse than \\(π\\)."
  },
  {
    "objectID": "mdps/mobile-edge-computing.html#notes",
    "href": "mdps/mobile-edge-computing.html#notes",
    "title": "13  Service Migration in Mobile edge computing",
    "section": "Notes",
    "text": "Notes\nThe model and results presented here are taken from Wang et al. (2019). See Urgaonkar et al. (2015) for a variation of this model.\n\n\n\n\nUrgaonkar, R., Wang, S., He, T., Zafer, M., Chan, K., and Leung, K.K. 2015. Dynamic service migration and workload scheduling in edge-clouds. Performance Evaluation 91, 205–228. DOI: 10.1016/j.peva.2015.06.013.\n\n\nWang, S., Urgaonkar, R., Zafer, M., He, T., Chan, K., and Leung, K.K. 2019. Dynamic service migration in mobile edge computing based on Markov decision process. IEEE/ACM Transactions on Networking 27, 3, 1272–1288. DOI: 10.1109/tnet.2019.2916577."
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#span-norm-contraction",
    "href": "mdps/computational-complexity-vi.html#span-norm-contraction",
    "title": "14  Computational complexity of value interation",
    "section": "14.1 Span norm contraction",
    "text": "14.1 Span norm contraction\nLet \\(\\SPAN(v) = \\max(v) - \\min(v)\\) denotes the span semi-norm. We start by stating some basic properties of span semi-norm.\n\n\\(\\SPAN(v) \\ge 0\\) for all \\(v \\in \\reals^n\\)\n\\(\\SPAN(v + w) \\le \\SPAN(v) + \\SPAN(w)\\) for all \\(v, w \\in \\reals^n\\).\n\\(\\SPAN(m v) \\le |m| \\SPAN(v)\\) for all \\(v \\in \\reals^n\\) and \\(m \\in \\reals\\).\n\\(\\SPAN(v + m \\mathbf{1}) = \\SPAN(v)\\) for all \\(m \\in \\reals\\).\n\\(\\SPAN(v) = \\SPAN(-v)\\).\n\\(\\SPAN(v) \\le 2\\|v\\|\\).\n\nProperties 1–3 imply that \\(\\SPAN(v)\\) is a semi-norm. However, it is not a norm because of property 4; that is, \\(\\SPAN(v) = 0\\) does not imply that \\(v = 0\\). If \\(\\SPAN(v) = 0\\), then \\(v = m \\mathbf{1}\\) for some scalar \\(m\\).\nA basic result for our analysis is the following:\n\nProposition 14.1 Let \\(v \\in \\reals^n\\) and \\(P\\) be any matrix of compatible dimensions. Then, \\[ \\SPAN(P v) \\le β_P \\SPAN(v), \\] where \\[\\begin{equation} \\label{eq:span-matrix}\n  β_P = 1 - \\min_{s, s' \\in \\ALPHABET S}\n  \\sum_{z \\in \\ALPHABET S} \\min\\{ P_{sz}, P_{s'z} \\}.\n\\end{equation}\\] Furhermore, \\(β_P \\in [0, 1]\\) and there exists a \\(v \\in \\reals^n\\) such that \\(\\SPAN(Pv) = β_P \\SPAN(v)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nNote that for any \\(v \\in \\reals^n\\) \\[\\begin{align*}\n\\SPAN(Pv) &= \\max_{s \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S} P_{sz} v_z\n- \\min_{s' \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z \\\\\n&= \\max_{s, s' \\in \\ALPHABET S}\n  \\sum_{z \\in \\ALPHABET S} P_{sz} v_z - \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z\n\\end{align*}\\] Let \\(B(z; s,s') = \\min\\{ P_{sz}, P_{s'z} \\}\\). Then consider \\[\\begin{align*}\n  \\sum_{z \\in \\ALPHABET S} P_{sz} v_z - \\sum_{z \\in \\ALPHABET S} P_{s'z} v_z\n  &=\n  \\sum_{z \\in \\ALPHABET S} [ P_{sz} - B(z; s,s') ] v_z -\n  \\sum_{z \\in \\ALPHABET S} [ P_{s'z} - B(z; s,s') ] v_z \\\\\n  &\\le\n  \\sum_{z \\in \\ALPHABET S} [ P_{sz} - B(z; s,s') ] \\max(v) -\n  \\sum_{z \\in \\ALPHABET S} [ P_{s'z} - B(z; s,s') ] \\min(v)\n  \\\\\n  &= \\biggl[ 1 - \\sum_{z \\in \\ALPHABET S} B(z; s, s') \\biggr] \\SPAN(v).\n\\end{align*}\\] Hence, \\[ \\SPAN(Pv) \\le \\max_{s, s' \\in \\ALPHABET S} \\biggl[\n  1 - \\sum_{z \\in \\ALPHABET S} B(z; s, ) \\biggr] \\SPAN(v). \\]\nNow we show that there exists a \\(v\\) such that \\eqref{eq:span-matrix} holds with equality. If \\(β_P = 0\\), then \\(P\\) has equal rows, so that \\(\\SPAN(Pv) = 0 = 0 \\cdot \\SPAN(v)\\) for all \\(v \\in \\reals^n\\). Suppose \\(β_P > 0\\). Using the identity \\([ a - b]^{+} = a - \\min(a,b)\\), we can write \\[\nβ_P = \\max_{s,s' \\in \\ALPHABET S}\n\\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{sz} - P_{s'z} \\bigr]^{+}.\n\\] Let \\(s^*\\) and \\(s'^*\\) be such that \\[\n  β_P = \\sum_{z \\in \\ALPHABET S}  \n\\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{s^*z} - P_{s'^*z} \\bigr]^{+}.\n\\] Define \\(v\\) by \\[\n  v_z = \\IND\\{ P_{s^*z} > P_{s'^*z} \\}.\n\\] Then, note that \\(\\SPAN(v) = 1\\) and \\[\\begin{align*}\n  \\SPAN(Pv) &\\ge\n  \\sum_{z \\in \\ALPHABET S} P_{s^* z} v_z -\n  \\sum_{z \\in \\ALPHABET S} P_{s'^* z} v_z \\\\\n  &=\n  \\sum_{z \\in \\ALPHABET  S} \\bigl[ P_{s^*z} - P_{s'^*z} \\bigr]^{+}\n  \\\\\n  &= β_P \\SPAN(v).\n\\end{align*}\\] Combining with \\eqref{eq:span-matrix}, we get \\(\\SPAN(Pv) = β_P \\SPAN(v)\\).\n\n\n\nProposition 14.1 illustrates the “averaging” property of a transition matrix. By multiplying a vector by a transition matrix, the resulting vector has components which are more nearly equal. When \\(P\\) is a square matrix, the quantity \\(β_P\\) is called the ergodicity coefficient, which is often written in an alternative form by using the relation \\(|a - b| = (a + b) - 2 \\min(a,b)\\): \\[\n  β_P = \\frac12\n  \\max_{s,s' \\in \\ALPHABET S} \\sum_{z \\in \\ALPHABET S}\n  \\bigl| P_{sz} - P_{s'z} \\bigr|. \\] The ergodicity coefficient is an upper bound on the second largest eigenvalue of \\(P\\). \\(β_P\\) equals \\(0\\) if all rows of \\(P\\) are equal and equals \\(1\\) if at least two rows of \\(P_d\\) are orthogonal. From a different perspective, \\(β_P < 1\\) if for each pair of states there exists at least one state which they both can reach with positive probability in one step.\n\n\n\n\n\n\nRemark\n\n\n\nNote that \\[\\begin{equation}\\label{eq:ergodicity-bound}\nβ_P \\le 1 - \\sum_{z \\in \\ALPHABET S} \\min_{s \\in \\ALPHABET S} P_{sz}\n=: β_P'\n\\end{equation}\\] which is easier to compute.\n\n\nDefine the contraction factor \\[\\begin{equation}\\label{eq:contraction}\n  β = \\max_{\\substack{ s,s' \\in \\ALPHABET S \\\\ a \\in \\ALPHABET A(s), w \\in\n  \\ALPHABET A(s')}}\n  \\biggl[ 1 - \\sum_{z \\in \\ALPHABET S}\n    \\min\\{ P(z | s,a), P(z | s', w) \\biggr].\n\\end{equation}\\] Note that \\(β \\in [0, 1]\\).\n\nTheorem 14.1 For any \\(V_1, V_2 \\in \\reals^n\\), \\[ \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) \\le γ β\\, \\SPAN(V_1 - V_2). \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π_i\\) be such that \\(\\mathcal B V_i = \\mathcal B_{π_i}V_i\\). Let \\[\\begin{align*}\n  s^* &= \\arg \\max_{s \\in \\ALPHABET S}(\\mathcal B V_1(s) - \\mathcal B V_2(s)),\n  \\\\\n  s_* &= \\arg \\min_{s \\in \\ALPHABET S}(\\mathcal B V_1(s) - \\mathcal B V_2(s)),\n\\end{align*}\\] Then, \\[\n  \\mathcal B V_1(s^*) - \\mathcal B V_2(s^*) \\le\n  \\mathcal B_{π_2} V_1(s^*) - \\mathcal B_{π_2} V_2(s^*)\n  = γ P_{π_2}(V_1 - V_2)(s^*)\n\\] and \\[\n  \\mathcal B V_1(s_*) - \\mathcal B V_2(s_*) \\ge\n  \\mathcal B_{π_1} V_1(s^*) - \\mathcal B_{π_1} V_2(s^*)\n  = γ P_{π_1}(V_1 - V_2)(s^*).\n\\] Therefore, \\[\\begin{align*}\n  \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) &\\le\n  γ P_{π_2}(V_1 - V_2)(s^*) - γ P_{π_1}(V_1 - V_2)(s_*) \\\\\n  &\\le \\max_{s \\in \\ALPHABET S} γ P_{π_2} (V_1 - V_2)(s) -\n  \\min_{s \\in \\ALPHABET S} γ P_{π_1}(V_1 - V_2)(s)\n  \\\\\n  &\\le \\SPAN(γ\\, \\ROWS(P_{π_2}, P_{π_1})(V_1 - V_2).\n\\end{align*}\\] By applying Proposition 14.1, we get \\[ \\SPAN(\\mathcal B V_1 - \\mathcal B V_2) \\le γβ_{\\bar P} \\SPAN(V_1 - V_2), \\] where \\(β_{\\bar P}\\) is given by \\eqref{eq:span-matrix} with \\(\\bar P = \\ROWS(P_{π_2}, P_{π_1})\\). The result follows by noting that \\(β_{\\bar P}\\) is at most \\(β\\)."
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#computational-complexity-of-value-iteration",
    "href": "mdps/computational-complexity-vi.html#computational-complexity-of-value-iteration",
    "title": "14  Computational complexity of value interation",
    "section": "14.2 Computational complexity of value iteration",
    "text": "14.2 Computational complexity of value iteration\nNote that \\(β \\in [0, 1]\\). We will first rule out the case \\(β = 0\\). If \\(β = 0\\), then \\(P(z | s, a) = P(z | s', w)\\) for all \\(s, s', z \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A(s)\\) and \\(w \\in \\ALPHABET A(s')\\), which implies that all deterministic policies have the same transition probabilities. Therefore, a deterministic policy \\(π\\) is optimal if and only if it minimize the one-step cost. Thus, the case with \\(β = 0\\) is trivial. So, in the rest of the analysis, we assume that \\(β \\in (0, 1)\\).\n\nTheorem 14.2 Start with an abritrary \\(V_0\\). If \\(Δ_1 = 0\\), then we obtain an optimal policy in iteration 1. Otherwise, for any \\(ε > 0\\), value iteration finds an \\(ε\\)-optimal policy in no more than \\(K^*(γ)\\) iterations, where \\[\\begin{equation}\\label{eq:K*}\n  K^*(γ) =  \\left\\lceil\n  \\frac{ \\log \\frac{(1-γ) ε β}{Δ_1} } {\\log(γβ)}\n  \\right\\rceil.\n\\end{equation}\\] In addition, each iteration uses at most \\(O(nM)\\) operations.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf follows from the definition of Bellman operator that each iteration uses at most \\(O(nM)\\) iterations (to compute the \\(Q\\) function, for each state-action pair, we need to compute a sum over \\(n\\) terms).\nFrom Theorem 14.1, we get that \\(Δ_k \\le (γβ)^{k-1} Δ_1\\). Therefore, the minimum number of iterations required to achieve \\(Δ_k \\le \\frac{1-γ}{γ}ε\\) is given \\(K^*(γ)\\). \\(\\Box\\)\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that finding the value of \\(β\\) requires computing the sum in \\eqref{eq:contraction} for all couples \\(\\{ (s,a), (s',b) \\}\\) of state action pairs such that \\((s,a) \\neq (s',b)\\). The total number of such pairs are \\(M(M-1)/2 = O(M^2)\\). Therefore, the number of arithematic operators in \\eqref{eq:contraction}, which are additions, is \\(n\\) for each couple. Therefore, computation of \\(β\\) requires \\(O(nM^2)\\) operations, which can be significantly larger than the complexity of computing an \\(ε\\)-optimal policy which is given by Theorem 14.2! Based on \\eqref{eq:ergodicity-bound} we can replace \\(β\\) by in \\eqref{eq:K*} by \\[\n  β' = 1 - \\sum_{z \\in \\ALPHABET S} \\min_{s \\in \\ALPHABET S, a \\in \\ALPHABET A}\n       P(z | s,a)\n\\] which requies \\(O(nM)\\) operations."
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#the-bound-may-be-exact",
    "href": "mdps/computational-complexity-vi.html#the-bound-may-be-exact",
    "title": "14  Computational complexity of value interation",
    "section": "14.3 The bound may be exact",
    "text": "14.3 The bound may be exact\nWe now present an example (due to Feinberg and He (2020)) to show that the bound in Theorem 14.2 may be exact. Consider an MDP with \\(\\ALPHABET S = \\{1, 2, 3\\}\\), \\(\\ALPHABET A = \\{1,2 \\}\\), with \\(\\ALPHABET A(1) = \\{1, 2\\}\\) and \\(\\ALPHABET A(2) = \\ALPHABET A(3) = \\{1\\}\\). The per-step transitions are \\[\n  P(1) = \\MATRIX{0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 },\n  \\quad\\text{and}\\quad\n  P(2) = \\MATRIX{0 & 1 & 0 \\\\ * & * & * \\\\ * & * & *},\n\\] where \\(*\\) indicates that the corresponding action is infeasible. The reward matrix is \\[\n  r = \\MATRIX{1 & 0 \\\\ 1 & * \\\\ -1 & *}.\n\\]\nSuppose we start with an initial \\(V_0 = \\VEC(1, 2, -2)\\). Then elementary calculations show that \\[\n  V_k = \\MATRIX{ γ^k \\\\ γ^k \\\\ -γ^k} +\n  \\MATRIX{ \\sum_{\\ell = {\\color{red} 1}}^{k} γ^\\ell \\\\\n           \\sum_{\\ell = 0}^{k} γ^\\ell \\\\\n           \\sum_{\\ell = 0}^{k} γ^\\ell }.\n\\] Thus, \\[\n  V_k - V_{k-1} = \\MATRIX{\n  2 γ^k - γ^{k-1} \\\\\n  2 γ^k - γ^{k-1} \\\\\n  - 2 γ^k + γ^{k-1} }.\n\\] Hence, \\[\n\\SPAN(V_k - V_{k-1}) = 2γ^{k-1} |2γ - 1| = γ^{k-1} \\SPAN(V_1 - V_0).\n\\]\nThus, for this model, the expression \\eqref{eq:K*} is exact.\n\n\n\n\n\n\nRemark\n\n\n\nThe exact number of iterations need not be monotone in \\(γ\\)! In the above example, let \\(ε = 0.02\\), then \\[\n  K^*(0.24) = 3, \\quad\n  K^*(0.47) = 4, \\quad\n  K^*(0.48) = 3.\n\\]\nThus, the number of iterations is not monotone in \\(γ\\)."
  },
  {
    "objectID": "mdps/computational-complexity-vi.html#notes",
    "href": "mdps/computational-complexity-vi.html#notes",
    "title": "14  Computational complexity of value interation",
    "section": "Notes",
    "text": "Notes\nThe discussion on span semi-norm and Theorem 14.1 is from Puterman (2014). Theorem 14.2 is from Feinberg and He (2020).\n\n\n\n\nFeinberg, E.A. and He, G. 2020. Complexity bounds for approximately solving discounted MDPs by value iterations. Operations Research Letters. DOI: 10.1016/j.orl.2020.07.001.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887."
  },
  {
    "objectID": "mdps/linear-programming.html#constrained-mdps",
    "href": "mdps/linear-programming.html#constrained-mdps",
    "title": "15  Linear programming formulation",
    "section": "15.1 Constrained MDPs",
    "text": "15.1 Constrained MDPs\nSuppose, in addition to the per-step cost function \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\to \\reals\\), we have a per-step constraint function \\(d \\colon \\ALPHABET S \\times \\ALPHABET A \\to \\reals\\) and we are interested in the following constrained optimization problems:\n\\[\\begin{gather*}\n\\min \\EXP\\Bigl[ \\sum_{t=1}^\\infty γ^{t-1} c(S_t, A_t) \\Bigr] \\\\\n\\text{subject to}\\quad\n\\EXP\\Bigl[ \\sum_{t=1}^\\infty γ^{t-1} d(S_t, A_t) \\Bigr] \\le D.\n\\end{gather*}\\]\nThe dual LP in this case is given by \\[\\begin{gather}\n  \\min \\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A} \\mu(s,a) c(s,a)\n  \\notag \\\\\n  \\text{subject to}\\quad\n  \\mu(s,a) - γ\\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A}\n  P(s | z, a) \\mu(s,a) = p(s), \\quad \\forall s \\in \\ALPHABET S, \\notag \\\\\n  \\sum_{s \\in \\ALPHABET S} \\sum_{a \\in \\ALPHABET A} \\mu(s,a) d(s,a) \\le D \\notag \\\\\n  \\mu(s,a) \\ge 0, \\quad \\forall s \\in \\ALPHABET S, a \\in \\ALPHABET A.\n\\end{gather}\\]\nIf we interpret \\(\\mu(s,a)\\) as the occupation measure of any policy, then this formulation follows immediately."
  },
  {
    "objectID": "mdps/linear-programming.html#notes",
    "href": "mdps/linear-programming.html#notes",
    "title": "15  Linear programming formulation",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Puterman (2014). See Altman (1999) for a detailed treatment of constrained MDPs.\n\n\n\n\nAltman, Eitan. 1999. Constrained markov decision processes. CRC Press. Available at: http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#preliminaries",
    "href": "mdps/lipschitz-mdps.html#preliminaries",
    "title": "16  Lipschitz MDPs",
    "section": "16.1 Preliminaries",
    "text": "16.1 Preliminaries\n\nLipschitz continuous functions\nGiven two metric spaces \\((\\ALPHABET X, d_X)\\) and \\((\\ALPHABET Y, d_Y)\\), the Lipschitz constant of function \\(f \\colon \\ALPHABET X \\to \\ALPHABET Y\\) is defined by \\[ \\| f\\|_{L} = \\sup_{x_1 \\neq x_2}\n    \\left\\{ \\frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } :\n    x_1, x_2 \\in \\ALPHABET X \\right\\} \\in [0, ∞]. \\] The function is called Lipschitz continuous if its Lipschitz constant is finite.\nIntuitively, a Lipschitz continuous function is limited by how fast it can change. For example, the following image from Wikipedia shows that for a Lipschitz continuous function, there exists a double cone (white) whose origin can be moved along the graph so that the whole graph always stays outside the double cone.\n\n\n\n\n\nImage credit: https://en.wikipedia.org/wiki/File:Lipschitz_Visualisierung.gif\n\n\nLet \\(\\ALPHABET Z\\) be an arbitrary set. A function \\(f \\colon \\ALPHABET X × \\ALPHABET Z \\to \\ALPHABET Y\\) is said to be uniformly Lipschitz in \\(u\\) if \\[ \\sup_{z \\in \\ALPHABET Z} \\| f(\\cdot, z) \\|_L  =\n  \\sup_{z \\in \\ALPHABET Z} \\sup_{x_1 \\neq x_2}\n  \\dfrac{ d_Y(f(x_1,z), f(x_2, z)) }{ d_X(x_1, x_2) } < ∞. \\]\n\n\nSome examples\nA function \\(f \\colon \\reals \\to \\reals\\) is Lipschitz continuous if and only if it has bounded first derivative. The Lipschitz constant of such a function is equal to the maximum absolute value of the derivative.\nHere are some examples of Lipschitz continuous functions:\n\nThe function \\(f(x) = \\sqrt{x^2 + 1}\\) defined over \\(\\reals\\) is Lipschitz continuous because it is everywhere differentiable and the maximum value of the derivative is \\(L = 1\\).\nThe function \\(f(x) = |x|\\) defined over \\(\\reals\\) is Lipschitz continuous with Lipschitz constant equal to \\(1\\). Note that this function is continuous but not differentiable.\nThe function \\(f(x) = x + \\sin x\\) defined over \\(\\reals\\) is Lipschitz continuous with a Lipschitz constant equal to \\(1\\).\nThe function \\(f(x) = \\sqrt{x}\\) defined over \\([0,1]\\) is not Lipschitz continuous because the function becomes infinitely steep as \\(x\\) approaches \\(0\\).\nThe function \\(f(x) = x^2\\) defined over \\(\\reals\\) is not Lipschitz continuous because it becomes arbitrarily steep as \\(x\\) approaches infinity.\nThe function \\(f(x) = \\sin(1/x)\\) is bounded but not Lipschitz because becomes infinitely steep as \\(x\\) approaches \\(0\\).\n\n\n\nProperties of Lipschitz functions\n\nProposition 16.1 Lipschitz continuous functions have the following properties:\n\nIf a function \\(f \\colon (\\ALPHABET X, d_X) \\to (\\ALPHABET Y, d_Y)\\) is Lipschitz continuous, then \\(f\\) is uniformly continuous and measurable.\n\\(\\| f\\|_L = 0\\) if and only if \\(f\\) is a constant.\nIf \\(f \\colon (\\ALPHABET X, d_X) \\to (\\ALPHABET Y, d_Y)\\) and \\(g \\colon (\\ALPHABET Y, d_Y) \\to (\\ALPHABET Z, d_Z)\\) are Lipschitz continuous, then \\[ \\| f \\circ g \\|_L \\le \\| f \\|_L \\cdot \\| g \\|_L. \\]\nThe \\(\\| \\cdot \\|_{L}\\) is a seminorm on the vector space of Lipschitz functions from a metric space \\((\\ALPHABET X, d_X)\\) to \\((\\ALPHABET Y, d_Y)\\). In particular, \\(\\| \\cdot \\|_L\\) has the following properties: \\(\\| f \\|_L \\in [0, ∞]\\), \\(\\| α f\\|_L = |α| \\cdot \\|f\\|_L\\) for any \\(α \\in \\reals\\), and \\(\\| f_1 + f_2 \\|_L \\le \\|f_1 \\|_L + \\|f_2 \\|_L\\).\nGiven a family of functions \\(f_i\\), \\(i \\in I\\), on the same metric space such that \\(\\sup_{i \\in I} f_i < ∞\\), \\[ \\| \\sup_{i \\in I} f_i \\|_{L} \\le \\sup_{i \\in I} \\| f_i \\|_{L}. \\]\nLet \\(f_n\\), \\(n \\in \\integers_{\\ge 1}\\), and \\(f\\) be functions from \\((\\ALPHABET X, d_X)\\) to \\((\\ALPHABET Y, d_Y)\\). If \\(f_n\\) converges pointwise to \\(f\\) for \\(n \\to ∞\\), then \\[ \\| f \\|_{L} \\le \\lim\\inf_{n \\to ∞} \\| f_i \\|_{L}. \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#kantorovich-distance",
    "href": "mdps/lipschitz-mdps.html#kantorovich-distance",
    "title": "16  Lipschitz MDPs",
    "section": "16.2 Kantorovich distance",
    "text": "16.2 Kantorovich distance\nLet \\(\\mu\\) and \\(\\nu\\) be probability measures on \\((\\ALPHABET X, d_X)\\). The Kantorovich distance between distributions \\(\\mu\\) and \\(\\nu\\) is defined as: \\[ K(\\mu,\\nu) = \\sup_{f : \\| f\\|_L \\le 1 }\n   \\left| \\int_{\\ALPHABET X} f\\, d\\mu - \\int_{\\ALPHABET X} f\\, d\\nu \\right|. \\]\nThe next results follow immediately from the definition of Kantorovich distance.\n\nProposition 16.2 For any Lipschitz function \\(f \\colon (\\ALPHABET X, d_X) \\to (\\reals, \\lvert \\cdot \\rvert)\\), and \\(μ,ν\\) are probability measures on \\((\\ALPHABET X, d_X)\\), \\[ \\left|\n  \\int_{\\ALPHABET X} f\\, dμ - \\int_{\\ALPHABET X} f\\, dν \\right| \\le\n  \\| f \\|_L \\cdot K(μ,ν). \\]\n\n\nSome examples\n\nLet \\((\\ALPHABET X, d_X)\\) be a metric space and for any \\(x,y \\in \\ALPHABET X\\), let \\(δ_x\\) and \\(δ_y\\) denote the Dirac delta distributions centered at \\(x\\) and \\(y\\). Then, \\[ K(δ_x, δ_y) = d_X(x,y). \\]\nLet \\((\\ALPHABET X, d_X)\\) be a Euclidean space with Euclidean norm. Let \\(μ \\sim \\mathcal{N}(m_1, \\Sigma_1)\\) and \\(ν \\sim \\mathcal{N}(m_2, \\Sigma_2)\\) be two Gaussian distributions on \\(\\ALPHABET X\\). Then, \\[K(μ,ν) = \\sqrt{ \\| m_1 - m_2 \\|_2^2\n+ \\text{Tr}( \\Sigma_1 + \\Sigma_2 - 2(\\Sigma_2^{1/2} \\Sigma_1 \\Sigma_2^{1/2})^{1/2} ) }. \\] If the two covariances commute, i.e. \\(\\Sigma_1\\Sigma_2 = \\Sigma_2 \\Sigma_1\\), then, \\[K(μ,ν) = \\sqrt{ \\| m_1 - m_2 \\|_2^2\n+ \\| \\Sigma_1^{1/2} - \\Sigma_2^{1/2} \\|^2_F},\\] where \\(\\| ⋅ \\|_{F}\\) denotes the Frobeinus norm of a matrix.\nWhen \\(\\Sigma_1 = \\Sigma_2\\), we have \\[K(μ,ν) = \\| m_1 - m_2 \\|_2. \\]\nIf \\(\\ALPHABET X = \\reals\\) and \\(d_X = | \\cdot |\\), then for any two distributions \\(μ\\) and \\(ν\\), \\[ K(μ,ν) = \\int_{-∞}^∞ \\left| F_μ(x) - F_ν(x) \\right| dx, \\] where \\(F_μ\\) and \\(F_ν\\) denote the CDF of \\(μ\\) and \\(ν\\).\nFurthermore, if \\(μ\\) is stochastically dominated by \\(ν\\), then \\(F_μ(x) \\ge F_ν(x)\\). Thus, \\[ K(μ, ν) = \\bar μ - \\bar ν \\] where \\(\\bar μ\\) and \\(\\bar ν\\) are the means of \\(μ\\) and \\(ν\\)."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#lipschitz-mdps",
    "href": "mdps/lipschitz-mdps.html#lipschitz-mdps",
    "title": "16  Lipschitz MDPs",
    "section": "16.3 Lipschitz MDPs",
    "text": "16.3 Lipschitz MDPs\nNow consider an MDP where the state and action spaces are Metric spaces. We denote the corresponding metric by \\(d_S\\) and \\(d_A\\) respectively. For ease of exposition, we define a metric \\(d\\) on \\(\\ALPHABET S × \\ALPHABET A\\) by \\[ d( (s_1, a_1), (s_2, a_2) ) = d_S(s_1, s_2) + d_A(a_1, a_2). \\]\nWe allow for randomized policies. Thus, given any state \\(s \\in \\ALPHABET S\\), \\(π(\\cdot | s)\\) is a probability distribution on \\(\\ALPHABET A\\). We say that a (possibly) randomized policy \\(π\\) has a Lipschitz constant of \\(L_π\\) if for any \\(s_1, s_2 \\in \\ALPHABET S\\), \\[ K(π(\\cdot| s_1), π(\\cdot | s_2)) \\le L_π d_S(s_1, s_2). \\]\nNote that if \\(π\\) is deterministic, then due to property of Kantorovich distance between delta distributions, the above relationship simplifies to \\[ d_A(π(s_1), π(s_2)) \\le L_π d_S(s_1, s_2). \\]\n\nDefinition 16.1 An MDP is \\((L_c, L_p)\\)-Lipschitz if for all \\(s_1, s_2 \\in \\ALPHABET S\\) and \\(a_1, a_2 \\in \\ALPHABET A\\),\n\n\\(| c(s_1, a_1) - c(s_2, a_2) | \\le L_c\\bigl( d_S(s_1, s_2) + d_A(a_1, a_2) \\bigr)\\).\n\\(K(p(\\cdot | s_1, a_1), p(\\cdot | s_2, a_2)) \\le L_p\\bigl( d_S(s_1, s_2) + d_A(a_1, a_2) \\bigr)\\).\n\n\n\nLipschitz continuity of Bellman updates\nWe now prove a series of results for the Lipschitz continuity of Bellman updates.\n\nLemma 16.1 Let \\(V \\colon \\ALPHABET S \\to \\reals\\) be \\(L_V\\)-Lipschitz continuity. Define \\[ Q(s,a) = c(s,a) + γ \\int V(y) p(y|s,a)dy. \\] Then \\(Q\\) is \\((L_c + γ L_p L_V)\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider, \\[\\begin{align*}\n| Q(s_1, a_1) - Q(s_2, a_2) | &\\stackrel{(a)}\\le\n| c(s_1, a_1) - c(s_2, a_2) | \\\\\n& \\quad +\n\\beta \\left|\\int V(y) p(y|s_1, a_1) dy -\n             \\int V(y) p(y|s_2, a_2) dy \\right|\n  \\\\\n  &\\stackrel{(b)}\\le  L_c d( (s_1, a_1), (s_2, a_2) ) +\n  \\beta L_V L_p d( (s_1, a_1), (s_2, a_2) ),\n\\end{align*}\\] where \\((a)\\) follows from the triangle inequality and \\((b)\\) follows from Proposition 16.2. Thus, \\(L_Q = L_c + γ L_p L_V\\).\n\n\n\n\nLemma 16.2 Let \\(Q \\colon \\ALPHABET S × \\ALPHABET A \\to \\reals\\) be \\(L_Q\\)-Lipschitz continuous. Define \\[V(s) = \\min_{a \\in \\ALPHABET A} Q(s,a).\\] Then \\(V\\) is \\(L_Q\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s_1, s_2 \\in \\ALPHABET S\\) and let \\(a_1\\) and \\(a_2\\) denote the corresponding optimal action. Then, \\[ \\begin{align*}\nV(s_1) - V(s_2) &= Q(s_1, a_1) - Q(s_2, a_2) \\\\\n&\\stackrel{(a)}\\le Q(s_1, a_2) - Q(s_2, a_2) \\\\\n&\\stackrel{(b)}\\le L_Q( d_S(s_1, s_2) + d_A(a_2, a_2) )\\\\\n&= L_Q d_S(s_1, s_2).\n\\end{align*} \\]\nBy symmetry, \\[ V(s_2) - V(s_1) \\le L_Q d_S(s_2, s_1). \\] Thus, \\[ | V(s_1) - V(s_2) | \\le L_Q d_S(s_1, s_2). \\] Thus, \\(V\\) is \\(L_Q\\)-Lipschitz continuous.\n\n\n\n\nLemma 16.3 Let \\(Q \\colon \\ALPHABET S × \\ALPHABET A \\to \\reals\\) be \\(L_Q\\)-Lipschitz continuous and \\(π\\) be a (possibly randomized) \\(L_π\\)-Lipschitz policy. Define \\[V_π(s) = \\int Q(s, a) π(a | s) du.\\] Then, \\(V_π\\) is \\(L_Q( 1 + L_π)\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(s_1, s_2 \\in \\ALPHABET S\\), consider \\[ \\begin{align}\n| V_π(s_1) - V_π(s_2) | &=\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_2) du \\right|\n\\notag \\\\\n&\\stackrel{(a)}\\le\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_1) du \\right|\n\\notag \\\\\n& \\quad +\n\\left| \\int Q(s_2, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_2) du \\right|\n\\label{eq:split}\n\\end{align} \\] where \\((a)\\) follows from the triangle inequality. Now we consider both terms separately.\nThe first term of \\eqref{eq:split} simplifies as follows: \\[\\begin{align}\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_1) du \\right|\n&\\stackrel{(b)}\\le\n\\int \\left|Q(s_1, a) - Q(s_2, a)\\right| π(a | s_1) du \\notag \\\\\n&\\stackrel{(c)}\\le\n\\int L_Q d_S(s_1, s_2) π(a | s_1) du \\notag \\\\\n&= L_Q d_S(s_1, s_2), \\label{eq:first}\n\\end{align} \\] where \\((b)\\) follows from the triangle inequality and \\((c)\\) follows from Lipschitz continuity of \\(Q\\).\nThe second term of \\eqref{eq:split} simplifies as follows: \\[ \\begin{align}\n  \\left| \\int Q(s, a) π(a | s_1) du - \\int Q(s,a) π(a | s_2) du \\right|\n  &\\stackrel{(d)}\\le L_Q K (π(\\cdot | s_1), π(\\cdot | s_2))\n  \\notag \\\\\n  &\\stackrel{(e)}\\le L_Q L_π d_S(s_1, s_2),\n  \\label{eq:second}\n  \\end{align}\n\\] where the \\((d)\\) inequality follows from Proposition 16.2 and \\((e)\\) follows from the definition of Lipschitz continuous policy.\nSubstituting \\eqref{eq:first} and \\eqref{eq:second} in \\eqref{eq:split}, we get \\[ \\begin{align*}\n| V_π(s_1) - V_π(s_2) | &\\le L_Q d_S(s_1, s_2) + L_Q L_π d_S(s_1, s_2)\n\\\\\n&= L_Q(1 + L_π) d_S(s_1, s_2).\n\\end{align*} \\] Thus, \\(V\\) is Lipschitz continuous with Lipschitz constant \\(L_Q(1 + L_π)\\)."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#lipschitz-continuity-of-value-iteration",
    "href": "mdps/lipschitz-mdps.html#lipschitz-continuity-of-value-iteration",
    "title": "16  Lipschitz MDPs",
    "section": "16.4 Lipschitz continuity of value iteration",
    "text": "16.4 Lipschitz continuity of value iteration\n\nLemma 16.4 Consider a discounted infinite horizon MDP which is \\((L_c, L_p)\\)-Lipschitz. Start with \\(V^{(0)} = 0\\) and recursively define\n\n\\(\\displaystyle Q^{(n+1)}(s,a) = c(s,a) + γ \\int V^{(n)}(y) p(y|s,a) dy.\\)\n\\(\\displaystyle V^{(n+1)}(s) = \\min_{a \\in \\ALPHABET A} Q^{(n+1)}(s,a).\\)\n\nThen, \\(V^{(n)}\\) is Lipschitz continuous and its Lipschitz constant \\(L_{V^{(n)}}\\) satisfies the following recursion: \\[L_{V^{(n+1)}} = L_c + γ L_p L_{V^{(n)}}.\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by induction. For \\(n=1\\), \\(Q^{(1)}(s,a) = c(s,a)\\), which is Lipschitz with Lipschitz constant \\(L_{Q^{(1)}} = L_c\\). Then, by Lemma 16.2, \\(V^{(1)}\\) is Lipschitz with Lipschitz constant \\(L_{V^{(1)}} = L_{Q^{(1)}} = L_c\\). This forms the basis of induction. Now assume that \\(V^{(n)}\\) is \\(L_{V^{(n)}}\\)-Lipschitz. Then, by Lemma 16.1, \\(Q^{(n+1)}\\) is \\((L_c + γL_p L_{V^{(n)}})\\)-Lipschitz. Therefore, by Lemma 16.2, \\(V^{(n+1)}\\) is Lipschitz with constant \\[ L_{V^{(n+1)}} = L_c + γ L_p L_{V^{(n)}}. \\space\\Box\\]\n\n\n\n\nLemma 16.5 Consider a discounted infinite horizon MDP which is \\((L_c, L_p)\\)-Lipschitz and let \\(π\\) be any randomized time-homogeneous policy which is \\(L_π\\)-Lipschitz. Start with \\(V^{(0)} = 0\\) and then recursively define\n\n\\(V^{(n)}_π(s) = \\int Q^{(n)}_π(s,a)π(a|s) du.\\)\n\\(\\displaystyle Q^{(n+1)}_π(s,a) = c(s,a) + γ \\int V^{(n)}_π(y) p(y|s,a) dy.\\)\n\nThen, then \\(Q^{(n)}_π\\) is Lipschitz continuous and its Lipschitz constant \\(L_{Q^{(n)}_π}\\) satisfies the follwoing recursion: \\[ L_{Q^{(n+1)}_π} + L_c + \\beta(1 + L_π)L_p L_{Q^{(n)}_π}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by induction. For \\(n=1\\), \\(Q^{(1)}_π(s,a) = c(s,a)\\), which is Lipschitz with Lipschitz constant \\(L_{Q^{(1)}_π} = L_c\\). This forms the basis of induction. Now assume that \\(Q^{(n)}_π\\) is \\(L_{Q^{(n)}_π}\\)-Lipschitz. Then, by Lemma 16.3, \\(V^{(n)}_π\\) is Lipschitz with Lipschitz constant \\(L_{V^{(n)}_π} = L_{Q^{(n)}_π}(1 + L_π)\\) and by Lemma 16.1, \\(Q^{(n+1)}_π\\) is Lipschitz with Lipschitz constant \\(L_{Q^{(n+1)}_π} = L_c + γL_p L_{V^{(n)}_π}.\\) Combining these two we get \\[ L_{Q^{(n+1)}_π} + L_c + \\beta(1 + L_π)L_p L_{Q^{(n)}_π}. \\]\n\n\n\n\nTheorem 16.1 Given any \\((L_c, L_p)\\)-Lipschitz MDP, if \\(\\beta L_p < 1\\), then the infinite horizon \\(\\beta\\)-discounted value function \\(V\\) is Lipschitz continuous with Lipschitz constant \\[ L_{V} = \\frac{L_c}{1 - γ L_p} \\] and the action-value function \\(Q\\) is Lipschitz with Lipschitz constant \\[ L_Q = L_V = \\frac{L_c}{1 - γ L_p}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the sequence of \\(L_n = L_{V^{(n)}}\\) values. For simplicity write \\(α = γ L_p\\). Then the sequence \\(\\{L_n\\}_{n \\ge 1}\\) is given by: \\(L_1 = L_c\\) and for \\(n \\ge 1\\), \\[ L_{n+1} = L_c + α L_n. \\] Hence, \\[ L_n = L_c + α L_c + \\dots + α^{n-1} L_c = \\frac{1 - α^n}{1 - α} L_c. \\] This sequence converges if \\(|α| < 1\\). Since \\(α\\) is non-negative, this is equivalent to \\(α < 1\\), which is true by hypothesis. Hence \\(L_n\\) is a convergent sequence. At convergence, the limit \\(L_V\\) must satisfy the fixed point of the recursion relationship introduced in Lemma 16.4, hence \\[ L_V = L_c + γ L_p L_V. \\] Consequently, the limit is equal to \\[ L_V = \\frac{L_c}{1 - γ L_p}. \\] The Lipschitz constant of \\(Q\\) follows from Lemma 16.1.\n\n\n\n\nTheorem 16.2 Given any \\((L_c, L_p)\\)-Lipschitz MDP and an \\(L_π\\)-Lipschitz (possibly randomized) time-homogeneous policy \\(π\\), if \\(\\beta (1 + L_π) L_p < 1\\), then the infinite horizon \\(\\beta\\)-discounted value-action function \\(Q_π\\) is Lipschitz continuous with Lipschitz constant \\[ L_{Q_π} = \\frac{L_c}{1 - γ(1 + L_π) L_p} \\] and the value function \\(V_π\\) is Lipschitz with Lipschitz constant \\[ L_{V_π} = L_{Q_π}(1 + L_π) =\n   \\frac{L_c(1 + L_π)}{1 - γ(1 + L_π) L_p}. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThe restrictive assumption in the result is that \\(γ(1 + L_π)L_p < 1\\). For a specific model, even when this assumption does not hold, it may be possible to directly check if the \\(Q\\)-function is Lipschitz continuous. Such a direct check often gives a better Lipschitz constant.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the sequence of \\(L_n = L_{Q^{(n)}_π}\\) values. For simplicity, write \\(α = γ(1 + L_π)L_p\\). Then, the sequence \\(\\{L_n\\}_{n \\ge 1}\\) is given by: \\(L_1 = L_c\\) and for \\(n \\ge 1\\), \\[L_{n+1} = L_c + α L_n. \\] Hence, \\[ L_n = L_c + α L_c + \\dots + α^{n-1} L_c = \\frac{1 - α^n}{1 - α} L_c. \\] This sequence converges if \\(|α| < 1\\). Since \\(α\\) is non-negative, this is equivalent to \\(α < 1\\), which is true by hypothesis. Hence \\(L_n\\) is a convergent sequence. At convergence, the limit \\(L_{Q_π}\\) must satisfy the fixed point of the recursion relationship introduced in Lemma 16.5, hence \\[ L_{Q_π} = L_c + γ(1 + L_π)L_p L_{Q_π}. \\] Consequently, the limit is equal to \\[ L_{Q_π} = \\frac{L_c}{1 - γ(1 + L_π) L_p}. \\]\nThe Lipschitz constant of \\(V_π\\) follows from Lemma 16.3."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#influence-radius",
    "href": "mdps/lipschitz-mdps.html#influence-radius",
    "title": "16  Lipschitz MDPs",
    "section": "16.5 Influence Radius",
    "text": "16.5 Influence Radius\nWhen the \\(Q\\)-function of an MDP is Lipschitz continuous, then the optimal action does not change too abruptly. More precisely, suppose an action \\(a\\) is optimal at state \\(s\\). Then, we can identify a hyperball \\(B(s, ρ(s))\\) of radius \\(ρ(s)\\) centered around \\(s\\) such that \\(a\\) is guaranteed to be the dominating action in \\(ρ(s)\\). This radius \\(ρ(s)\\) is called the influence radius.\nLet \\(π\\) denote the optimal policy, i.e., \\[ π(s) = \\arg \\min_{a \\in \\ALPHABET A} Q(s,a) \\] and \\(h\\) denote the second best action, i.e., \\[ h(s) = \\arg \\min_{a \\in \\ALPHABET A \\setminus \\{π(s)\\}} Q(s,a). \\] Define the domination value of state \\(s\\) to be \\[ Δ(s) = Q(s, h(s)) - Q(s, π(s)). \\]\n\nTheorem 16.3 For a Lipschitz continuous \\(Q\\)-function, the influence radius at state \\(s\\) is given by \\[ ρ(s) = \\frac{ Δ(s) }{ 2 L_Q }. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nCombining Theorem 16.2 and Theorem 16.3 implies that under the condition of Theorem 16.2, the influence radius at state \\(s\\) is at least \\[ ρ(s) = Δ(s)(1 - γ(1 + L_π)L_p)/2L_c. \\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe intuition behind the proof is the following. The value of the action \\(π(s)\\) can only decrease by \\(L_Q ρ(s)\\) in \\(B(s, ρ(s))\\), while the value of the second best action \\(h(s)\\) can only increase by \\(L_Q ρ(s)\\). So, the shortest distance \\(ρ(s)\\) from \\(s\\) needed for an action \\(h(s)\\) to “catch-up” with action \\(π(s)\\) should satisfy \\(2 L_Q ρ(s) = Δ(s)\\) or \\(ρ(s) = Δ(s)/2L_Q\\).\nFormally, for any \\(s' \\in B(s,ρ(s))\\), \\(d_S(s,s') \\le ρ(s)\\). Thus, for any action \\(a \\in \\ALPHABET A\\), \\[ | Q(s,a) - Q(s',a)| \\le L_Q d_S(s,s') \\le L_Q ρ(s). \\] Equivalently, \\[ Q(s,a) - L_Q ρ(s) \\le Q(s',a) \\le Q(s,a) + L_Q ρ(s) \\] which states that as \\(s'\\) moves away from \\(s\\), the value of \\(Q(s',a)\\) remains within a symmetric bound that depends on the radius \\(ρ(s)\\). Since this bound holds for all \\(a\\), they also hold for \\(a = π(s)\\). Thus, \\[ Q(s, π(s)) - L_Q ρ(s) \\le Q(s', π(s)) \\le Q(s, π(s)) + L_Q ρ(s). \\]\nSince \\(π(s)\\) is the optimal action, for any other action \\(a \\neq π(s)\\), \\[ Q(s,π(s)) \\le Q(s,a). \\] Thus, the action \\(π(s)\\) is optimal as long as the upper bound on \\(Q(s', π(s))\\) is lower than the lower bound on \\(Q(s',a)\\), i.e., \\[ Q(s, π(s)) + L_Q ρ(s) \\le Q(s,a) - L_Q ρ(s).  \\] Thus, the maximum value of \\(ρ(s)\\) is when the relationship holds with equality, i.e., \\[ ρ(s) = \\frac{Q(s,a) - Q(s,π(s))}{2 L_Q} \\ge \\frac{Δ(s)}{2 L_Q}. \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#exercises",
    "href": "mdps/lipschitz-mdps.html#exercises",
    "title": "16  Lipschitz MDPs",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 16.1 Let \\((\\ALPHABET S, d_S)\\) be a metric space and \\(s, s' \\in \\ALPHABET S\\). Consider two Bernoulli measures \\[ μ = a δ_s + (1-a) δ_{s'}, \\qquad\n      ν = b δ_s + (1-b) δ_{s'}. \\]\nShow that \\[ K(μ,ν) = |a - b| d(s,s'). \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#notes",
    "href": "mdps/lipschitz-mdps.html#notes",
    "title": "16  Lipschitz MDPs",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Rachelson and Lagoudakis (2010) and Hinderer (2005).\n\n\n\n\nHinderer, K. 2005. Lipschitz continuity of value functions in Markovian decision processes. Mathematical Methods of Operations Research 62, 1, 3–22. DOI: 10.1007/s00186-005-0438-1.\n\n\nRachelson, E. and Lagoudakis, M.G. 2010. On the locality of action domination in sequential decision making. Proceedings of 11th international symposium on artificial intelligence and mathematics. Available at: https://oatao.univ-toulouse.fr/17977/."
  },
  {
    "objectID": "pomdps/intro.html#history-dependent-dynamic-program",
    "href": "pomdps/intro.html#history-dependent-dynamic-program",
    "title": "17  Introduction",
    "section": "17.1 History dependent dynamic program",
    "text": "17.1 History dependent dynamic program\nOur first step to develop an efficient dynamic programming decomposition is to simply ignore efficiency and develop a dynamic programming decomposition. We start by deriving a recursive formula to compute the performance of a generic history dependent strategy \\(π = (π_1, \\dots, π_T)\\).\n\nPerformance of history-dependent strategies\nLet \\(H_t = (Y_{1:t}, A_{1:t-1})\\) denote all the information available to the decision maker at time \\(t\\). Thus, given any history dependent strategy \\(π\\), we can write \\(A_t = π_t(H_t)\\). Define the cost-to-go functions as follows: \\[\n  J_t(h_t; π) = \\EXP^π\\biggl[ \\sum_{s=t}^T c_s(S_s, A_s) \\biggm| H_t = h_t\n  \\biggr].\n\\] Note that \\(J_t(h_t; π)\\) only depends on the future strategy \\((π_t, \\dots, π_T)\\). These functions can be computed recursively as follows: \\[\\begin{align*}\n  J_t(h_t; π) &= \\EXP^π\\biggl[ \\sum_{s=t}^T c_s(H_s, π_s(H_s)) \\biggm|\n    H_t = h_t \\biggr] \\\\\n    &\\stackrel{(a)}= \\EXP^π \\biggl[ c_t(h_t, π_t(h_t)) + \\EXP^π\\biggl[\n    \\sum_{s=t+1}^T c_s(S_s, π_s(S_s)) \\biggm| H_{t+1} \\biggr] \\biggm|\n    H_t = h_t \\biggr]  \\\\\n    &= \\EXP^π[ c_t(h_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid H_t = h_t ],\n\\end{align*}\\] where \\((a)\\) follows from the towering property of conditional expectation and the fact that \\(H_t \\subseteq H_{t+1}\\).\nThus, we can use the following dynamic program to recursively compute the performance of a history-dependent strategy: \\(J_{T+1}(h_{T+1}) = 0\\) and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\nJ_t(h_t; π) = \\EXP^π [ c_t(h_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid\n  H_t = h_t ].\n\\]\n\n\nHistory-dependent dynamic programming decomposition\nWe can use the above recursive formulation for performance evaluation to derive a history-dependent dynamic program.\n\nTheorem 17.1 Recursively define _value functions \\(\\{V_t\\}_{t = 1}^{T+1}\\), where \\(V_t \\colon \\ALPHABET H_t \\to \\reals\\) as follows: \\[\\begin{equation}\n  V_{T+1}(h_{T+1}) = 0\n\\end{equation}\\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n  Q_t(h_t, a_t) &= \\EXP[ c_t(S_t, a_t) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = a_t ] \\\\\n  V_t(h_t) &= \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t)\n\\end{align}\\] Then, a history-dependent policy \\(π\\) is optimal if and only if it satisfies \\[\\begin{equation} \\label{eq:history-verification}\n  π_t(h_t) \\in \\arg \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t).\n\\end{equation}\\]\n\nThe proof idea is similar to the proof for MDPs. Instead of proving the above result, we prove a related result.\n\nTheorem 17.2 (The comparison principle) For any history-dependent strategy \\(π\\) \\[ J_t(h_t; π) \\ge V_t(h_t) \\] with equality at \\(t\\) if and only if the future straegy \\(π_{t:T}\\) satisfies the verification step \\eqref{eq:history-verification}.\n\nNote that the comparison principle immediately implies that the strategy obtained using dynamic program of Theorem 17.1 is optimal. The proof of the comparison principle is almost identical to the proof for MDPs.\n\n\n\n\n\n\nProof of the comparison principle\n\n\n\n\n\nThe proof proceeds by backward induction. Consider any history dependent policy \\(π = (π_1, \\dots, π_T)\\). For \\(t = T+1\\), the comparison principle is satisfied by definition and this forms the basis of induction. We assume that the result holds for time \\(t+1\\), which is the induction hypothesis. Then for time \\(t\\), we have \\[\\begin{align*}\n  V_t(h_t) &= \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t) \\\\\n  &\\stackrel{(a)}= \\min_{a_t \\in \\ALPHABET A}\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = π_t(h_t) ]\n  \\\\\n  &\\stackrel{(b)}\\le\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = π_t(h_t)]\n  \\\\\n  &\\stackrel{(c)}\\le\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid\n  H_t = h_t, A_t = π_t(h_t)]\n  \\\\\n  &= J_t(h_t, π).\n\\end{align*}\\] where \\((a)\\) follows from the definition of the \\(Q\\)-function; \\((b)\\) follows from the definition of minimization; and \\((c)\\) follows from the induction hyothesis. We have the equality at step \\((b)\\) iff \\(π_t\\) satisfies the verification step \\eqref{eq:history-verification} and have the equality in step \\((c)\\) iff \\(π_{t+1:T}\\) is optimal (this is part of the induction hypothesis). Thus, the result is true for time \\(t\\) and, by the principle of induction, is true for all time."
  },
  {
    "objectID": "pomdps/intro.html#the-notion-of-an-information-state",
    "href": "pomdps/intro.html#the-notion-of-an-information-state",
    "title": "17  Introduction",
    "section": "17.2 The notion of an information state",
    "text": "17.2 The notion of an information state\nNow that we have obtained a dynamic programming decomposition, let’s try to simplify it. To do so, we define the notion of an information state.\n\n\n\n\n\n\nInformation state\n\n\n\nA stochastic process \\(\\{Z_t\\}_{t = 1}^T\\), \\(Z_t \\in \\ALPHABET Z\\), is called an information state if \\(Z_t\\) be a function of \\(H_t\\) (which we denote by \\(Z_t = φ_t(H_t)\\)) and satisfies the following two properties:\nP1. Sufficient for performance evaluation, i.e., \\[ \\EXP^π[ c_t(S_t, A_t) \\mid H_t = h_t, A_t = a_t]\n    =  \\EXP[ c_t(S_t, A_t) \\mid Z_t = φ_t(h_t), A_t = a_t ] \\]\nP2. Sufficient to predict itself, i.e., for any Borel measurable subset \\(B\\) of \\(\\ALPHABET Z\\), we have \\[ \\PR^π(Z_{t+1} \\in B \\mid H_t = h_t, A_t = a_t) =\n       \\PR(Z_{t+1} \\in B \\mid Z_t = φ_t(h_t), A_t = a_t).\n    \\]\n\n\nInstead of (P2), the following sufficient conditions are easier to verify in some models:\n\n\n\n\n\n\nAn equivalent characterization\n\n\n\nP2a. Evolves in a state-like manner, i.e., there exist measurable functions \\(\\{ψ_t\\}_{t=1}^T\\) such that \\[ Z_{t+1} = ψ_t(Z_t, Y_{t+1}, A_t). \\]\nP2b. Is sufficient for predicting future observations, i.e., for any Borel subset \\(B\\) of \\(\\ALPHABET Y\\), \\[ \\PR^π(Y_{t+1} \\in B | H_t = h_t, A_t = a_t) =\n        \\PR(Y_{t+1} \\in B | Z_t = φ_t(h_t), A_t = a_t).\n     \\]\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThe right hand sides of (P1) and (P2) as well as (P2a) and (P2b) do not depend on the choice of the policy \\(π\\).\n\n\n\nProposition 17.1 : (P2a) and (P2b) imply (P2).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any Borel measurable subset \\(B\\) of \\(\\ALPHABET Z\\), we have \\[\\begin{align*}\n  \\hskip 1em & \\hskip -1em\n  \\PR(Z_{t+1} \\in B \\mid H_t = h_t, A_t = a_t)  \n  \\stackrel{(a)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(Y_{t+1} = y_{t+1}, Z_{t+1} \\in B\n  \\mid H_t = h_t, A_t = a_t ]\n  \\\\\n  &\\stackrel{(b)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\IND\\{ ψ_t(φ_t(h_t), y_{t+1}, a_t) \\}\n  \\PR(Y_{t+1} = y_{t+1} \\mid H_t = h_t, A_t = a_t)\n  \\\\\n  &\\stackrel{(c)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\IND\\{ ψ_t(φ_t(h_t), y_{t+1}, a_t) \\}\n  \\PR(Y_{t+1} = y_{t+1} \\mid Z_t = φ_t(h_t), A_t = a_t)\n  \\\\\n  &\\stackrel{(d)}=\n  \\PR(Z_{t+1} \\in B \\mid Z_t = φ_t(h_t), A_t = a_t)  \n\\end{align*}\\] where \\((a)\\) follows from the law of total probability, \\((b)\\) follows from (P2a), \\((c)\\) follows from (P2b), and \\((d)\\) from the law of total probability."
  },
  {
    "objectID": "pomdps/intro.html#examples-of-an-information-state",
    "href": "pomdps/intro.html#examples-of-an-information-state",
    "title": "17  Introduction",
    "section": "17.3 Examples of an information state",
    "text": "17.3 Examples of an information state\nWe start by define the belief state \\(b_t \\in Δ(\\ALPHABET S)\\) as follows: for any \\(s \\in \\ALPHABET S\\) \\[ b_t(s) = \\PR^π(S_t = s \\mid H_t = h_t). \\] The belief state is a function of the history \\(h_t\\). When we want to explicitly show the dependence of \\(b_t\\) on \\(h_t\\), we write it as \\(b_t[h_t]\\).\n\nLemma 17.1 The belief state \\(b_t\\) does not depend on the policy \\(π\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is an extremely important result which has wide-ranging implications in stochastic control. For a general discussion of this point, see Witsenhausen (1975).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the law of total probability and Bayes rule, we have \\[\\begin{equation} \\label{eq:belief}\n  \\PR(s_t | y_{1:t}, a_{1:t-1})\n  = \\sum_{s_{1:t-1}} \\PR(s_{1:t} | y_{1:t}, a_{1:t-1})\n  = \\sum_{s_{1:t-1}}\n   \\frac{\\PR(s_{1:t}, y_{1:t}, a_{1:t-1})}\n   {\\sum_{s'_{1:t}} \\PR(s'_{1:t}, y_{1:t}, a_{1:t-1})}\n\\end{equation}\\]\nNow consider \\[\\begin{align*}\n  \\PR(s_{1:t}, y_{1:t}, a_{1:t-1}) &=\n  \\PR(s_1) \\PR(y_1 | s_1) \\IND\\{ a_1 = π_1(y_1) \\} \\\\\n  & \\times\n  \\PR(s_2 | s_1, a_1) \\PR(y_2 | s_2) \\IND \\{ a_2 = π_2(y_{1:2}, a_1)\\} \\\\\n  & \\times \\cdots \\\\\n  & \\times\n  \\PR(s_{t-1} | s_{t-2}, a_{t-2}) \\PR(y_{t-1} | s_{t-1}) \\IND \\{ a_{t-1} =\n  π_{t-1}(y_{1:t-1}, a_{1:t-2}) \\} \\\\\n  & \\times\n  \\PR(s_{t} | s_{t-1}, a_{t-1}) \\PR(y_{t} | s_{t}).\n\\end{align*}\\] Substitute the above expression in both the numerator and the denominator of \\eqref{eq:belief}. Observe that the terms of the form \\(\\IND\\{ a_s = π_s(y_{1:s}, a_{1:s-1})\\) are common to both the numerator and the denominator and cancel each other. Thus, \\[\\begin{equation} \\label{eq:belief-fn}\n  \\PR(s_t | y_{1:t}, a_{1:t-1}) = \\sum_{s_{1:t-1}}\n  \\frac{ \\prod_{s=1}^t \\PR(s_s \\mid s_{s-1}, a_{s-1}) \\PR(y_s \\mid s_s) }\n  { \\sum_{s'_{1:t}} \\prod_{s=1}^t \\PR(s'_s \\mid s'_{s-1}, a_{s-1}) \\PR(y_s \\mid s'_s) }.\n\\end{equation}\\] None of the terms here depend on the policy \\(π\\). Hence, the belief state does not depend on the policy \\(π\\).\n\n\n\n\nLemma 17.2 The belief state \\(b_t\\) updates in a state like manner. In particular, for any \\(s_{t+1} \\in \\ALPHABET S\\), we have \\[\n  b_{t+1}(s_{t+1}) = \\sum_{s_t \\in \\ALPHABET S}\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n   { \\sum_{s'_{t:t+1}} \\PR(y_{t+1} | s'_{t+1}) \\PR(s'_{t+1} | s'_t, a_t) b_t(s'_t) }.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(s_{t+1} \\in \\ALPHABET S\\), consider\n\\[\\begin{align}\nb_{t+1}(s_{t+1}) &= \\PR(s_{t+1} | y_{1:t+1}, a_{1:t}) \\notag \\\\\n&= \\sum_{s_t} \\PR(s_{t:t+1} | y_{1:t+1}, a_{1:t}) \\notag \\\\\n&= \\sum_{s_t} \\frac{ \\PR(s_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) }\n  {\\sum_{s'_{t:t+1}}\\PR(s'_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) }.\n\\label{eq:update-1}\n\\end{align}\\]\nNow consider \\[\\begin{align}\n\\hskip 1em & \\hskip -1em\n\\PR(s_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) \\notag \\\\\n&= \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t)\n   \\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\n   \\PR(s_t | y_{1:t}, a_{1_t-1}) \\notag \\\\\n&= \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t)\n   \\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\n   b_t(s_t). \\label{eq:belief-2}\n\\end{align}\\] Substitute the above expression in both the numerator and the denominator of \\eqref{eq:update-1}. Observe that \\(\\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\\) is common to both the numerator and the denominator and cancels out. Thus, we get the result of the lemma.\n\n\n\nNow, we present three examples of information state here. See the Exercises for more examples.\n\nExample 17.1 The complete history \\(H_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove that \\(Z_t = H_t\\) satisfies properties (P1), (P2a), and (P2b).\nP1. \\(\\displaystyle \\EXP^π[ c_t(S_t, A_t) | H_t = h_t, A_t = a_t ] = \\sum_{s_t \\in \\ALPHABET S} c_t(s_t, a_t) b_t[h_t](s_t)\\).\nP2a. \\(H_{t+1} = (H_t, Y_{t+1}, A_t)\\)\nP2b. \\(\\displaystyle \\PR^π(y_{t+1} | y_{1:t}, a_{1:t}) = \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR( s_{t+1} | s_t, a_t) \\PR(s_t | y_{1:t}, a_{1:t})\\). Note that in the last term \\(\\PR^π(s_t | y_{1:t}, a_{1:t})\\) we can drop \\(a_t\\) from the conditioning because it is a function of \\((y_{1:t}, a_{1:t-1})\\). Thus, \\[ \\PR^π(s_t | y_{1:t}, a_{1:t}) = \\PR^π(s_t | y_{1:t}, a_{1:t-1}) =\nb_t[h_t](s_t).\\] Note that in the last step, we have used Lemma 17.1. Thus, \\(\\displaystyle \\PR^π(y_{t+1} | y_{1:t}, a_{1:t}) = \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR( s_{t+1} | s_t, a_t) b_t[h_t](s_t)\\).\n\n\n\n\nExample 17.2 The belief state \\(b_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe belief state \\(b_t\\) is a function of the history \\(h_t\\). (The exact form of this function is given by \\eqref{eq:belief-fn}). In the proof of Example 17.1, we have already shown that \\(b_t\\) satisfies (P1) and (P2b). Moreover Lemma 17.2 implies that the belief update satisfies (P2a).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nBoth the above information states are generic information states which work for all models. For specific models, it is possible to identify other information states as well. We present some examples of such an information state below.\n\n\n\nExample 17.3 An MDP is a special case of a POMDP where \\(Y_t = S_t\\). For an MDP \\(Z_t = S_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will show that \\(Z_t = S_t\\) satisfies (P1) and (P2).\n(P1) is satisfied because the per-step cost is a function of the \\((S_t, A_t)\\). (P2) is equivalent to the control Markov property."
  },
  {
    "objectID": "pomdps/intro.html#information-state-based-dynamic-program",
    "href": "pomdps/intro.html#information-state-based-dynamic-program",
    "title": "17  Introduction",
    "section": "17.4 Information state based dynamic program",
    "text": "17.4 Information state based dynamic program\nThe main feature of an information state is that one can always write a dynamic program based on an information state.\n\nTheorem 17.3 Let \\(\\{Z_t\\}_{t=1}^T\\) be any information state, where \\(Z_t = φ_t(H_t)\\). Recursively define value functions \\(\\{ \\hat V_t \\}_{t=1}^T\\), where \\(\\hat V_t \\colon \\ALPHABET Z \\to \\reals\\), as follows: \\[ \\hat V_{T+1}(z_{T+1}) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n  \\hat Q_t(z_t, a_t) &= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}(Z_{t+1}) \\mid\n  Z_t = z_t, A_t = a_t] \\\\\n  \\hat V_t(z_t) &= \\min_{a_t \\in \\ALPHABET A} \\hat Q_t(z_t, a_t).\n\\end{align}\\] Then, we have the following: for any \\(h_t\\) and \\(a_t\\), \\[\\begin{equation} \\label{eq:history-info}\n  Q_t(h_t, a_t) = \\hat Q_t(φ_t(h_t), a_t)\n  \\quad\\text{and}\\quad\n  V_t(h_t) = \\hat V_t(φ_t(h_t)).\n\\end{equation}\\] Any strategy \\(\\hat π = (\\hat π_1, \\dots, \\hat π_T)\\), where \\(\\hat π_t \\colon \\ALPHABET Z \\to \\ALPHABET A\\), is optimal if and only if \\[\\begin{equation}\\label{eq:info-verification}\n    \\hat π_t(z_t) \\in \\arg\\min_{a_t \\in \\ALPHABET A} \\hat Q_t(z_t, a_t).\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs usual, we prove the result by backward induction. By construction, Eq. \\eqref{eq:history-info} is true at time \\(T+1\\). This forms the basis of induction. Now assume that \\eqref{eq:history-info} is true at time \\(t+1\\) and consider the system at time \\(t\\). Then, \\[\\begin{align*}\nQ_t(h_t, a_t) &= \\EXP[ c_t(S_t, A_t) + V_{t+1}(H_{t+1}) | H_t = h_t, A_t = a_t\n] \\\\\n&\\stackrel{(a)}= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}( φ_t(H_{t+1}) ) | H_t =\nh_t, A_t = a_t ]  \\\\\n&\\stackrel{(b)}= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}( φ_t(H_{t+1}) ) | Z_t =\nφ_t(h_t), A_t = a_t ]  \\\\\n&\\stackrel{(c)}= \\hat Q_t(φ_t(h_t), a_t),\n\\end{align*}\\] where \\((a)\\) follows from the induction hypothesis, \\((b)\\) follows from the properties (P1) and (P2) of the information state, and \\((c)\\) follows from the definition of \\(\\hat Q_t\\). This shows that the action value functions are equal. By minimizing over the actions, we get that the value functions are also equal."
  },
  {
    "objectID": "pomdps/intro.html#belief-state-based-dynamic-program",
    "href": "pomdps/intro.html#belief-state-based-dynamic-program",
    "title": "17  Introduction",
    "section": "17.5 Belief state based dynamic program",
    "text": "17.5 Belief state based dynamic program\nAs shown in Example 17.2, the belief state \\(b_t\\) is an information state. Therefore, Theorem 17.3 implies that we can write a dynamic program based on \\(b_t\\). This is an important and commonly used formulation, so we study it separately and present some properties of the value functions. The belief state based dynamic program is given by: \\(V_{T+1}(b_{T+1}) = 0\\) and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\n  Q_t(b_t, a_t) =\n  \\EXP [ c_t(S_t, A_t) + V_{t+1}(B_{t+1}) \\mid B_t = b_t, A_t = a_t ].\n\\] and \\[ V_t(b_t) = \\min_{a_t \\in \\ALPHABET A} Q_t(b_t, a_t). \\]\nDefine \\[ \\PR(y_{t+1} | b_t, a_t) =\n   \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t) b_t(s_t).\n\\] Then, the belief update expression in Lemma 17.2 can be written as: \\[\n  b_{t+1}(s_{t+1}) =\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n  { \\PR(y_{t+1} | b_t, a_t) }.\n\\] For the ease of notation, we write this expression as \\(b_{t+1} = ψ(b_t, y_{t+1}, a_t)\\).\n\\[\\begin{align*}\n  Q_t(b_t, a_t) &= \\sum_{s_t \\in \\ALPHABET S} c_t(s_t, a_t) b_t(s_t) \\\\\n  & \\quad +  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  V_{t+1}( φ(b_t, y_{t+1}, a_t) ).\n\\end{align*}\\]\nA key property of the belief-state based value functions is the following.\n\nTheorem 17.4 The belief-state based value functions are piecewise linear and concave.\n\n\n\n\n\n\nAn illustration of a piecewise linear and concave function. Move the points around to see how the shape of the function changes.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs usual, we prove the result using backward induction. For any \\(a_T\\), \\[ Q_T(b_T, a_T) = \\sum_{s_T \\in \\ALPHABET S} c_T(s_T, a_T) b_T(s_T) \\] is linear in \\(b_T\\). Therefore, \\[ V_T(b_T) = \\min_{a_T \\in \\ALPHABET A} Q_T(b_T, a_T) \\] is the minimum of a finite number of linear functions. Hence \\(V_T(b_T)\\) is piecewise linear and concave.\nNow assume that \\(V_{t+1}(b_{t+1})\\) is piecewise linear and concave (PWLC). Any PWLC function can be represented as a minimum of a finite number of hyperplanes. Therefore, we can find a finite set of vectors \\(\\{ A_i \\}_{i \\in I}\\) indexed by finite set \\(I\\) such that \\[\n  V_{t+1}(b) = \\min_{i \\in I} \\langle A_i, b \\rangle.\n\\]\nWe need to show that \\(V_t(b_t)\\) is piecewise linear and concave (PWLC). We first show that \\(Q_t(b_t, a_t)\\) is PWLC. For any fixed \\(a_t\\), the first term \\(\\sum_{s_t} c_t(s_t, a_t) b_t(s_t)\\) is linear in \\(b_t\\). Now consider the second term: \\[\\begin{align*}\n  \\hskip 1em & \\hskip -1em\n  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  V_{t+1}( φ(b_t, y_{t+1}, a_t) ) \\\\\n  &=\n  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  \\min_{i \\in I}\n  \\left\\langle A_i,\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n  { \\PR(y_{t+1} | b_t, a_t) } \\right\\rangle \\\\\n  &=\n  \\sum_{y_{t+1} \\in \\ALPHABET Y}\n  \\min_{i \\in I}\n  \\Big\\langle A_i,\n   \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t)\n   \\Big\\rangle\n\\end{align*}\\] which is the sum of PWLC functions of \\(b_t\\) and therefore PWLC in \\(b_t\\).\nThus, \\(Q_t(b_t, a_t)\\) is PWLC. Hence, \\(V_t(b_t)\\) which is the pointwise minimum of PWLC functions is PWLC. Hence, the result holds due to principle of induction.\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nSince the value function is PWLC, we can identify a finite index set \\(I_t\\), and a set of vectors \\(\\{ A^i_t \\}_{i \\in I_t}\\) such that \\[\n    V_t(b) = \\min_{i \\in I_t} \\langle A^i_t, b \\rangle.\n\\] Smallwood and Sondik (1973) presented a “one-pass” algorithm to recursively compute \\(I_t\\) and \\(\\{ A^i_t \\}_{i \\in I_t}\\) which allows us to exactly compute the value function. Various efficient refinements of these algorithms have been presented in the literature, e.π., the linear-support algorithm (Cheng 1988), the witness algorithm (Cassandra et al. 1994), incremental pruning (Zhang and Liu 1996; Cassandra et al. 1997), duality based approach (Zhang 2009), and others. See https://pomdp.org/ for an accessible introduction to these algorithms."
  },
  {
    "objectID": "pomdps/intro.html#exercises",
    "href": "pomdps/intro.html#exercises",
    "title": "17  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 17.1 Consider an MDP where the state space \\(\\ALPHABET S\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L -1 , L\\}\\) and the action space \\(\\ALPHABET A\\) is discrete. Suppose the transition matrix \\(P(a)\\) and the cost function \\(c_t(s,a)\\) satisfy properties (A1) and (A2) of Exercise 7.6. Show that \\(Z_t = |S_t|\\) is an information state.\n\n\nExercise 17.2 Consider a linear system with state \\(x_t \\in \\reals^n\\), observations \\(y_t \\in \\reals^p\\), and action \\(u_t \\in \\reals^m\\). Note that we will follow the standard notation of linear systems and denote the system variables by lower case letters \\((x,u)\\) rather than upper case letter \\((S,A)\\). The dynamics of the system are given by \\[\\begin{align*}\n  x_{t+1} &= A x_t + B u_t + w_t  \\\\\n  y_t &= C x_t + n_t\n\\end{align*}\\] where \\(A\\), \\(B\\), and \\(C\\) are matrices of appropriate dimensions. The per-step cost is given by \\[\n  c(x_t, u_t) = x_t^\\TRANS Q x_t + u_t^\\TRANS R u_t,\n\\] where \\(Q\\) is a positive semi-definite matrix and \\(R\\) is a positive definite matrix. We make the standard assumption that the primitive random variables \\(\\{s_1, w_1, \\dots, w_T, n_1, \\dots, n_T \\}\\) are independent.\nShow that if the primitive variables are Guassian, then the conditional estimate of the state \\[\n  \\hat x_t = \\EXP[ x_t | y_{1:t}, u_{1:t-1} ]\n\\] is an information state.\n\n\nExercise 17.3 Consider a machine which can be in one of \\(n\\) ordered state where the first state is the best and the last state is the worst. The production cost increases with the state of the machine. The state evolves in a Markovian manner. At each time, an agent has the option to either run the machine or stop and inspect it for a cost. After inspection, the agent may either repair the machine (at a cost that depends on the state) or replace it (at a fixed cost). The objective is to identify a maintenance policy to minimize the cost of production, inspection, repair, and replacement.\nLet \\(τ\\) denote the time of last inspection and \\(S_τ\\) denote the state of the machine after inspection, repair, or replacement. Show that \\((S_τ, t-τ)\\) is an information state."
  },
  {
    "objectID": "pomdps/intro.html#notes",
    "href": "pomdps/intro.html#notes",
    "title": "17  Introduction",
    "section": "Notes",
    "text": "Notes\nThe discussion in this section is taken from Subramanian et al. (2022). Information state may be viewed as a generalization of the traditional notion of state Nerode (1958), which is defined as a statistic (i.e., a function of the observations) sufficient for input-output mapping. In contrast, we define an information state as a statistic sufficient for performance evaluation (and, therefore, for dynamic programming). Such a definition is hinted in Witsenhausen (1976). The notion of information state is also related to sufficient statistics for optimal control defined in Striebel (1965) for systems with state space models.\nAs far as we are aware, the informal definition of information state was first proposed by Kwakernaak (1965) for adaptive control systems. Formal definitions for linear control systems were given by Bohlin (1970) for discrete time systems and by Davis and Varaiya (1972) for continuous time systems. Kumar and Varaiya (1986) define an information state as a compression of past history which satisfies property (P2a) but do not formally show that such an information state always leads to a dynamic programming decomposition.\n\n\n\n\nBohlin, T. 1970. Information pattern for linear discrete-time models with stochastic coefficients. IEEE Transactions on Automatic Control (TAC) 15, 1, 104–106.\n\n\nCassandra, A., Littman, M.L., and Zhang, N.L. 1997. Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes. Proceedings of the thirteenth conference on uncertainty in artificial intelligence.\n\n\nCassandra, A.R., Kaelbling, L.P., and Littman, M.L. 1994. Acting optimally in partially observable stochastic domains. AAAI, 1023–1028.\n\n\nCheng, H.-T. 1988. Algorithms for partially observable markov decision processes.\n\n\nDavis, M.H.A. and Varaiya, P.P. 1972. Information states for linear stochastic systems. Journal of Mathematical Analysis and Applications 37, 2, 384–402.\n\n\nKumar, P.R. and Varaiya, P. 1986. Stochastic systems: Estimation identification and adaptive control. Prentice Hall.\n\n\nKwakernaak, H. 1965. Theory of self-adaptive control systems. In: Springer, 14–18.\n\n\nNerode, A. 1958. Linear automaton transformations. Proceedings of American Mathematical Society 9, 541–544.\n\n\nSmallwood, R.D. and Sondik, E.J. 1973. The optimal control of partially observable markov processes over a finite horizon. Operations Research 21, 5, 1071–1088. DOI: 10.1287/opre.21.5.1071.\n\n\nStriebel, C. 1965. Sufficient statistics in the optimal control of stochastic systems. Journal of Mathematical Analysis and Applications 12, 576–592.\n\n\nSubramanian, J., Sinha, A., Seraj, R., and Mahajan, A. 2022. Approximate information state for approximate planning and reinforcement learning in partially observed systems. Journal of Machine Learning Research 23, 12, 1–83. Available at: http://jmlr.org/papers/v23/20-1165.html.\n\n\nWitsenhausen, H.S. 1975. On policy independence of conditional expectation. Information and Control 28, 65–75.\n\n\nWitsenhausen, H.S. 1976. Some remarks on the concept of state. In: Y.C. Ho and S.K. Mitter, eds., Directions in large-scale systems. Plenum, 69–75.\n\n\nZhang, H. 2009. Partially observable Markov decision processes: A geometric technique and analysis. Operations Research.\n\n\nZhang, N. and Liu, W. 1996. Planning in stochastic domains: Problem characteristics and approximation. Hong Kong Univeristy of Science; Technology."
  },
  {
    "objectID": "probability/sub-gaussian.html#prelim-concentration-inequality-of-sum-of-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#prelim-concentration-inequality-of-sum-of-gaussian-random-variables",
    "title": "18  Sub-Gaussian random variables",
    "section": "18.1 Prelim: Concentration inequality of sum of Gaussian random variables",
    "text": "18.1 Prelim: Concentration inequality of sum of Gaussian random variables\nLet \\(\\phi(\\cdot)\\) denote the density of \\(\\mathcal{N}(0,1)\\) Gaussian random variable: \\[ \\phi(x) = \\frac{1}{\\sqrt{2π}} \\exp\\biggl( - \\frac{x^2}{2} \\biggr). \\]\nNote that if \\(X \\sim \\mathcal{N}(μ,σ^2)\\), then the density of \\(X\\) is \\[\n\\frac{1}{σ}\\phi\\biggl( \\frac{x-μ}{σ} \\biggr)\n= \\frac{1}{\\sqrt{2π}\\,σ} \\exp\\biggl( - \\frac{(x-μ)^2}{2 σ^2} \\biggr). \\]\nThe tails of Gaussian random variables decay fast which can be quantified using the following inequality.\n\nProposition 18.1 (Mills inequality) If \\(X \\sim \\mathcal{N}(0, 1)\\), then for any \\(t > 0\\), \\[ \\PR( |X| > t ) \\le \\frac{2\\phi(t)}{t}  \\]\nMore generally, if \\(X \\sim \\mathcal{N}(0, σ^2)\\), then for any \\(t > 0\\), \\[ \\PR( |X| > t ) \\le 2\\frac{σ}{t} \\phi\\biggl(\\frac{t}{σ}\\biggr) =\n\\sqrt{\\frac{2}{π} } \\frac{σ}{t}\n  \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr). \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nIn the communication theory literature, this bound is sometimes known as the bound on the erfc or \\(Q\\) function.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe’ll first prove the result for unit variance random variable. Note that \\(X\\) is symmetric around origin. Therefore, \\[ \\PR(|X| > t) = 2\\PR(X > t). \\]\nNow, by using an idea similar to the proof of Markov’s inequality, we have \\[\\begin{align*}\nt \\cdot \\PR( |X| > t) &= t \\int_{t}^∞ \\phi(x) dx  \\\\\n& \\le \\int_{t}^∞ x \\phi(x) dx \\\\\n& = \\int_{t}^∞ \\frac{1}{\\sqrt{2π}} x \\exp\\biggl( - \\frac{x^2}{2} \\biggr) dx \\\\\n&= \\frac{1}{\\sqrt{2π}} \\int_{t}^∞ - \\frac{∂}{∂x} \\exp\\biggl( -\\frac{x^2}{2}\n\\biggr) dx \\\\\n& = \\frac{1}{\\sqrt{2π}} \\exp\\biggl( - \\frac{t^2}{2} \\biggr)\n\\end{align*}\\]\nThe proof for the general case follows by observing that \\[\n\\PR(|X| > t) = \\PR\\biggl( \\biggl| \\frac{X}{σ} \\biggr| > \\frac{t}{σ} \\biggr)\n\\] where \\(X/σ \\sim \\mathcal{N}(0,1)\\).\n\n\n\nThe fact that a Gaussian random variable has tails that decay to zero exponentially fast can be be seen in the moment generating function: \\[\n  M(s) = \\EXP[ \\exp(sX) ] = \\exp\\bigl( sμ + \\tfrac12 s^2 σ^2\\bigr).\n\\]\nA useful application of Mills inequality is the following concentration inequality.\n\nProposition 18.2 (Concentration inequality.) Let \\(X_i \\sim \\mathcal{N}(0, σ^2)\\) (not necessarily independent). Then, for any \\(t > 0\\), \\[\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} |X_i| > t\\Bigr) \\le\n  2n \\frac{σ}{t} \\phi\\biggl( \\frac{t}{σ} \\biggr).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\nThis follows immediately from Mills inequality and the union bound.\n\n\nAnother useful result is the following:\n\nProposition 18.3 (Max of Gaussian random variables.) Let \\(X_i \\sim \\mathcal{N}(0,σ^2)\\) (not necessarily independent). Then, \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\] and \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} |X_i| \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\]\n\nSee these notes for a lower bound with the same rate!\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the first inequality. The second follows by considering \\(2n\\) random variables \\(X_1, \\dots, X_n\\), \\(-X_1, \\dots, -X_n\\).\nFor any \\(s > 0\\), \\[\\begin{align*}\n\\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] &=\n\\frac{1}{s}\n\\EXP\\Bigl[ \\log \\Bigl( \\exp\\Bigl( s \\max_{1 \\le i \\le n} X_i \\Bigr) \\Bigr) \\Bigr]\n\\\\\n&\\stackrel{(a)}\\le\n\\frac{1}{s}\n\\log \\Bigl( \\EXP\\Bigl[ \\exp\\Bigl( s \\max_{1 \\le i \\le n} X_i \\Bigr) \\Bigr] \\Bigr)\n\\\\\n&\\stackrel{(b)}=\n\\frac{1}{s}\n\\log \\Bigl( \\EXP\\Bigl[ \\max_{1 \\le i \\le n} \\exp( s X_i ) \\Bigr] \\Bigr)\n\\\\\n&\\stackrel{(c)}\\le\n\\frac{1}{s}\n\\log \\Bigl(\\sum_{i=1}^n \\EXP\\bigl[ \\exp( s X_i ) \\bigr] \\Bigr)\n\\\\\n&\\stackrel{(d)}=\n\\log \\Bigl( \\sum_{i=1}^n\\exp\\Bigl( \\frac{s^2 σ^2}{2} \\Bigr) \\Bigr)\n\\\\\n&= \\frac{\\log n}{s} + \\frac{s^2 σ^2}{2}\n\\end{align*}\\] where \\((a)\\) follows from Jensen’s inequality, \\((b)\\) follows from monotonicity of \\(\\exp(\\cdot)\\), \\((c)\\) follows from definition of max, \\((d)\\) follows from the definition of moment generating function of Gaussian random variables. We get the result by setting \\(s = \\sqrt{2 \\log n}/σ\\) (which minimizes the upper bound).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nWe have stated and proved these inequalities for real-valued random variables. But a version of them continue to hold for vector valued Gaussian variables as well. For a complete treatment, see Picard (2007)."
  },
  {
    "objectID": "probability/sub-gaussian.html#sub-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#sub-gaussian-random-variables",
    "title": "18  Sub-Gaussian random variables",
    "section": "18.2 Sub-Gaussian random variables",
    "text": "18.2 Sub-Gaussian random variables\nIt turns out that the concentration inequalities of the form above continue to hold for more general distributions than the Gaussian. In particular, consider the bound on the max of Gaussian random variables that we established above. The only step which depends on the assumption that the random variables \\(X_i\\) were Gaussian in step \\((d)\\). Thus, as long as \\(\\EXP[ \\exp(s X_i) ] \\le \\exp(\\tfrac12 s^2 σ^2)\\), the result will continue to hold! This motivates the definition of sub-Gaussian random variables.\n\nDefinition 18.1 (Sub-Gaussian random variable) A random variable \\(X \\in \\reals\\) is said to be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and its moment generating function satisfies \\[ \\EXP[ \\exp(sX) ] \\le \\exp( \\tfrac12 s^2 σ^2),\n\\quad \\forall s \\in \\reals. \\]\n\nThe reason the parameter \\(σ^2\\) is called a variance proxy is because by a straight forward application of Taylor series expansion and comparing coefficients, it can be shown that \\(\\text{var}(X) \\le σ^2\\). See Rivasplata (2012) for a proof.\nThis definition can be generalized to random vectors and matrices. A random vector \\(X \\in \\reals^d\\) is said the be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and for any unit vector \\(u \\in \\reals^d\\), \\(u^\\TRANS X\\) is sub-Gaussian with variance proxy \\(σ^2\\).\nSimilarly, a random matrix \\(X \\in \\reals^{d_1 × d_2}\\) is said to be sub-Gaussian with variance proxy \\(σ^2\\) if \\(\\EXP[X] = 0\\) and for any unit vectors \\(u \\in \\reals^{d_1}\\) and \\(v \\in \\reals^{d_2}\\), \\(u^\\TRANS X v\\) is sub-Gaussian with variance proxy \\(σ^2\\).\nWe will use the phrase “\\(σ\\)-sub-Gaussian” as a short form of “sub-Gaussian with variance proxy \\(σ^2\\)”. One typically writes \\(X \\sim \\text{subG}(σ^2)\\) to denote a random variable with sub-Gaussian distribution with variance proxy \\(σ^2\\). (Strictly speaking, this notation is a bit ambiguous since \\(\\text{subG}(σ^2)\\) is a class of distributions rather than a single distribution.)"
  },
  {
    "objectID": "probability/sub-gaussian.html#examples-of-sub-gaussian-distributions",
    "href": "probability/sub-gaussian.html#examples-of-sub-gaussian-distributions",
    "title": "18  Sub-Gaussian random variables",
    "section": "18.3 Examples of sub-Gaussian distributions",
    "text": "18.3 Examples of sub-Gaussian distributions\n\nIf \\(X\\) be a Rademacher random variable, i.e., \\(X\\) takes the values \\(\\pm 1\\) with probability \\(1/2\\). Then, \\[ \\EXP[ \\exp(sX) ] = \\frac12 e^{-s} + \\frac12 e^s = \\cosh s \\le\n\\exp(\\tfrac12 s^2), \\] so \\(X\\) is\nIf \\(X\\) is uniformly distributed over \\([-a, a]\\). Then, for any \\(s \\neq 0\\), \\[ \\EXP[ \\exp(s X) ] = \\frac{1}{2as}[ e^{as} - e^{-as} ]\n   = \\sum_{n=0}^∞ \\frac{(as)^{2n}}{(2n+1)!}. \\] Using the inequality \\((2n+1)! \\ge n!2^n\\), we get that \\(X\\) is \\(a\\)-sub-Gaussian.\nIt can be shown that (see Rivasplata (2012) ) if \\(X\\) is a random variable with \\(\\EXP[X] = 0\\) and \\(|X| < 1\\) a.s., then \\[ \\EXP[ \\exp(sX) ] \\le \\cosh s, \\quad \\forall s \\in \\reals. \\] Therefore, \\(X\\) is 1-sub-Gaussian.\nAn immediate corollary of the previous example is that if \\(X\\) is a random variable with \\(\\EXP[X] = 0\\) and \\(|X| \\le b\\) a.s., then \\(X\\) is \\(b\\)-sub-Gaussian.\nBy a similar arguement, we can show that if \\(X\\) is a zero mean random variable supported on some interval \\([a,b]\\), then \\(X\\) is \\((b-a)/2\\) sub-Gaussian.\nIf \\(X\\) is \\(σ^2\\) sub-Gaussian, then for any \\(α \\in \\reals\\), \\(α X\\) is \\(|α|σ\\)-sub-Gaussian.\nIf \\(X_1\\) and \\(X_2\\) are \\(σ_1\\) and \\(σ_2\\)-sub-Gaussian, then \\(X_1 + X_2\\) is \\(\\sqrt{σ_1^2 + σ_2^2}\\)-sub-Gaussian."
  },
  {
    "objectID": "probability/sub-gaussian.html#characterization-of-sub-gaussian-random-variables",
    "href": "probability/sub-gaussian.html#characterization-of-sub-gaussian-random-variables",
    "title": "18  Sub-Gaussian random variables",
    "section": "18.4 Characterization of sub-Gaussian random variables",
    "text": "18.4 Characterization of sub-Gaussian random variables\nSub-Gaussian random variables satisfy a concentration result similar to Mills inequality.\n\nLemma 18.1 Let \\(X \\in \\reals\\) be \\(σ\\)-sub-Gaussian. Then, for any \\(t > 0\\), \\[\\begin{equation}\\label{eq:sG-tail-bounds}\n  \\PR(X > t) \\le \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr)\n  \\quad\\text{and}\\quad\n  \\PR(X < t) \\le \\exp\\biggl( - \\frac{t^2}{2σ^2} \\biggr)\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis follows from Chernoff’s bound and the definition of sub-Gaussianity. In particular, for any \\(s > 0\\) \\[\n\\PR(X > t) = \\PR(\\exp(sX) > \\exp(st)) \\le \\frac{ \\EXP[\\exp(sX) ]} { \\exp(st) }\n\\le \\exp\\biggl( \\frac{s^2 σ^2}{2} - st \\biggr).\n\\] Now, to find the tightest possible bound, we minimize the above bound with respect to \\(s\\), which is attained at \\(s = t/σ^2\\). Substituting this in the above bound, we get the first inequality. The second inequality follows from a similar argument.\n\n\n\nRecall that the moments of \\(Z \\sim \\mathcal{N}(0,σ^2)\\) are given by \\[\n  \\EXP[ |Z|^k ] = \\frac{1}{\\sqrt{π}} (2σ^2)^{k/2} Γ\\biggl(\\frac{k+1}{2}\\biggr),\n\\] where \\(Γ(\\cdot)\\) denotes the Gamma function. The next result shows that the tail bounds \\eqref{eq:sG-tail-bounds} are sufficient to show that the absolute moments of \\(X \\sim \\text{subG}(σ^2)\\) can be bounded by those of \\(Z \\sim \\mathcal{N}(0,σ^2)\\) up to multiplicative constants.\n\nLemma 18.2 Let \\(X\\) be a random variable such that \\[ \\PR( |X| > t) \\le 2 \\exp\\biggl(- \\frac{t^2}{2σ^2} \\biggr),\\] then for any positive integer \\(k \\ge 1\\), \\[ \\EXP[ |X|^k ] \\le (2σ^2)^{k/2} k Γ(k/2). \\]\n\nNote that for the special case of \\(k=1\\), the above bound implies \\(\\EXP[ |X| ] \\le σ \\sqrt{2π}\\) and for \\(k=2\\), \\(\\EXP[|X|^2] \\le 4σ^2\\).\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis is a simple application of the tail bound. \\[\\begin{align*}\n\\EXP[ |X|^k ] &= \\int_{0}^∞ \\PR( |X|^k > t ) dt \\\\\n&= \\int_{0}^∞ \\PR( |X| > t^{1/k}) dt \\\\\n&\\le 2 \\int_{0}^∞ \\exp\\biggl( - \\frac{t^{2/k}}{2σ^2} \\biggr) dt \\\\\n&= (2σ^2)^{k/2} k \\int_{0}^∞ e^{-u} u^{k/2 - 1} du,\n\\qquad u = \\frac{t^{2/k}}{2σ^2} \\\\\n&= (2σ^2)^{k/2}k Γ(k/2).\n\\end{align*}\\]\nThe result for \\(k=1\\) follows from \\(Γ(1/2) = \\sqrt{π/2}\\).\n\n\n\nUsing moments, we can bound the moment generating function in terms of the tail bounds.\n\nLemma 18.3 Let \\(X\\) be a random variable such that \\[ \\PR( |X| > t) \\le 2 \\exp\\biggl(- \\frac{t^2}{2σ^2} \\biggr)\\] then, \\[\\EXP[ \\exp(sX) ] \\le \\exp(4 s^2 σ^2). \\]\n\nFor this reason, sometimes it is stated that \\(X \\sim \\text{subG}(s^2)\\) when it satisfies the tail bound \\eqref{eq:sG-tail-bounds}.\nThe proof follows from the following Taylor series bound on the exponential function. \\[\n\\exp(sX) \\le 1 + \\sum_{k=2}^∞ \\frac{s |X|^k}{k!}\n\\] and apply the result of Lemma 18.2. See Rigollet (2015) for details."
  },
  {
    "objectID": "probability/sub-gaussian.html#properties-of-sub-gaussian-random-vectors",
    "href": "probability/sub-gaussian.html#properties-of-sub-gaussian-random-vectors",
    "title": "18  Sub-Gaussian random variables",
    "section": "18.5 Properties of sub-Gaussian random vectors",
    "text": "18.5 Properties of sub-Gaussian random vectors\n\nTheorem 18.1 Let \\(X = (X_1, \\dots, X_n)\\) be a vector of independent \\(σ\\)-sub-Gaussian random variables. Then, the random vector \\(X\\) is \\(σ\\)-sub-Gaussian.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any unit vector \\(u \\in \\reals^n\\), and any \\(s \\in \\reals\\) \\[\\begin{align*}\n\\EXP[ \\exp( s u^\\TRANS X) ] &= \\prod_{i=1}^n \\EXP[ \\exp(s u_i X_i) ] \\\\\n&\\le \\prod_{i=1}^n \\exp\\bigl( \\tfrac{1}{2} s^2 u_i^2 σ^2 \\bigr) \\\\\n&= \\exp\\bigl( \\tfrac{1}{2} s^2 \\| u \\|^2 σ^2 \\bigr) \\\\\n&= \\exp\\bigl( \\tfrac{1}{2} s^2 σ^2 \\bigr).\n\\end{align*}\\]"
  },
  {
    "objectID": "probability/sub-gaussian.html#concentration-inequalities",
    "href": "probability/sub-gaussian.html#concentration-inequalities",
    "title": "18  Sub-Gaussian random variables",
    "section": "18.6 Concentration inequalities",
    "text": "18.6 Concentration inequalities\nRecall that if \\(X_1\\) and \\(X_2\\) and \\(σ_1\\) and \\(σ_2\\)-sub-Gaussian, then \\(X_1 + X_2\\) is sub-Gaussian with variance proxy \\(σ_1^2 + σ_2^2\\). An immediate implication of this property is the following:\n\nProposition 18.4 (Hoeffding inequality) Suppose that variables \\(X_i\\), \\(i \\in \\{1,\\dots,n\\}\\), are independent and \\(X_i\\) has mean \\(μ_i\\) and \\(σ_i\\)-sub-Gaussian. Then, for all \\(t > 0\\), we have\n\\[ \\PR\\biggl( \\sum_{i=1}^n( X_i - μ_i) \\ge t \\biggr)\n   \\le \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n σ_i^2 } \\biggr).\n\\]\n\nThe Hoeffding inequality is often stated for the special case of bounded random variables. In particular, if \\(X_i \\in [a,b]\\), then we know that \\(X_i\\) is sub-Gaussian with parameter \\(σ = (b-a)/2\\), so we obtain the bound \\[ \\PR\\biggl( \\sum_{i=1}^n( X_i - μ_i) \\ge t \\biggr)\n   \\le \\exp\\biggl( - \\frac{2t^2}{\\sum_{i=1}^n n(b-a)^2 } \\biggr).\n\\]\nThe Hoeffding inequality can be generalized to Martingales. Recall that a sequence \\(\\{ (D_i, \\mathcal F_i)\\}_{i \\ge 1}\\) is called a martingale difference sequence is for all \\(i \\ge 1\\), \\(D_i\\) is \\(\\mathcal{F}_i\\) measurable, \\[ \\EXP[ |D_i| ] < ∞ \\quad\\text{and}\\quad\n   \\EXP[ D_{i+1} \\mid \\mathcal{F}_i ] = 0. \\]\n\nProposition 18.5 (Asuma-Hoeffding Inequality.) Let \\(\\{ (D_i, \\mathcal{F}_i)\\}_{i \\ge 1}\\) be a martingale difference sequence and suppose that \\(|D_i| \\le b_i\\) almost surely for all \\(i \\ge 1\\). Then for all \\(t \\ge 0\\) \\[ \\PR\\biggl( \\biggl| \\sum_{i=1}^n D_i \\biggr| \\ge t \\biggr)\n   \\le 2 \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince \\(|D_i| \\le b_i\\), \\(D_i\\) is \\(b_i\\)-subGaussian. Using the smoothing property of conditional expectation, we have \\[\\begin{align}\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^n D_i \\biggr) \\biggr) \\biggl]\n&=\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^{n-1} D_i \\biggr) \\biggr) \\biggl]\n\\,\n\\EXP\\bigl[ \\exp\\bigl( s D_n \\bigr) \\bigm| \\mathcal{F}_{n-1} \\bigl]\n\\notag \\\\\n&\\le\n\\EXP\\biggl[ \\exp\\biggl( s \\biggl( \\sum_{i=1}^{n-1} D_i \\biggr) \\biggr) \\biggl]\n\\, \\exp\\bigl( \\tfrac12 s^2 b_n^2 \\bigr),\n\\end{align}\\] where the inequality followed from \\(D_n\\) being \\(b_n\\)-subGaussian. Iterating backwards this way, we get \\[ \\PR\\biggl(  \\sum_{i=1}^n D_i  \\ge t \\biggr)\n   \\le  \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\] By a symmetric argument, we can show that \\[ \\PR\\biggl(  \\sum_{i=1}^n D_i  \\le -t \\biggr)\n   \\le  \\exp\\biggl( - \\frac{t^2}{2 \\sum_{i=1}^n b_i^2 } \\biggr).\n\\] Conbining these two, we get the stated result.\n\n\n\nNote that we can easily generalize the above inequality to the case when \\(D_k \\in [a_i, b_i]\\) because in that case \\(D_k\\) will be \\((b_i - a_i)/2\\) sub-Gaussian."
  },
  {
    "objectID": "probability/sub-gaussian.html#maximal-inequalities",
    "href": "probability/sub-gaussian.html#maximal-inequalities",
    "title": "18  Sub-Gaussian random variables",
    "section": "18.7 Maximal inequalities",
    "text": "18.7 Maximal inequalities\nAs we explained in the motivation for the definition of sub-Gaussian random variables, the definition implies that sub-Gaussian random variables will satisfy the concentration and maximal inequalities for Gaussian random variables. In particular, we have the following general result.\n\nTheorem 18.2 Let \\(X_i \\in \\reals\\) be \\(σ\\)-sub-Gaussian random variables (not necessarily independent). Then, \\[\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} X_i \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\quad\\text{and}\\quad\n  \\EXP\\Bigl[ \\max_{1 \\le i \\le n} |X_i| \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\] Moreover, for any \\(t > 0\\), \\[\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} X_i > t\\Bigr) \\le\n  n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr)\n\\quad\\text{and}\\quad\n  \\PR\\Bigl( \\max_{1 \\le i \\le n} |X_i| > t\\Bigr) \\le\n  2n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr).\n\\]\n\nThe proof is exactly the same as the Gaussian case!\nNow we state two generalizations without proof. See Rigollet (2015) for proof.\n\nMaximum over a convex polytope\n\nTheorem 18.3 Let \\(\\mathsf{P}\\) be a polytope with \\(n\\) vertices \\(v^{(1)}, \\dots, v^{(n)} \\in  \\reals^d\\) and let \\(X \\in \\reals^d\\) be a random variable such that \\([  v^{(i)} ]^\\TRANS X\\), \\(i \\in \\{1, \\dots, n\\}\\) are \\(σ\\)-sub-Gaussian random variables. Then, \\[\n  \\EXP\\Bigl[ \\max_{θ \\in \\mathsf{P}} θ^\\TRANS X \\Bigr] \\le σ \\sqrt{2 \\log n}\n\\quad\\text{and}\\quad\n  \\EXP\\Bigl[ \\max_{θ \\in \\mathsf{P}} | θ^\\TRANS X | \\Bigr] \\le σ \\sqrt{2 \\log 2n}.\n\\] Moreover, for any \\(t > 0\\), \\[\n  \\PR\\Bigl(  \\max_{θ \\in \\mathsf{P}} θ^\\TRANS X > t\\Bigr) \\le\n  n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr)\n\\quad\\text{and}\\quad\n  \\PR\\Bigl(  \\max_{θ \\in \\mathsf{P}} |θ^\\TRANS X| > t\\Bigr) \\le\n  2n \\exp\\biggl( -\\frac{t^2}{2σ^2} \\biggr).\n\\]\n\n\n\nMaximum over the \\(\\ell_2\\) ball\n\nTheorem 18.4 Let \\(X \\in \\reals^d\\) be a \\(σ\\)-sub-Gaussian random variable. Then, \\[ \\EXP[ \\max_{ \\| θ \\| \\le 1 } θ^\\TRANS X ] =\n   \\EXP[ \\max_{ \\| θ \\| \\le 1 } | θ^\\TRANS X | ] \\le 4σ \\sqrt{d}.\n\\] Moreover, for any \\(t > 0\\) \\[ \\PR( \\max_{ \\| θ \\| \\le 1 } θ^\\TRANS X > t) =\n   \\PR( \\max_{ \\| θ \\| \\le 1 } | θ^\\TRANS X | > t ) \\le\n  6^d \\exp\\biggl(- \\frac{t^2}{8σ^2} \\biggr).\n\\]\n\n\n\n\n\n\n\nRemark\n\n\n\nFor any \\(δ > 0\\), take \\(t = σ\\sqrt{8d \\log 6} + 2σ\\sqrt{2 \\log(1/δ)}\\), we obtain that with probability less than \\(1-δ\\), it holds that \\[\n  \\max_{\\|θ\\| \\le 1} θ^\\TRANS X\n  =\n  \\max_{\\|θ\\| \\le 1} | θ^\\TRANS X |\n  \\le 4σ\\sqrt{d} + 2σ \\sqrt{2\\log(1/δ)}.\n\\]"
  },
  {
    "objectID": "probability/sub-gaussian.html#lipschitz-functions-of-gaussian-variables.",
    "href": "probability/sub-gaussian.html#lipschitz-functions-of-gaussian-variables.",
    "title": "18  Sub-Gaussian random variables",
    "section": "18.8 Lipschitz functions of Gaussian variables.",
    "text": "18.8 Lipschitz functions of Gaussian variables.\nRecall that a function \\(f \\colon \\reals^d \\to \\reals\\) is \\(L\\)-Lipschitz with respect to the Eucledian norm if \\[\n  | f(x) - f(y) | \\le L \\| x - y \\|_2,\n  \\quad \\forall x, y \\in \\reals^d.\n\\]\nThe following results shows that any Lipschitz function of a Gaussian random variable is \\(L\\)-sub-Gaussian.\n\nTheorem 18.5 Let \\(X = (X_1, \\dots, X_n)\\) be a vector of i.i.d. standard Gaussian random variables and let \\(f \\colon \\reals^n \\to \\reals\\) be \\(L\\)-Lipschitz with respect to the Euclidean norm. Then, the variable \\(f(X) - \\EXP[ f(X) ]\\) is \\(L\\)-sub-Gaussian and therefore \\[\n  \\PR\\bigl[ \\bigl| f(X) - \\EXP[f(X)] \\bigr| \\ge t \\bigr]\n  \\le 2 \\exp\\biggl(- \\frac{t^2}{2L^2} \\biggr).\n\\]\n\nThis result is remarkable because it guarantees that any \\(L\\)-Lipschitz function of a standard Gaussian random vector, irrespective of the dimension, exhibits concetration like a scalar Gaussian variable with variance \\(L^2\\).\nFor a proof, see Chapter 2 of Wainwright (2019).\n\n\n\n\nPicard, J. 2007. Concentration inequalities and model selection. Springer Berlin Heidelberg. DOI: 10.1007/978-3-540-48503-2.\n\n\nRigollet, P. 2015. High-dimensional statistics. Available at: https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/.\n\n\nRivasplata, O. 2012. Subgaussian random variables: An expository note. Available at: http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf.\n\n\nWainwright, M.J. 2019. High-dimensional statistics. Cambridge University Press. DOI: 10.1017/9781108627771."
  },
  {
    "objectID": "linear-algebra/postive-definite-matrix.html#definite-and-basic-properties",
    "href": "linear-algebra/postive-definite-matrix.html#definite-and-basic-properties",
    "title": "19  Positive definite matrices",
    "section": "19.1 Definite and basic properties",
    "text": "19.1 Definite and basic properties\n\nDefinition 19.1 A \\(n \\times n\\) symmetric matrix \\(M\\) is called\n\npositive definite (written as \\(M > 0\\)) if for all \\(x \\in \\reals^n\\), \\(x \\neq 0\\), we have \\[x^\\TRANS M x > 0.\\]\npositive semi definite (written as \\(M \\ge 0\\)) if for all \\(x \\in \\reals^n\\), \\(x \\neq 0\\), we have \\[x^\\TRANS M x \\ge 0.\\]\n\n\n\nExamples\n\n\\(\\MATRIX{ x_1 & x_2 } \\MATRIX{ 3 & 0 \\\\ 0 & 2 } \\MATRIX{ x_1 \\\\ x_2 } = 3 x_1^2 + 2 x_2^2\\).\nThus, \\(\\MATRIX{ 3 & 0 \\\\ 0 & 2 } > 0\\).\n\\(\\MATRIX{ x_1 & x_2 } \\MATRIX{ 0 & 0 \\\\ 0 & 2 } \\MATRIX{ x_1 \\\\ x_2 } = 2 x_2^2\\).\nThus, \\(\\MATRIX{ 0 & 0 \\\\ 0 & 2 } \\ge 0\\)."
  },
  {
    "objectID": "linear-algebra/postive-definite-matrix.html#remarks-on-positive-definite-matrices",
    "href": "linear-algebra/postive-definite-matrix.html#remarks-on-positive-definite-matrices",
    "title": "19  Positive definite matrices",
    "section": "19.2 Remarks on positive definite matrices",
    "text": "19.2 Remarks on positive definite matrices\n\nBy making particular choices of \\(x\\) in the definition of positive definite matrix, we have that for a positive definite matrix \\(M\\),\n\n\\(M_{ii} > 0\\) for all \\(i\\)\n\\(M_{ij} < \\sqrt{M_{ii} M_{jj}}\\) for all \\(i \\neq j\\).\n\nHowever, satisfying these inequalities is not sufficient for positive definiteness.\nA symmetric matrix is positive definite (respt. postive semi-definite) if and only if all of its eigenvalues are positive (respt. non-negative).\nTherefore, a sufficient condition for a symmetric matrix to be positive definite is that all diagonal elements are positive and the matrix is diagonally dominant, i.e., \\(M_{ii} > \\sum_{j \\neq i} | M_{ij}|\\) for all {-} \\(i\\).\nIf \\(M\\) is symmetric positive definite, then so is \\(M^{-1}\\).\nIf \\(M\\) is symmetric positive definite, then \\(M\\) has a unique symmetric positive definite square root \\(R\\) (i.e., \\(RR = M\\)).\nIf \\(M\\) is symmetric positive definite, then \\(M\\) has a unique Cholesky factorization \\(M = T^\\TRANS T\\), where \\(T\\) is upper triangular with positive diagonal elements.\nThe set of positive semi-definite matrices forms a convex cone.\nPositive definiteness introduces a partial order on the convex cone of positive semi-definite matrices. In particular, we say that for two positive semi-definite matrices \\(M\\) and \\(N\\) of the same dimension, \\(M \\succeq N\\) if \\(M - N\\) is positive semi-definite. For this reason, often \\(M \\succ 0\\) and \\(M \\succeq 0\\) is used a short-hand to denote that \\(M\\) is positive definite and positive semi-definite.\nLet \\(M\\) is a symmetric square matrix. Let \\[ λ_1(M) \\ge λ_2(M) \\ge \\dots \\ge λ_n(M) \\] denote the ordered (real) eigenvalues of \\(M\\). Then \\[ λ_1(M)I \\succeq M \\succeq λ_n(M)I. \\]\nIf \\(M \\succeq N\\), then \\[ λ_k(M) \\ge λ_k(N), \\quad k \\in \\{1, \\dots, n\\}. \\]\nIf \\(M \\succeq N \\succ 0\\), then \\[ N^{-1} \\succeq M^{-1} \\succ 0. \\]\nIf \\(M \\succeq N\\) are \\(n × n\\) matrices and \\(T\\) is a \\(m × n\\) matrix, then \\[ T^\\TRANS M T \\succeq T^\\TRANS N T. \\]\nIf \\(M, N\\) are \\(n×\\) positive semi-definite matrices, then \\[ \\sum_{i=1}^k λ_i(M) λ_{n-i+1}(N) \\le\n  \\sum_{i=1}^k λ_i(MN) \\le\n  \\sum_{i=1}^k λ_i(M)λ_i(N),\n  \\quad k \\in \\{1, \\dots, n\\}.\n   \\] Note that this property does not require \\(M - N\\) to be positive or negative semi-definite.\nIf \\(M \\succ 0\\) and \\(T\\) are square matrices of the same size, then \\[ TMT + M^{-1} \\succeq 2T. \\]"
  },
  {
    "objectID": "linear-algebra/postive-definite-matrix.html#a-useful-relationship.",
    "href": "linear-algebra/postive-definite-matrix.html#a-useful-relationship.",
    "title": "19  Positive definite matrices",
    "section": "19.3 A useful relationship.",
    "text": "19.3 A useful relationship.\nSymmetric block matrices of the form\n\\[ C = \\MATRIX{ A & X \\\\ X^\\TRANS & B } \\]\noften appear in applications. If \\(A\\) is non-singular, we can write\n\\[\n\\MATRIX{A & X \\\\ X^\\TRANS & B } =\n\\MATRIX{I & 0 \\\\ X^\\TRANS A^{-1} & I}\n\\MATRIX{A & 0 \\\\ 0 & B - X^\\TRANS A^{-1} X }\n\\MATRIX{I & A^{-1} X \\\\ 0 & I }\n\\] which shows that \\(C\\) is congruent to a block diagonal matrix, which is positive definite when its diagonal blocks are postive definite. Therefore, \\(C\\) is positive definite if and only if both \\(A\\) and \\(B - X^\\TRANS A^{-1} X\\) are positive definite. The matrix \\(B = X^\\TRANS A^{-1} X\\) is called the Shur complement of \\(A\\) in \\(C\\)."
  },
  {
    "objectID": "linear-algebra/postive-definite-matrix.html#determinant-bounds",
    "href": "linear-algebra/postive-definite-matrix.html#determinant-bounds",
    "title": "19  Positive definite matrices",
    "section": "19.4 Determinant bounds",
    "text": "19.4 Determinant bounds\n\nProposition 19.1 (Fischer’s inequality) Suppose \\(A\\) and \\(C\\) are positive semidefinite matrix and \\[ M = \\MATRIX{A & B \\\\ B^\\TRANS & C}. \\] Then \\[ \\det(M) \\le \\det(A) \\det(C). \\]\n\nRecursive application of Fischer’s inequality gives the Hadamard’s inequality for a symmetric positive definite matrix: \\[ \\det(A) \\le A_{11} A_{22} \\cdots A_{nn}, \\] with equality if and only if \\(A\\) is diagonal.\n\nProposition 19.2 If \\(M \\succ N \\succ 0\\) are \\(n × n\\) matrices and \\(T\\) is a \\(m × n\\) matrix, then \\[ \\sup_{ T \\neq 0} \\frac{ \\| T^\\TRANS M T \\| }{ \\| T^\\TRANS N T \\|}\n   \\le \\frac{ \\det(M) }{ \\det(N) }, \\] where for any matrix \\(M\\), \\[\n  \\| M \\| = \\sup_{x \\neq 0} \\frac{ \\| M x \\|_2 }{ \\|x\\|_2 }\n\\] is the \\(2\\)-norm of the matrix.\n\nProposition 19.2 is taken from (Abbasi-Yadkori2011?)."
  },
  {
    "objectID": "linear-algebra/postive-definite-matrix.html#references",
    "href": "linear-algebra/postive-definite-matrix.html#references",
    "title": "19  Positive definite matrices",
    "section": "References",
    "text": "References\nThe properties of positive definite matrices are stated in any book on the theory of matrices. See for example Marshall et al. (2011).\nHistorically, a matrix used as a test matrix for testing positive definiteness was the Wilson matrix \\[ W = \\MATRIX{5 & 7 & 6 & 5 \\\\ 7 & 10 & 8 & 7 \\\\ 6 & 8 & 10 & 9 \\\\ 5 & 7 & 9\n& 10}. \\] For a nice overview of Wilson matrix, see this blog post.\n\n\n\n\n\nMarshall, A.W., Olkin, I., and Arnold, B.C. 2011. Inequalities: Theory of majorization and its applications. Springer New York. DOI: 10.1007/978-0-387-68276-1."
  },
  {
    "objectID": "linear-algebra/svd.html",
    "href": "linear-algebra/svd.html",
    "title": "20  Singular value decomposition",
    "section": "",
    "text": "21 Best rank-\\(k\\) approximations\nThere are two important matrix norms, the Frobenius norm which is defined as \\[\n  \\| A \\|_{F} = \\sqrt{ \\sum_{i,j} a_{ij}^2 }\n\\] and the induced norm which is defined as \\[\n  \\| A \\|_2 = \\max_{\\|x \\| = 1} \\| A x \\|.\n\\]\nNote that the Frobenius norm is equal to the square root of the sum of squares of the singular values and the \\(2\\)-norm is the largest singular value.\nLet \\(A\\) be an \\(n × d\\) matrix and think of \\(A\\) as the \\(n\\) points in \\(d\\)-dimensional space. The Frobenius norm of \\(A\\) is the square root of the sum of squared distance of the points to the origin. The induced norm is the square root of the sum of squared distances to the origin along the direction that maximizes this quantity."
  },
  {
    "objectID": "linear-algebra/svd.html#singular-values",
    "href": "linear-algebra/svd.html#singular-values",
    "title": "20  Singular value decomposition",
    "section": "20.1 Singular values",
    "text": "20.1 Singular values\nLet \\(A\\) be a \\(n × d\\) matrix. Then, the matrix \\(A^\\TRANS A\\) is a symmetric \\(d × d\\) matrix, so its eigenvalues are real. Moreover, \\(A^\\TRANS A\\) is positive semi-definite, so the eigen values are non-negative. Let \\(\\{ λ_1, \\dots, λ_d \\}\\) denote the eigenvalues of \\(A^\\TRANS A\\), with repetitions. Order then so that \\(λ_1 \\ge λ_2 \\ge \\dots \\ge λ_d \\ge 0\\). Let \\(σ_i = \\sqrt{λ_i}\\), so that \\(σ_1 \\ge σ_2 \\ge \\dots σ_d \\ge 0\\). These numbers are called the singular values of \\(A\\).\n\nProperties of singular values\n\nThe number of non-zero singular values of \\(A\\) equals to the rank of \\(A\\). In particular, if \\(A\\) is \\(n × d\\) where \\(n < d\\), then \\(A\\) has at most \\(n\\) nonzero singular values.\nIt can be shown that\n\\[ σ_1 = \\max_{\\|x\\| = 1}  \\| A x \\| . \\]\nLet \\(v_1\\) denote the arg-max of the above optimization. \\(v_1\\) is called the first singular vector of \\(A\\). Then,\n\\[ σ_2 = \\max_{ x \\perp v_1, \\|x \\| = 1}  \\| A x\\|. \\]\nLet \\(v_2\\) denote the arg-max of the above optimization. \\(v_2\\) is called the second singular vector of \\(A\\), and so on.\nLet \\(A\\) be a \\(n × d\\) matrix and \\(v_1, \\dots, v_r\\) be the singular vectors, where \\(r = \\text{rank}(A)\\). Then for any \\(k \\in \\{1, \\dots, r\\}\\), let \\(V_k\\) be the subspace spanned by \\(\\{v_1, \\dots, v_k\\}\\). Then, \\(V_k\\) is the best \\(k\\)-dimensional subspace for \\(A\\).\nFor any matrix \\(A\\), \\[ \\sum_{i =1}^r σ_i^2(A) = \\| A \\|_{F}^2\n   := \\sum_{j,k} a_{jk}^2. \\]\nAny vector \\(v\\) can be written as a linear combination of \\(v_1, \\dots, v_r\\) and a vector perpendicular to \\(V_r\\) (defined above). Now, \\(Av\\) can be written as the same linear combination of \\(Av_1, Av_2, \\dots, Av_r\\). So, \\(Av_1, \\dots, Av_r\\) form a fundamental set of vectors associated with \\(A\\). We normalize them to length one by \\[ u_i = \\frac{1}{σ_i(A)} A v_i. \\] The vectors \\(u_1, \\dots, u_r\\) are called the left singular vectors of \\(A\\). The \\(v_i\\) are called the right singular vectors.\nBoth the left and the right singular vectors are orthogonal.\n\n\n\n\n\n\n\nSingular value decomposition\n\n\n\nFor any matrix \\(A\\), \\[ A = \\sum_{i=1}^r σ_i u_i v_i^\\TRANS \\] where \\(u_i\\) and \\(v_i\\) are the left and right singular vectors, and \\(σ_i\\) are the singular values.\nEquivalently, in matrix notation: \\[ A = U D V^\\TRANS \\] where the columns of \\(U\\) and \\(V\\) consist of the left and right singular vectors, respectively, and \\(D\\) is a diagonal matrix whose diagonal entries are the singular values of \\(A\\).\n\n\nIf \\(A\\) is a positive definite square matrix, then the SVD and the eigen-decomposition coincide."
  },
  {
    "objectID": "linear-algebra/svd.html#notes",
    "href": "linear-algebra/svd.html#notes",
    "title": "20  Singular value decomposition",
    "section": "Notes",
    "text": "Notes\nThe chapter on SVD in Hopcroft and Kannan (2012) contains a nice intuitive explanation of SVD.\n\n\n\n\nHopcroft, J. and Kannan, R. 2012. Computer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf."
  },
  {
    "objectID": "linear-algebra/rkhs.html#review-of-linear-operators",
    "href": "linear-algebra/rkhs.html#review-of-linear-operators",
    "title": "21  Reproducing Kernel Hilbert Space",
    "section": "21.1 Review of Linear Operators",
    "text": "21.1 Review of Linear Operators\n\n\n\n\n\n\nLinear Operator\n\n\n\nLet \\(\\mathcal F\\) and \\(\\mathcal G\\) be normed vector spaces over \\(\\reals\\). A function \\(A \\colon \\mathcal F \\to \\mathcal G\\) is called a linear operator if it satisfies the following properties:\n\nHonogeneity: For any \\(α \\in \\reals\\) and \\(f \\in \\mathcal F\\), \\(A(αf) = α (Af)\\).\nAdditivity: For any \\(f,g \\in \\mathcal F\\), \\(A(f + g) = Af + Ag\\).\n\nThe operator norm of a linear operator is defined as \\[ \\NORM{A} = \\sup_{f \\in \\mathcal F} \\frac{ \\NORM{A f}_{\\mathcal G}}\n{\\NORM{f}}_{\\mathcal F}. \\]\nIf \\(\\NORM{A} < ∞\\), then the operator is said to be a bounded operator.\n\n\nAs an example, suppose \\(\\mathcal F\\) is an inner product space. For a \\(g \\in \\mathcal F\\), the operator \\(A_g \\colon \\mathcal F \\to \\reals\\) defined by \\(A_g(f) = \\langle f, g \\rangle\\) is a linear operator. Such scalar valued operators are called functionals on \\(\\mathcal F\\).\nLinear operators satisfy the following property.\n\nTheorem 21.1 If \\(A \\colon \\mathcal F \\to \\mathcal G\\) is a linear operator, then the following three conditions are equivalent:\n\n\\(A\\) is a bounded operator.\n\\(A\\) is continuous on \\(\\mathcal F\\).\n\\(A\\) is continious at one point of \\(\\mathcal F\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "href": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "title": "21  Reproducing Kernel Hilbert Space",
    "section": "21.2 Dual of a linear operator",
    "text": "21.2 Dual of a linear operator\nThere are two notions of dual of a linear operator: algebraic dual and topological dual. If \\(\\mathcal F\\) is a normed space, then the space of all linear functionals \\(A \\colon \\mathcal F \\to \\reals\\) is the algebraic dual space of \\(\\mathcal F\\); the space of all continuous linear functions \\(A \\colon \\mathcal F \\to \\reals\\) is the topological dual space of \\(\\mathcal F\\).\nIn finite-dimensional space, the two notions of dual spaces coincide (every linear operator on a normed, finite dimensional space is bounded). But this is not the case for infinite dimensional spaces.\n\nTheorem 21.2 (Riesz representation) In a Hilbert space \\(\\mathcal F\\), all continuous linear functionals are of the form \\(\\langle\\cdot, g\\rangle\\), for some \\(g \\in \\mathcal F\\).\n\nTwo Hilbert spaces \\(\\mathcal F\\) and \\(\\mathcal G\\) are said to be isometrically isomorphic if there is a linear bijective map \\(U \\colon \\mathcal F \\to \\mathcal G\\) which preserves the inner product, i.e., \\(\\langle f_1, f_2 \\rangle_{\\mathcal F} = \\langle U f_1, U f_2 \\rangle_{\\mathcal G}\\).\nNote that Riesz representation theorem gives a natural isometric isomorphism \\(\\psi \\colon g \\mapsto \\langle \\cdot, g \\rangle_{\\mathcal F}\\) between \\(\\mathcal F\\) and its topological dual \\(\\mathcal F'\\), whereby \\(\\NORM{ψ(g)}_{\\mathcal F'} = \\NORM{g}_{\\mathcal F}\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "href": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "title": "21  Reproducing Kernel Hilbert Space",
    "section": "21.3 Reproducing kernel Hilbert space",
    "text": "21.3 Reproducing kernel Hilbert space\nLet \\(\\mathcal H\\) be a Hilbert space of functions mapping from some non-empty set \\(\\ALPHABET X\\) to \\(\\reals\\). Note that for every \\(x \\in \\ALPHABET X\\), there is a very special functional on \\(\\mathcal H\\): the one that assigns to each \\(f \\in \\mathcal H\\), its value at \\(x\\). This is called the evaluation functional and denoted by \\(δ_x\\). In particular, \\(δ_x \\colon \\mathcal H \\to \\reals\\), where \\(δ_x \\colon f \\mapsto f(x)\\).\n\n\n\n\n\n\nReproducing kernel Hilbert space (RKHS)\n\n\n\nA Hilbert space \\(\\mathcal H\\) of functions \\(f \\colon \\ALPHABET X \\to \\reals\\) defined on a non-empty set \\(\\ALPHABET X\\) is said to be a RKHS if \\(δ_x\\) is continuous for all \\(x \\in \\ALPHABET X\\).\n\n\nIn view of Theorem 21.1, an equivalent definition is that a Hilbert space \\(\\mathcal H\\) is RKHS if the evaluation functionals \\(δ_x\\) are bounded, i.e., for every \\(x \\in \\ALPHABET X\\), there exists a \\(M_x\\) such that \\[ | δ_x | = | f(x) | \\le M_x \\| f \\|_{\\mathcal H}, \\quad \\forall f \\in \\mathcal H\\]\nAn immediate implication of the above property is that two functions which agree in RKHS norm agree at every point: \\[ | f(x) - g(x) | = | δ_x(f - g) | \\le M_x \\| f - g \\|_{\\mathcal H},\n   \\quad \\forall f,g \\in \\mathcal H. \\]\nFor example, the \\(L_2\\) space of square integrable functions i.e., \\(\\int_{\\reals^n} f(x)^2 dx < ∞\\) with inner product \\(\\int_{\\reals^n} f(x) g(x)dx\\) is a Hilbert space, but not an RKHS because the delta function, which has the reproducing property \\[ f(x) = \\int_{\\reals^n} δ(x - y) f(y) dy \\] is not bounded.\nRKHS are particularly well behaved. In particular, if we have a sequence of functions \\(\\{f_n\\}_{n \\ge 1}\\) which converges to a limit \\(f\\) in the Hilbert-space norm, i.e., \\(\\lim_{n \\to ∞} \\NORM{f_n - f}_{\\mathcal H} = 0\\), then they also converge pointwise, i.e., \\(\\lim_{n \\to ∞} f_n(x) = f(x)\\) for all \\(x \\in \\ALPHABET X\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-rhks",
    "href": "linear-algebra/rkhs.html#properties-of-rhks",
    "title": "21  Reproducing Kernel Hilbert Space",
    "section": "21.4 Properties of RHKS",
    "text": "21.4 Properties of RHKS\nRKHS has many useful properties:\n\nFor any RKHS, there exists a unique kernel \\(k \\colon \\ALPHABET X × \\ALPHABET X \\to \\reals\\) such that\n\nfor any \\(x \\in \\ALPHABET X\\), \\(k(\\cdot, x) \\in \\mathcal H\\),\nfor any \\(x \\in \\ALPHABET X\\) and \\(f \\in \\mathcal H\\), \\(\\langle f, k(\\cdot, x) \\rangle = f(x)\\) (the reproducing property).\n\nIn particular, for any \\(x,y \\in \\ALPHABET X\\), \\[ k(x,y) = \\langle k(\\cdot, x), k(\\cdot, y) \\rangle. \\] Thus, the kernel is a symmetric function.\nThe kernel is positive definite, i.e., for any \\(n \\ge 1\\), for all \\((a_1, \\dots, a_n) \\in \\reals^n\\) and \\((x_1, \\dots, x_n) \\in \\ALPHABET X^n\\), \\[ \\sum_{i=1}^n \\sum_{j=1}^n a_i a_i h(x_i, x_j) \\ge 0 \\]\nA conseuqence of positive definiteness is that \\[| k(x, y)|^2 \\le k(x, x) k(y, y). \\]\n(Moore-Aronszajn Theorem) For every positive definite kernel \\(K\\) on \\(\\ALPHABET X × \\ALPHABET X\\), there is a unique RKHS on \\(\\ALPHABET X\\) with \\(K\\) as its reproducing kernel."
  },
  {
    "objectID": "linear-algebra/rkhs.html#examples-of-kernels",
    "href": "linear-algebra/rkhs.html#examples-of-kernels",
    "title": "21  Reproducing Kernel Hilbert Space",
    "section": "21.5 Examples of kernels",
    "text": "21.5 Examples of kernels\nSome common examples of symmetric positive definite kernels for \\(\\ALPHABET X = \\reals^n\\) are as follows:\n\nLinear kernel \\[ k(x,y) = \\langle x, y \\rangle\\]\nGaussian kernel \\[ k(x,y) = \\exp\\biggl( - \\frac{\\| x - y \\|^2}{σ^2} \\biggr),\n   \\quad σ > 0. \\]\nPolynomail kernel \\[ k(x,y) = \\bigl( 1 + \\langle x, y \\rangle \\bigr)^d,\n   \\quad d \\in \\integers_{> 0}. \\]"
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-kernels",
    "href": "linear-algebra/rkhs.html#properties-of-kernels",
    "title": "21  Reproducing Kernel Hilbert Space",
    "section": "21.6 Properties of kernels",
    "text": "21.6 Properties of kernels\n\nSuppose \\(φ \\colon \\ALPHABET X \\to \\reals^n\\) is a feature map, then \\[ k(x,y) := \\langle φ(x), φ(y) \\rangle \\] is a kernel.\nNote that there are no conditions on \\(\\ALPHABET X\\) (e.g., \\(\\ALPHABET X\\) doesn’t need to be an inner product space).\nIf \\(k\\) is a kernel on \\(\\ALPHABET X\\), then for any \\(α > 0\\), \\(αk\\) is also a kernel.\nIf \\(k_1\\) and \\(k_2\\) are kernels on \\(\\ALPHABET X\\), then \\(k_1 + k_2\\) is also a kernel.\nIf \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be arbitrary sets and \\(A \\colon \\ALPHABET X \\to \\ALPHABET Y\\) is a map. Let \\(k\\) be a kernel on \\(\\ALPHABET Y\\). Then, \\(k(A(x_1), A(x_2))\\) is a kernel on \\(\\ALPHABET X\\).\nIf \\(k_1 \\colon \\ALPHABET X_1 × \\ALPHABET X_1 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_1\\) and \\(k_2 \\colon \\ALPHABET X_2 × \\ALPHABET X_2 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_2\\), then \\[ k( (x_1, x_2), (y_1, y_2) ) = k_1(x_1, y_1) k_2(x_2, y_2) \\] is a kernel on \\(\\ALPHABET X_1 × \\ALPHABET X_2\\).\n(Mercer-Hilber-Schmit theorems) If \\(k\\) is positive definite kernel (that is continous with finite trace), then there exists an infinite sequence of eiegenfunctions \\(\\{ φ_i \\colon \\ALPHABET X \\to \\reals \\}_{i \\ge 1}\\) and real eigenvalues \\(\\{λ_i\\}_{i \\ge 1}\\) such that we can write \\(k\\) as: \\[ k(x,y) = \\sum_{i=1}^∞ λ_i φ_i(x) φ_i(y). \\] This is analogous to the expression of a matrix in terms of its eigenvector and eigenvalues, except in this case we have functions and an infinity of them.\nUsing this property, we can define the inner product of RKHS in a simpler form. First, for any \\(f \\in \\mathcal H\\), define \\[ f_i = \\langle f, φ_i \\rangle.\\] Then, for any \\(f, g \\in \\mathcal H\\), \\[ \\langle f, g \\rangle = \\sum_{i=1}^∞ \\frac{ f_i g_i } { λ_i }. \\]"
  },
  {
    "objectID": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "href": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "title": "21  Reproducing Kernel Hilbert Space",
    "section": "21.7 Kernel ridge regression",
    "text": "21.7 Kernel ridge regression\nGiven labelled data \\(\\{ (x_i, y_i) \\}_{i=1}^n\\), and a feature map \\(φ \\colon \\ALPHABET X \\to \\ALPHABET Z\\), define the RKHS \\(\\ALPHABET H\\) of functions from \\(\\ALPHABET Z \\to \\reals\\) with the kernel \\(k(x,y) = \\langle φ(x), φ(y) \\rangle_{\\mathcal H}\\). Now, consider the problem of minimizing\n\\[f^* = \\arg \\min_{f \\in \\ALPHABET H}\n\\biggl(\n  \\sum_{i=1}^n \\bigl( y_i - \\langle f, φ(x_i) \\rangle_{\\mathcal{H}} \\bigr)^2 +\n  λ \\NORM{f}^2_{\\mathcal H}\n\\bigr).\\]\n\nTheorem 21.3 (The representer theoreom (simple version)) Given a loss function \\(\\ell \\colon \\ALPHABET Z^n \\to \\reals\\) and a penalty function \\(Ω \\colon \\reals \\to \\reals\\), there is as a solution of \\[ f^* = \\arg \\min_{f \\in \\mathcal H} \\ell(f(x_1), \\dots, f(x_n))\n        + Ω(\\NORM{f}^2_{\\mathcal H}). \\] that takes the the form \\[ f^* = \\sum_{i=1}^n α_i k(\\cdot, x_i).\\]\nIf \\(Ω\\) is strictly increasing, all solutions have this form.\n\nUsing the representer theorem, we know that the solution is of the form \\[ f = \\sum_{i=1}^n α_i φ(x_i). \\] Then, \\[\n\\sum_{i=1}^n \\bigl( y_i - \\langle f, φ_i(x_i) \\rangle_{\\mathcal H} \\bigr)^2\n  + λ \\NORM{f}_{\\mathcal H}^2\n= \\NORM{ y - K α}^2 + λ α^\\TRANS K α. \\]\nDifferentiating wrt \\(α\\) and setting this to zero, we get \\[\n  α^* = (K + λI_n)^{-1} y.\n\\]"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Altman, Eitan. 1999. Constrained\nmarkov decision processes. CRC Press. Available at: http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf.\n\n\nArrow, K.J., Harris, T., and Marschak, J.\n1952. Optimal inventory policy. Econometrica 20, 1,\n250–272. DOI: 10.2307/1907830.\n\n\nBellman, R., Glicksberg, I., and Gross,\nO. 1955. On the optimal inventory equation. Management\nScience 2, 1, 83–104. DOI: 10.1287/mnsc.2.1.83.\n\n\nBerry, R.A. 2000. Power and delay\ntrade-offs in fading channels. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay\ntradeoffs in fading channels—small-delay asymptotics. IEEE\nTransactions on Information Theory 59, 6, 3939–3952. DOI:\n10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002.\nCommunication over fading channels with delay constraints.\nIEEE Transactions on Information Theory\n48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M.\n2012. Energy-efficient scheduling under delay constraints for wireless\nnetworks. Synthesis Lectures on Communication Networks\n5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nBertsekas, D.P. 2011. Dynamic\nprogramming and optimal control. Athena Scientific. Available at:\nhttp://www.athenasc.com/dpbook.html.\n\n\nBitar, E., Poolla, K., Khargonekar, P.,\nRajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind.\n2012 45th hawaii international conference on system sciences,\nIEEE, 1931–1937.\n\n\nBlackwell, D. 1964. Memoryless strategies\nin finite-stage dynamic programming. The Annals of Mathematical\nStatistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nBohlin, T. 1970. Information pattern for\nlinear discrete-time models with stochastic coefficients. IEEE\nTransactions on Automatic Control (TAC) 15, 1, 104–106.\n\n\nCassandra, A., Littman, M.L., and Zhang,\nN.L. 1997. Incremental pruning: A simple, fast, exact method for\npartially observable Markov decision processes.\nProceedings of the thirteenth conference on uncertainty\nin artificial intelligence.\n\n\nCassandra, A.R., Kaelbling, L.P., and Littman,\nM.L. 1994. Acting optimally in partially observable stochastic\ndomains. AAAI, 1023–1028.\n\n\nChakravorty, J. and Mahajan, A. 2018.\nSufficient conditions for the value function and optimal strategy to be\neven and quasi-convex. IEEE Transactions on Automatic Control\n63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nCheng, H.-T. 1988. Algorithms for\npartially observable markov decision processes.\n\n\nDaley, D.J. 1968. Stochastically monotone\nmarkov chains. Zeitschrift für\nWahrscheinlichkeitstheorie und verwandte Gebiete 10, 4,\n305–317. DOI: 10.1007/BF00531852.\n\n\nDavis, M.H.A. and Varaiya, P.P. 1972.\nInformation states for linear stochastic systems. Journal of\nMathematical Analysis and Applications 37, 2, 384–402.\n\n\nDevlin, S. 2014. Potential based reward\nshaping tutorial. Available at: http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf.\n\n\nDevlin, S. and Kudenko, D. 2012. Dynamic\npotential-based reward shaping. Proceedings of the 11th\ninternational conference on autonomous agents and multiagent\nsystems, International Foundation for Autonomous Agents; Multiagent\nSystems, 433–440.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A.\n2016. On monotonicity of the optimal transmission policy in cross-layer\nadaptive m -QAM modulation.\nIEEE Transactions on Communications 64, 9, 3771–3785.\nDOI: 10.1109/TCOMM.2016.2590427.\n\n\nEdgeworth, F.Y. 1888. The mathematical\ntheory of banking. Journal of the Royal Statistical Society\n51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nFeinberg, E.A. and He, G. 2020.\nComplexity bounds for approximately solving discounted MDPs\nby value iterations. Operations Research Letters. DOI: 10.1016/j.orl.2020.07.001.\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004.\nOptimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nGrzes, M. and Kudenko, D. 2009.\nTheoretical and empirical analysis of reward shaping in reinforcement\nlearning. International conference on machine learning and\napplications, 337–344. DOI: 10.1109/ICMLA.2009.33.\n\n\nHarris, F.W. 1913. How many parts to make\nat once. The magazine of management 10, 2, 135–152.\nDOI: 10.1287/opre.38.6.947.\n\n\nHinderer, K. 2005. Lipschitz continuity\nof value functions in Markovian decision processes.\nMathematical Methods of Operations Research 62, 1,\n3–22. DOI: 10.1007/s00186-005-0438-1.\n\n\nHopcroft, J. and Kannan, R. 2012.\nComputer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf.\n\n\nHoward, R.A. 1960. Dynamic\nprogramming and markov processes. The M.I.T. Press.\n\n\nJenner, E., Hoof, H. van, and Gleave, A.\n2022. Calculus on MDPs: Potential shaping as a gradient. Available at:\nhttps://arxiv.org/abs/2208.09570v1.\n\n\nKeilson, J. and Kester, A. 1977. Monotone\nmatrices and monotone markov processes. Stochastic Processes and\ntheir Applications 5, 3, 231–241.\n\n\nKelly, J.L., Jr. 1956. A new\ninterpretation of information rate. Bell System Technical\nJournal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nKoole, G. 2006. Monotonicity in markov\nreward and decision chains: Theory and applications. Foundations and\nTrends in Stochastic Systems 1, 1, 1–76. DOI:\n10.1561/0900000002.\n\n\nKumar, P.R. and Varaiya, P. 1986.\nStochastic systems: Estimation identification and adaptive\ncontrol. Prentice Hall.\n\n\nKwakernaak, H. 1965. Theory of\nself-adaptive control systems. In: Springer, 14–18.\n\n\nLevy, H. 1992. Stochastic dominance and\nexpected utility: Survey and analysis. Management Science\n38, 4, 555–593. DOI: 10.1287/mnsc.38.4.555.\n\n\nLevy, H. 2015. Stochastic dominance:\nInvestment decision making under uncertainty. Springer. DOI: 10.1007/978-3-319-21708-6.\n\n\nMarshall, A.W., Olkin, I., and Arnold,\nB.C. 2011. Inequalities: Theory of majorization and its\napplications. Springer New York. DOI: 10.1007/978-0-387-68276-1.\n\n\nMorse, P. and Kimball, G. 1951.\nMethods of operations research. Technology Press of MIT.\n\n\nNerode, A. 1958. Linear automaton\ntransformations. Proceedings of American Mathematical\nSociety 9, 541–544.\n\n\nNg, A.Y., Harada, D., and Russell, S.\n1999. Policy invariance under reward transformations: Theory and\napplication to reward shaping. ICML, 278–287. Available at: http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf.\n\n\nPicard, J. 2007. Concentration\ninequalities and model selection. Springer Berlin Heidelberg. DOI:\n10.1007/978-3-540-48503-2.\n\n\nPorteus, E.L. 1975. Bounds and\ntransformations for discounted finite markov decision chains.\nOperations Research 23, 4, 761–784. DOI: 10.1287/opre.23.4.761.\n\n\nPorteus, E.L. 2008. Building intuition:\nInsights from basic operations management models and principles. In: D.\nChhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nPuterman, M.L. 2014. Markov decision\nprocesses: Discrete stochastic dynamic programming. John Wiley\n& Sons. DOI: 10.1002/9780470316887.\n\n\nRachelson, E. and Lagoudakis, M.G. 2010.\nOn the locality of action domination in sequential decision making.\nProceedings of 11th international symposium on artificial\nintelligence and mathematics. Available at: https://oatao.univ-toulouse.fr/17977/.\n\n\nRigollet, P. 2015. High-dimensional\nstatistics. Available at: https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/.\n\n\nRivasplata, O. 2012. Subgaussian random\nvariables: An expository note. Available at: http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf.\n\n\nRoss, S.M. 1974. Dynamic programming and\ngambling models. Advances in Applied Probability 6, 3,\n593–606. DOI: 10.2307/1426236.\n\n\nSayedana, B. and Mahajan, A. 2020.\nCounterexamples on the monotonicity of delay optimal strategies for\nenergy harvesting transmitters. IEEE Wireless\nCommunications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E.\n2020. Cross-layer communication over fading channels with adaptive\ndecision feedback. International symposium on modeling and\noptimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nSerfozo, R.F. 1976. Monotone optimal\npolicies for markov decision processes. In: Mathematical programming\nstudies. Springer Berlin Heidelberg, 202–215. DOI: 10.1007/bfb0120752.\n\n\nShwartz, A. 2001. Death and discounting.\nIEEE Transactions on Automatic Control\n46, 4, 644–647. DOI: 10.1109/9.917668.\n\n\nSkinner, B.F. 1938. Behavior of\norganisms. Appleton-Century.\n\n\nSmallwood, R.D. and Sondik, E.J. 1973.\nThe optimal control of partially observable markov processes over a\nfinite horizon. Operations Research 21, 5, 1071–1088.\nDOI: 10.1287/opre.21.5.1071.\n\n\nStriebel, C. 1965. Sufficient statistics\nin the optimal control of stochastic systems. Journal of\nMathematical Analysis and Applications 12, 576–592.\n\n\nSubramanian, J., Sinha, A., Seraj, R., and\nMahajan, A. 2022. Approximate information state for approximate\nplanning and reinforcement learning in partially observed systems.\nJournal of Machine Learning Research 23, 12, 1–83.\nAvailable at: http://jmlr.org/papers/v23/20-1165.html.\n\n\nTopkis, D.M. 1998. Supermodularity\nand complementarity. Princeton University Press.\n\n\nTsitsiklis, J.N. 1984. Periodic review\ninventory systems with continuous demand and discrete order sizes.\nManagement Science 30, 10, 1250–1254. DOI: 10.1287/mnsc.30.10.1250.\n\n\nUrgaonkar, R., Wang, S., He, T., Zafer, M.,\nChan, K., and Leung, K.K. 2015. Dynamic service migration and\nworkload scheduling in edge-clouds. Performance Evaluation\n91, 205–228. DOI: 10.1016/j.peva.2015.06.013.\n\n\nVeinott, A.F. 1965. The optimal inventory\npolicy for batch ordering. Operations Research 13, 3,\n424–432. DOI: 10.1287/opre.13.3.424.\n\n\nWainwright, M.J. 2019.\nHigh-dimensional statistics. Cambridge University Press. DOI:\n10.1017/9781108627771.\n\n\nWang, S., Urgaonkar, R., Zafer, M., He, T.,\nChan, K., and Leung, K.K. 2019. Dynamic service migration in\nmobile edge computing based on Markov decision process.\nIEEE/ACM Transactions on Networking\n27, 3, 1272–1288. DOI: 10.1109/tnet.2019.2916577.\n\n\nWhitin, S. 1953. The theory of\ninventory management. Princeton University Press.\n\n\nWhittle, P. 1982. Optimization over\ntime: Dynamic programming and stochastic control. Vol. 1 and 2.\nWiley.\n\n\nWhittle, P. 1996. Optimal control:\nBasics and beyond. Wiley.\n\n\nWhittle, P. and Komarova, N. 1988. Policy\nimprovement and the newton-raphson algorithm. Probability in the\nEngineering and Informational Sciences 2, 2, 249–255. DOI:\n10.1017/s0269964800000760.\n\n\nWiewiora, E. 2003. Potential-based\nshaping and q-value initialization are equivalent. Journal of\nArtificial Intelligence Research 19, 1, 205–208.\n\n\nWitsenhausen, H.S. 1975. On policy\nindependence of conditional expectation. Information and\nControl 28, 65–75.\n\n\nWitsenhausen, H.S. 1976. Some remarks on\nthe concept of state. In: Y.C. Ho and S.K. Mitter, eds., Directions\nin large-scale systems. Plenum, 69–75.\n\n\nWitsenhausen, H.S. 1979. On the structure\nof real-time source coders. Bell System Technical Journal\n58, 6, 1437–1451.\n\n\nYeh, E.M. 2012. Fundamental performance\nlimits in cross-layer wireless optimization: Throughput, delay, and\nenergy. Foundations and Trends in Communications and Information\nTheory 9, 1, 1–112. DOI: 10.1561/0100000014.\n\n\nZhang, H. 2009. Partially observable\nMarkov decision processes: A geometric technique and\nanalysis. Operations Research.\n\n\nZhang, N. and Liu, W. 1996. Planning\nin stochastic domains: Problem characteristics and approximation.\nHong Kong Univeristy of Science; Technology."
  },
  {
    "objectID": "assignments/01.html",
    "href": "assignments/01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Exercise 2.1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 2.2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 3.3 from the notes on the newsvendor problem. Provide an analytic solution to the problem, similar to the derivation of the analytic solution for the case of continuous demand and actions in the notes."
  }
]