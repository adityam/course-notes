[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Notes",
    "section": "",
    "text": "About the course"
  },
  {
    "objectID": "stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "href": "stochastic-optimization/intro.html#the-stochastic-optimization-problem",
    "title": "1  Introduction",
    "section": "1.1 The stochastic optimization problem",
    "text": "1.1 The stochastic optimization problem\nNow consider the simplest stochastic optimization problem. A decision maker has to choose an action \\(a \\in \\ALPHABET A\\). Upon choosing the action \\(a\\), the decision maker incurs a cost \\(c(a,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable with known probability distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(a, W) ]\\), where the expectation is with respect to the random variable \\(W\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:stochastic}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(a, W) ].\n\\end{equation}\\]\nDefine \\(J(a) = \\EXP[ c(a, W) ]\\). Then Problem \\eqref{eq:stochastic} is conceptually the same as Problem \\eqref{eq:basic} with the cost function \\(J(a)\\). Numerically, Problem \\eqref{eq:stochastic} is more difficult because computing \\(J(a)\\) involves evaluating an expectation, but we ignore the computational complexity for the time being."
  },
  {
    "objectID": "stochastic-optimization/intro.html#key-simplifying-idea",
    "href": "stochastic-optimization/intro.html#key-simplifying-idea",
    "title": "1  Introduction",
    "section": "1.2 Key simplifying idea",
    "text": "1.2 Key simplifying idea\nIn the stochastic optimization problems considered above, the decision maker does not observe any data before making a decision. In many situations, the decision maker does observe some data, which is captured by the following model. Suppose a decision maker observes a random variable \\(S \\in \\ALPHABET S\\) and then chooses an action \\(A \\in \\ALPHABET A\\) as a function of his observation according to a decision rule \\(π\\), i.e., \\[ A = π(S). \\]\nUpon choosing the action \\(A\\), the decision maker incurs a cost \\(c(S,A,W)\\), where \\(W \\in \\ALPHABET W\\) is a random variable. We assume that the primitive random variables \\((S,W)\\) are defined on a common probability space and have a known joint distribution. Assume that the decision maker is risk neutral and, therefore, wants to minimize \\(\\EXP[ c(S, π(S), W)]\\), where the expectation is taken with respect to the joint probability distribution of \\((S,W)\\).\nFormally, the above optimization problem may be written as \\[\\begin{equation} \\label{eq:obs} \\tag{P1}\n  \\min_{π \\colon \\ALPHABET S \\to \\ALPHABET A} \\EXP[ c(S, π(S), W) ].\n\\end{equation}\\]\nDefine \\(J(π) = \\EXP[ c(S, π(S), W) ]\\). Then, Problem \\eqref{eq:obs} is conceptually the same as Problem \\eqref{eq:basic} with one difference: In Problem \\eqref{eq:basic}, the minimization is over a parameter \\(a\\), while in Problem \\eqref{eq:obs}, the minimization is over a function \\(π\\).\nWhen \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are finite sets, the optimal policy can be obtained by an exhaustive search over all policies as follows: for each policy \\(π\\) compute the performance \\(J(π)\\) and then pick the policy \\(π\\) with the smallest expected cost.\nSuch an exhaustive search is not satisfying for two reasons. First, it has a high computational cost. There are \\(| \\ALPHABET A |^{| \\ALPHABET S |}\\) policies and, for each policy, we have to evaluate an expectation, which can be expensive. Second, the above enumeration procedure does not work when \\(\\ALPHABET S\\) or \\(\\ALPHABET A\\) are continuous sets.\nThere is an alternative way of viewing the problem that simplifies it considerably. Instead of viewing the optimization problem before the system starts running (i.e., the ex ante view), imagine that the decision maker waits until they see the realization \\(s\\) of \\(S\\) (i.e., the interim view). they then asks what action \\(a\\) should they take to minimize the expected conditional cost \\(Q(s,a) := \\EXP[ c(s,a, W) | S = s]\\), i.e., they consider the problem\n\\[\\begin{equation} \\label{eq:cond-1} \\tag{P2}\n  \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a,W) | S = s], \\quad\n  \\forall s \\in \\ALPHABET S.\n\\end{equation}\\]\nThus, Problem \\eqref{eq:obs}, which is a functional optimization problem, has been reduced to a collection of parameter optimization problems (Problem \\eqref{eq:cond-1}), one for each possible of \\(s\\).\nNow define \\[ \\begin{equation} \\label{eq:cond} \\tag{P2-policy}\n  π^∘(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]\n\\end{equation} \\] where ties (in the minimization) are broken arbitrarily.\n\nTheorem 1.1 The decision rule \\(π^∘\\) defined in \\eqref{eq:cond} is optimal for Problem \\ref{eq:basic}.\n\n\n\n\n\n\n\nRemark\n\n\n\nWe restricted the proof to finite \\(\\ALPHABET S\\), \\(\\ALPHABET A\\), \\(\\ALPHABET W\\). This is to avoid any measurability issues. If \\(\\ALPHABET S\\) and \\(\\ALPHABET A\\) are continuous sets, we need to restrict to measurable \\(π\\) in Problem \\ref{eq:basic} (otherwise the expectation is not well defined; of course the cost \\(c\\) also has to be measurable). However, it is not immediately obvious that \\(π^∘\\) defined in \\eqref{eq:cond} is measurable. Conditions that ensure this are known as measurable selection theorems.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet \\(π\\) be any other decision rule. Then, \\[ \\begin{align*}\n  \\EXP[ c(S, π(S), W) ] &\\stackrel{(a)}= \\EXP[ \\EXP[c(S, π(S), W) | S ] ] \\\\\n  &\\stackrel{(b)}\\ge \\EXP[\\EXP[ c(S, π^∘(S), W) | S ] ] \\\\\n  &\\stackrel{(c)}= \\EXP[ c(S, π^∘(S), W) ],\n\\end{align*} \\] where \\((a)\\) and \\((c)\\) follow from the law of iterated expectations and \\((b)\\) follows from the definition of \\(π^∘\\) in \\eqref{eq:cond}.\n\n\n\nWe can also provide a partial converse of Theorem 1.1.\n\nTheorem 1.2 If \\(\\PR(S = s) > 0\\) for all \\(s\\), then any optimal policy \\(π^∘\\) for Problem \\(\\ref{eq:basic}\\) must satisfy \\(\\eqref{eq:cond}\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove this by contradiction. Suppose \\(π^*\\) is an optimal policy that does not satisfy \\eqref{eq:cond}. By definition of \\(π^∘\\), it must be the case that for all states \\[\\begin{equation}\n   \\EXP[ c(s, π^∘(s), W) | S = s ]\n   \\le\n   \\EXP[ c(s, π^*(s), W) | S = s ] .\n   \\label{eq:ineq:1}\n\\end{equation}\\] Now, since \\(π^*\\) does not satisfy \\eqref{eq:cond}, there exists some state \\(s^∘ \\in \\ALPHABET S\\) such that \\[\\begin{equation}\n   \\EXP[ c(s^∘, π^*(s^∘), W) | S = s^∘ ]\n   >\n   \\EXP[ c(s^∘, π^∘(s^∘), W) | S = s^∘ ] .\n   \\label{eq:ineq:2}\n\\end{equation}\\] Therefore, \\[\\begin{align*}\n   \\EXP[ c(S, π^*(S), W) ]\n   &=\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^*(s), W) | S = s ] ]\n   \\\\\n   & \\stackrel{(a)}>\n   \\sum_{s \\in \\ALPHABET S} \\PR(S = s)\n   \\EXP[ \\EXP[ c(s, π^∘(s), W) | S = s ] ]\n   \\\\\n   &=\n   \\EXP[ c(S, π^∘(S), W) ]\n\\end{align*}\\] where \\((a)\\) follows from \\eqref{eq:ineq:1} and \\eqref{eq:ineq:2} and the inequality is strict becase \\(\\PR(S = s^∘) > 0\\). Thus, \\(J(π^*) > J(π^∘)\\) and, hence, \\(π^*\\) cannot be an optimal policy."
  },
  {
    "objectID": "stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "href": "stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information",
    "title": "1  Introduction",
    "section": "1.3 Blackwell’s principle of irrelevant information",
    "text": "1.3 Blackwell’s principle of irrelevant information\nIn many scenarios, the decision maker may observe data which is irrelevant for evaluating performance. In such instances, the decision maker may ignore such information without affecting performance. Formally, we have the following result, which is known as Blackwell’s principle of irrelevant information.\n\nTheorem 1.3 (Blackwell’s principle of irrelevant information) Let \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), and \\(\\ALPHABET A\\) be standard Borel spaces and \\(S \\in \\ALPHABET S\\), \\(Y \\in \\ALPHABET Y\\), \\(W \\in \\ALPHABET W\\) be random variables defined on a common probability space.\nA decision maker observes \\((S,Y)\\) and chooses \\(A = π(S,Y)\\) to minimize \\(\\EXP[c(S,A,W)]\\), where \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is a measurable function.\nThen, if \\(W\\) is conditionally independent of \\(Y\\) given \\(S\\), then there is no loss of optimality in choosing \\(A\\) only as a function of \\(S\\).\nFormally, there exists a \\(π^* \\colon \\ALPHABET S \\to \\ALPHABET A\\) such that for all \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), \\[ \\EXP[c(S, π^*(S), W)] \\le \\EXP[ c(S, π(S,Y), W) ]. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result for the case when \\(\\ALPHABET S\\), \\(\\ALPHABET Y\\), \\(\\ALPHABET W\\), \\(\\ALPHABET A\\) are finite.\nDefine \\[π^*(s) = \\arg \\min_{a \\in \\ALPHABET A} \\EXP[ c(s,a, W) | S = s]. \\] Then, by construction, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), we have that \\[ \\EXP[ c(s, π^*(s), W ) | S = s]  \\le \\EXP[ c(s,a,W) | S = s]. \\] Hence, for any \\(π \\colon \\ALPHABET S \\times \\ALPHABET Y \\to \\ALPHABET A\\), and for any \\(s \\in \\ALPHABET S\\) and \\(y \\in \\ALPHABET Y\\), we have \\[ \\begin{equation} \\label{eq:opt}\n  \\EXP[ c(s, π^*(s), W) | S = s] \\le \\EXP[ c(s, π(s,y),W) | S = s].\n\\end{equation} \\] The result follows by taking the expectation of both sides of \\eqref{eq:opt}.\n\n\n\nThe above proof doesn’t work for general Borel spaces because \\(π^*\\) defined above may not exist (inf vs min) or may not be measurable. See Blackwell (1964) for a formal proof."
  },
  {
    "objectID": "stochastic-optimization/intro.html#exercises",
    "href": "stochastic-optimization/intro.html#exercises",
    "title": "1  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.1 (Computing optimal policies) Suppose \\(\\ALPHABET S = \\{1, 2 \\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET W\\) with joint distribution \\(P\\) shown below.\n\\[ P = \\MATRIX{ 0.25 & 0.15 & 0.05  \\\\ 0.30 & 0.10 & 0.15 } \\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S=2, W=1) = P_{21} = 0.30\\).\nThe cost function \\(c \\colon \\ALPHABET S \\times \\ALPHABET A \\times \\ALPHABET W \\to \\reals\\) is shown below\n\\[\nc(\\cdot,\\cdot,1) = \\MATRIX{3 & 5 & 1 \\\\ 2 & 3 & 1 }, \\quad\nc(\\cdot,\\cdot,2) = \\MATRIX{4 & 3 & 1 \\\\ 1 & 2 & 8 }, \\quad\nc(\\cdot,\\cdot,3) = \\MATRIX{1 & 2 & 2 \\\\ 4 & 1 & 3 }.\n\\]\nHere the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(a\\). For example \\(c(s=1,a=2,w=1) = 5\\).\nFind the policy \\(π \\colon \\ALPHABET S \\to \\ALPHABET A\\) that minimizes \\(\\EXP[ c(S, π(S), W) ]\\).\n\n\nExercise 1.2 (Blackwell’s principle) Suppose \\(\\ALPHABET S = \\{1, 2\\}\\), \\(\\ALPHABET Y = \\{1, 2\\}\\), \\(\\ALPHABET A = \\{1, 2, 3\\}\\), and \\(\\ALPHABET W = \\{1, 2, 3\\}\\). Let \\((S,Y,W)\\) be random variables taking values in \\(\\ALPHABET S × \\ALPHABET Y × \\ALPHABET W\\), with joint distribution \\(Q\\) shown below. \\[\nQ_{Y = 1} = \\MATRIX{0.15 & 0.10 & 0.00 \\\\ 0.15 & 0.05 & 0.10}\n\\qquad\nQ_{Y = 2} = \\MATRIX{0.10 & 0.05 & 0.05 \\\\ 0.15 & 0.05 & 0.05}\n\\] For a fixed value of \\(y\\), the row corresponds to the value of \\(s\\) and the column corresponds to the value of \\(w\\). For example \\(\\PR(S = 1, Y = 1, W = 3) = 0\\).\nThe cost function \\(c \\colon \\ALPHABET S × \\ALPHABET A × \\ALPHABET W \\to \\reals\\) is the same as the previous exercise.\n\nFind the policy \\(π \\colon \\ALPHABET S × \\ALPHABET Y \\to \\ALPHABET A\\) that minimizes \\(\\EXP[c(S, π(S,Y), W)]\\).\nCompare the solution with the solution of the previous exercise in view of Blackwell’s principle of irrelevant information. Clearly explain your observations.\n\n\n\n\nExercise 1.3 (Pollution monitoring) Consider the problem of monitoring the pollution level of a river. The river can have a high pollution level if there is a catastrophic failure of a factory upstream. There are then two “pollution states” indicating whether such a failure has not occured. We denote them by \\(S = 0\\) (indicating no failure) and \\(S = 1\\) (indicating catastrophic failure). Let \\([p, 1-p]\\) denote the prior probability mass function of \\(S\\).\nThe pollution monitoring system has a sensor which takes a measurement \\(y\\) of the pollution level. Let \\(f_s(y)\\) denote the probabiity density of the observation \\(y\\) conditional on the value of \\(s\\), \\(s \\in \\{0, 1\\}\\). Two actions are available at the monitoring system: raise an alarm or not raise an alarm. The cost of raising the alarm is \\(C_0\\) if the state \\(S\\) is \\(0\\) or zero if the state \\(S\\) is \\(1\\); the cost of not raising the alarm is zero if the state \\(S\\) is \\(0\\) or \\(C_1\\) if the state \\(S\\) is \\(1\\).\nShow that it is optimal to raise the alarm if \\[ p f_0(y) C_0 < (1 - p) f_1(y) C_1. \\] That is, it is optimal to raise the alarm if the likelihood ratio \\(f_1(y)/f_0(y)\\) exceeds the threshold value \\(p C_0/(1-p) C_1\\)."
  },
  {
    "objectID": "stochastic-optimization/intro.html#notes",
    "href": "stochastic-optimization/intro.html#notes",
    "title": "1  Introduction",
    "section": "Notes",
    "text": "Notes\nTheorem 1.3 is due to Blackwell (1964) in a short 2.5 page paper. A similar result was used by Witsenhausen (1979) to show the structure of optimal coding strategies in real-time communication. Also see the blog post by Maxim Ragisnsky.\nExercise 1.3 is adaptive from Whittle (1996). It is a special instance of Bayesian hypothesis testing problem. We will study a generalization of this model later in sequential hypothesis testing\n\n\n\n\n\nBlackwell, D. 1964. Memoryless strategies in finite-stage dynamic programming. The Annals of Mathematical Statistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nWhittle, P. 1996. Optimal control: Basics and beyond. Wiley.\n\n\nWitsenhausen, H.S. 1979. On the structure of real-time source coders. Bell System Technical Journal 58, 6, 1437–1451."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "href": "stochastic-optimization/newsvendor.html#interlude-with-continuous-version",
    "title": "2  The newsvendor problem",
    "section": "2.1 Interlude with continuous version",
    "text": "2.1 Interlude with continuous version\nThe problem above has discrete action and discrete demand. To build intuition, we first consider the case where both the actions and demand are continuous. Let \\(f(w)\\) denote the probability density of the demand and \\(F(w)\\) denote the cumulative probability density. Then, the expected reward is \\[ \\begin{equation} \\label{eq:J}\nJ(a) = \\int_{0}^a [ q w - p a ] f(w) dw + \\int_{a}^\\infty [ q a - p a ] f(w) dw.\n\\end{equation}\\]\nTo fix ideas, we consider an example where \\(p = 0.5\\), \\(q = 1\\), and the demand is a :Kumaraswamy distribution with parameters \\((a,b) = (2,5)\\) and support \\([0,100]\\). The performance of a function of action is shown below.\n\np = 0.5\nq = 1\nr = function(w,a){ if(w<=a) { return q*w - p*a } else { return q*a - p*a } }\n\na_opt = inverseCDF( (q-p)/q )\n\nconfig = ({\n  // Kumaraswamy Distribution: https://en.wikipedia.org/wiki/Kumaraswamy_distribution\n  a: 2,\n  b: 5,\n  max: 100\n})\n\npdf = {\n  const a = config.a\n  const b = config.b\n\n  return function(x) {\n    var normalized = x/config.max\n    return a*b*normalized**(a-1)*(1 - normalized**a)**(b-1)\n  }\n}\n\ninverseCDF= {\n  const a = config.a\n  const b = config.b\n  return function(y) {\n     // Closed form expression for inverse CDF of Kumaraswamy distribution \n     return config.max * (1 - (1-y)**(1/b))**(1/a)\n  }\n}\n\npoints = { \n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,action) }\n  }\n  return points\n}\n\ncost_values = {\n  const n = 1000\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\nJ = {\n  const a = config.a\n  const b = config.b\n\n  return function(action) {\n    const n = 1000\n    var cost = 0\n    var w = 0\n    for (var i = 0; i < n; i++) {\n      w = config.max*i/n\n      if (w <= action) {\n        cost += (q*w - p*action)*pdf(w)/n\n      } else {\n        cost += (q*action - p*action)*pdf(w)/n\n      }\n    }\n    return cost\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof action = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncost = Math.round(J(action)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJ = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [action, J(action)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_opt, J(a_opt)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_values, {x:\"x\", y:\"y\"})\n  ]\n})\n\nplotPDF = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [action,0], [action, pdf(action)] ], {stroke: \"blue\"}),\n    Plot.line(points,{x:\"x\", y:\"y\"}),\n    Plot.areaY(points.filter(pt => pt.x <= action),{x:\"x\", y:\"y\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt => pt.x > action),{x:\"x\", y:\"y\", fill: \"pink\"})\n  ]\n})\n\nplotReward = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(points, {x:\"x\", y:\"reward\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 2.1: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn Figure 2.1(a), the plot of \\(J(a)\\) is concave. We can verify that this is true in general.\n\n\n\n\n\n\nVerify that \\(J(a)\\) is concave\n\n\n\n\n\nTo verify that the function \\(J(a)\\) is concave, we compute the second derivative: \\[\n  \\frac{d^2 J(a)}{da^2} = - p f(a) - (q - p) f(a) = -q f(a) \\le 0.\n\\]\n\n\n\nThis suggests that we can use calculus to find the optimal value. In particular, to find the optimal action, we need to compute the \\(a\\) such that \\(dJ(a)/da = 0\\).\n\nProposition 2.1 For the newsvendor problem with continuous demand, the optimal action is \\[\n    a = F^{-1}\\left( 1 - \\frac{p}{q} \\right).\n  \\] In the literature, the quantity \\(1 - (p/q)\\) is called the critical fractile.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\n\n\nLeibniz integral rule\n\n\n\n\n\n\\[ \\dfrac{d}{dx} \\left( \\int_{p(x)}^{q(x)} f(x,t) dt \\right)\n   = f(x, q(x)) \\cdot \\dfrac {d}{dx} q(x)\n   - f(x, p(x)) \\cdot \\dfrac {d}{dx} p(x)\n   + \\int_{p(x)}^{q(x)} \\dfrac{\\partial}{\\partial x} f(x,t) dt.\n\\]\n\n\n\nUsing the Leibniz integral rule, the derivative of the first term of \\(\\eqref{eq:J}\\) is \\[ [q a - p a ] f(a) + \\int_{0}^a [ -p ] f(w) dw\n= [q a - p a ] f(a) - p F(a).\n\\]\nSimilarly, the derivative of the second term of \\(\\eqref{eq:J}\\) is \\[ - [q a - p a] f(a) + \\int_{a}^{\\infty} (q-p)f(w)dw\n= - [q a - p a] f(a) + (q -p)[ 1 - F(a)].\n\\]\nCombining the two, we get that \\[ \\dfrac{dJ(a)}{da} = - p F(a) + (q - p) [ 1 - F(a) ]. \\]\nEquating this to \\(0\\), we get \\[ F(a) = \\dfrac{ q - p }{ q}\n\\quad\\text{or}\\quad\na = F^{-1} \\left( 1 - \\dfrac{  p }{ q } \\right).\n\\]"
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "href": "stochastic-optimization/newsvendor.html#back-to-discrete-version",
    "title": "2  The newsvendor problem",
    "section": "2.2 Back to discrete version",
    "text": "2.2 Back to discrete version\nNow, we come back to the problem with discrete actions and discrete demand. Suppose \\(W\\) takes the values \\(\\ALPHABET W = \\{ w_1, w_2, \\dots, w_k \\}\\) (where \\(w_1 < w_2 < \\cdots < w_k\\)) with probabilities \\(\\{ μ_1, μ_2, \\dots, μ_k \\}\\). It is ease to see that in this case the action \\(a\\) should be in the set \\(\\{ w_1, w_2, \\dots, w_k \\}\\).\nTo fix ideas, we repeat the above numerical example when \\(\\ALPHABET W = \\{0, 1, \\dots, 100\\}\\).\n\npointsD = { \n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: pdf(x), reward: r(x,actionD) }\n  }\n  return points\n}\n\ncost_valuesD = {\n  const n = 100\n  var points = new Array(n)\n  for (var i = 0 ; i < n; i++) {\n    var x = config.max * i/n\n    points[i] = {x: x, y: J(x)}\n  }\n  return points\n}\n\n\na_optD = Math.round(a_opt*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof actionD = Inputs.range([0, 100], {value: 45, step: 0.01, label: \"a\"})\n\ncostD = Math.round(J(actionD)*100)/100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotJD = Plot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([ [actionD, J(actionD)] ], {fill: \"blue\", r:4}),\n    Plot.dot([ [a_optD, J(a_optD)] ], {fill: \"red\", r:4}),\n    Plot.line(cost_valuesD, {x:\"x\", y:\"y\", curve: \"step-after\"})\n  ]\n})\n\nplotPDFD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line([ [actionD,0], [actionD, pdf(actionD)] ], {stroke: \"blue\"}),\n    Plot.line(pointsD,{x:\"x\", y:\"y\", curve:\"step-after\"}),\n    Plot.areaY(points.filter(pt => pt.x <= actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"lightblue\"}),\n    Plot.areaY(points.filter(pt => pt.x > actionD),{x:\"x\", y:\"y\", curve: \"step-after\", fill: \"pink\"})\n  ]\n})\n\nplotRewardD = Plot.plot({\n  grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(pointsD, {x:\"x\", y:\"reward\", curve: \"step-after\"})\n  ]\n})\n\n\n\n\n\n\n\n\n(a) Performance: \n\n\n\n\n\n\n\n(b) PDF of demand\n\n\n\n\n\n\n\n(c) reward (as a function of demand)\n\n\n\nFigure 2.2: An example to illustrate the results. Plot (a) shows the performance as a function of action; the blue dot shows the value of chosen action and the red dot shows the value of optimal action. Plot (b) shows the PDF of the demand where the blue shared region shows the probability of getting a demand less than ordered goods and the red shaded region shows the probability of getting a demand greater than ordered goods. Plot (c) shows the reward function \\(r(a,\\cdot)\\), which depends on the values of \\(p\\) and \\(q\\).\n\n\n\nIn the discrete case, the brute force search is easier (because there are a finite rather than continuous number of values). We cannot directly use the ideas from calculus because functions over discrete domain are not differentiable. But we can use a very similar idea. Instead of checking if \\(dJ(a)/da = 0\\), we check the sign of \\(J(w_{i+1}) - J(w_i)\\).\n\nProposition 2.2 Let \\(\\{M_i\\}_{i \\ge 1}\\) denote the cumulative mass function of the demand. Then, the optimal action is the largest value of \\(w_i\\) such that \\[\n    M_i \\le 1 - \\frac{p}{q}.\n  \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe expected reward for choice \\(w_i\\) is \\[ \\begin{align*} J(w_i) &=\n\\sum_{j < i} μ_j [ q w_j - p w_i ] + \\sum_{j \\ge i} μ_j [q w_i - p w_i]\n\\\\\n&= -p w_i + q \\Bigl[ \\sum_{j < i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr].\n\\end{align*}\\]\nThus, \\[ \\begin{align*}\n  J(w_{i+1}) - J(w_i) &=\n  -p w_{i+1} + q \\Bigl[ \\sum_{j < i+1}  μ_j w_j + \\sum_{j \\ge i+1} μ_j w_{i+1} \\Bigr]\n  \\\\\n  &\\quad + p w_i - q \\Bigl[ \\sum_{j < i}  μ_j w_j + \\sum_{j \\ge i} μ_j w_i \\Bigr]\n  \\\\\n  &= -p (w_{i+1} - w_i) + q \\Bigl[ \\sum_{j \\ge i + 1} μ_j ( w_{i+1} - w_i) \\Bigr]\n  \\\\\n  &= \\big( - p + q [ 1 - M_i ] \\big) (w_{i+1} - w_i).\n\\end{align*}\\] Note that \\[\nM_i \\le \\dfrac{q-p}{q}\n\\iff\n-p + q [ 1 - M_i ] \\ge 0.\n\\] Thus, for all \\(i\\) such that \\(M_i \\le (q-p)/q\\), we have \\(J(w_{i+1}) \\ge J(w_i)\\). On the other hand, for all \\(i\\) such that \\(M_i > (q-p)/q)\\), we have \\(J(w_{i+1}) < J(w_i)\\). Thus, the optimal amount to order is the largest \\(w_i\\) such that \\(M_i \\le (q-p)/q\\).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nNote that the structure of the optimal solution is the same for continuous and discrete demand distributions."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#exercises",
    "href": "stochastic-optimization/newsvendor.html#exercises",
    "title": "2  The newsvendor problem",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 2.1 (Qualitative properties of optimal solution) Intuitively, we expect that if the purchase price of the newspaper increases but the selling price remains the same, then the newsvendor should buy less newspapers. Formally prove this statement.\nHint: The CDF of a distribution is a weakly increasing function.\n\n\nExercise 2.2 (Monotonicity of optimal action) Consider two scenarios for the case with continuous demand and actions. In scenario 1, the demand is distributed according to PDF \\(f_1\\). In scenario 2, it is distributed according to PDF \\(f_2\\). Suppose \\(F_1(w) \\le F_2(w)\\) for all \\(w\\). Show that the optimal action \\(a_1\\) for scenario 1 is greater than the optimal action \\(a_2\\) for scenario 2.\nHint: Plot the two CDFs and try to interpret the optimal decision rule graphically.\n\n\nExercise 2.3 (Selling random wind) The amount \\(W\\) of power generated by the wind turbine is a positive real-valued random variable with probability density function \\(f\\). The operator of the wind turbine has to commit to provide a certain amount of power in the day-ahead market. The price of power is \\(\\$p\\) per MW.\nIf the operator commits to provide \\(a\\) MW of power and the wind generation \\(W\\) is less than \\(a\\), then he has to buy the balance \\(a - W\\) from a reserves market at the cost of \\(\\$ q\\) per unit, where \\(q > p\\). Thus, the reward of the operator is \\(r(a,W)\\) where \\[ r(a, w) = \\begin{cases}\n  p a, & \\text{if } w > a \\\\\n  p a - q (a  - w), & \\text{if } w < a.\n\\end{cases}\\]\nFind the value of commitment \\(a\\) that maximizes the expected reward."
  },
  {
    "objectID": "stochastic-optimization/newsvendor.html#notes",
    "href": "stochastic-optimization/newsvendor.html#notes",
    "title": "2  The newsvendor problem",
    "section": "Notes",
    "text": "Notes\nPerhaps the earliest model of the newsvendor problem appeared in Edgeworth (1888) in the context of a bank setting the level of cash reserves to cover demands from its customers. The solution to the basic model presented above and some of its variants was provided in Morse and Kimball (1951); Arrow et al. (1952); Whitin (1953). See Porteus (2008) for an accessible introduction.\nThe property \\(F_1(w) \\le F_2(w)\\) used in Exercise 2 is called stochastic dominance. Later in the course, we will study how stochastic dominance is useful to establish monotonicity properties of general MDPs.\n\nThe example of selling random wind in Exercise 2.3 is taken from Bitar et al. (2012).\n\n\n\n\n\nArrow, K.J., Harris, T., and Marschak, J. 1952. Optimal inventory policy. Econometrica 20, 1, 250–272. DOI: 10.2307/1907830.\n\n\nBitar, E., Poolla, K., Khargonekar, P., Rajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind. 2012 45th hawaii international conference on system sciences, IEEE, 1931–1937.\n\n\nEdgeworth, F.Y. 1888. The mathematical theory of banking. Journal of the Royal Statistical Society 51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nMorse, P. and Kimball, G. 1951. Methods of operations research. Technology Press of MIT.\n\n\nPorteus, E.L. 2008. Building intuition: Insights from basic operations management models and principles. In: D. Chhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nWhitin, S. 1953. The theory of inventory management. Princeton University Press."
  },
  {
    "objectID": "mdps/gambling.html#computational-experiment",
    "href": "mdps/gambling.html#computational-experiment",
    "title": "4  Optimal gambling",
    "section": "4.1 Computational experiment",
    "text": "4.1 Computational experiment\nTo fix ideas, let’s try to find the optimal policy on our own. An example strategy is given below.\n\nviewof code = Inputs.textarea({label: \"\", height:800, rows:11, width: 800, submit: true,\n   value: `// function bet(t, states, outcomes) {\n// t: current time\n// states: Array of states\n// outcomes: Array of outcomes\n// \n// modify the (javascript) code between the lines:\n// ===============================\n     // As an illustration, we implement the policy to bet\n     //  half of the wealth as long as one is winning. \n     if(t == 0) { \n        return 0.5*states[t] \n     } else { \n        return outcomes[t-1] == 1 ? 0.5*states[t] : 0\n     }\n// ================================\n//}`\n                              })\nviewof strategy = Inputs.radio([\"user code\", \"optimal\"], {value: \"user code\", label: \"Select strategy\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT = 100\nn = 25\nS1 = 100\n\nBernoulli = function(p) { return Math.random() <= p ? 1 : -1 }\n\nuser_strategy = new Function('t', 'states', 'outcomes', code)\n\noptimal_strategy = function(t,states,outcomes) {\n  return p < 0.5 ? 0 : (2*p - 1)*states[t]\n}\n\nbet = function(t, states, outcomes) {\n  return strategy == \"optimal\" ? optimal_strategy(t, states, outcomes) : user_strategy(t, states, outcomes) \n}\n\ndata = { \n  run;\n  var states = new Array(T+1)\n  var outcomes = new Array(T+1)\n  var trajectory = new Array(T+1)\n  var sum = 0\n\n  const initial = 100\n  var idx = 0\n\n  for (var i = 0; i < n; i++) {\n      // Initialize the array to NaN values.\n      for (var t = 0; t < T+1; t++) {\n        states[t] = NaN\n        outcomes[t] = Bernoulli(p)\n      }\n    \n      states[0] = initial\n      var action = 0\n    \n      for (var t = 0; t < T; t++, idx++) {\n        action = bet(t, states, outcomes)\n        states[t+1] = states[t] + outcomes[t] * action\n        trajectory[idx] = { \n          time: t+1, \n          state: states[t],\n          action: action, \n          outcome: outcomes[t],\n          reward: Math.log10(states[t]),\n          sample: i,\n        }\n      }\n      sum += Math.log10(states[T])\n  }\n  return { trajectories: trajectory, mean: sum/n }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssuming that $S_1 = $ , we plot the performance of this policy below. Choosing “optimal” in the radio button above gives the performance of the optimal policy (derived below).\n\nviewof p = Inputs.range([0, 1], {value: 0.6, label: \"p\", step: 0.01})\n\nviewof run = Inputs.button(\"Re-run simulation\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrewardPlot = Plot.plot({\n  grid: true,\n  marginRight: 40,\n  marks: [\n    // Data\n    Plot.line(data.trajectories, {x: \"time\", y: \"reward\", z: \"sample\", stroke: \"gray\", curve: \"step-after\"}),\n    Plot.line(data.trajectories, Plot.groupX({y: \"mean\"}, {x:\"time\", y: \"reward\", stroke: \"red\", strokeWidth: 2, curve: \"step-after\"})),\n\n    // Final value\n    Plot.dot([ [T,data.mean] ], { fill: \"red\"}),\n    Plot.text([ [T,data.mean] ], { text: Math.round(data.mean*100)/100, dx:18, fill:\"red\", fontWeight:\"bold\" }),\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n  ]\n})\n\n\n\n\n\nFigure 4.1: Plot of the performance of the strategy for a horizon of \\(T=\\) . The curves in gray show the performance over $n = $  difference sample paths and the red curve shows its mean. For ease of visualization, we are plotting the utility at each stage (i.e., \\(\\log s_t\\)), even though the reward is only received at the terminal time step. The red line shows the mean performance over the \\(n\\) sample paths. The final mean value of the reward is shown in red. You can toggle the select strategy button to see how the optimal strategy performs (and how close you came to it).\n\n\n\nAs we can see, most intuitive policies do not do so well. We will now see how to compute the optimal policy using dynamic programming."
  },
  {
    "objectID": "mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "href": "mdps/gambling.html#optimal-gambling-strategy-and-value-functions",
    "title": "4  Optimal gambling",
    "section": "4.2 Optimal gambling strategy and value functions",
    "text": "4.2 Optimal gambling strategy and value functions\nThe above model of optimal gambling is a Markov decision process. Therefore, the optimal solution is given by dynamic programming.\n\nProposition 4.1 (Dynamic programming decomposition) Define the following value function \\(V_t \\colon \\reals_{\\ge 0} \\to \\reals\\) \\[ V_T(s) = \\log s \\] and for \\(t \\in \\{T-1, \\dots, 1\\}\\): \\[ \\begin{align*}\nQ_t(s,a) &= \\EXP[ r_t(s,a) + V_{t+1}(S_{t+1}) \\,|\\, S_t = s, A_t = a] \\\\\n&= p V_{t+1}(s+a) + (1-p) V_{t+1}(s-a),\n\\end{align*}\n\\] and \\[ \\begin{align*}\nV_t(s) &=  \\max_{a \\in [0, s]} Q_t(s,a), \\\\\nπ_t(s) &= \\arg \\max_{a \\in [0, s]} Q_t(s,a). \\\\\n\\end{align*}\n\\]\nThen the strategy \\(π = (π_1, \\dots, π_{T-1})\\) is optimal.\n\n\n\n\n\n\n\nRemark\n\n\n\nThe above model is one of the rare instances when the optimal strategy and the optimal strategy and value function of an MDP can be identified in closed form.\n\n\n\nTheorem 4.1 (Optimal gambling strategy) When \\(p \\le 0.5\\):\n\nthe optimal strategy is to not gamble, specifically \\(π_t(s) = 0\\);\nthe value function is \\(V_t(s) = \\log s\\).\n\nWhen \\(p > 0.5\\):\n\nthe optimal strategy is to bet a fraction of the current fortune, specifically \\(π_t(s) = (2p - 1)s\\);\nthe value function is \\(V_t(s) = \\log s + (T - t) C\\), where \\[ C = \\log 2 + p \\log p + (1-p) \\log (1-p).\\]\n\n\nThe constant \\(C\\) defined in Theorem 4.1 is equal to the capacity of a binary symmetric channel! In fact, the above model was introduced by Kelly (1956) to show a gambling interpretation of information rates.\nWe prove the two cases separately.\n\n\n\n\n\n\nProof when \\(p \\le 0.5\\)\n\n\n\n\n\nLet \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p \\le 0.5\\) implies that \\(p \\le 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V_{t+1}(s) = \\log s\\). Now consider\n\\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { - (q - p) s - a } {s^2 - a^2 }\n   \\\\\n   &< 0.\n  \\end{align*}   \n\\]\nThis implies that \\(Q_t(s,a)\\) is decreasing in \\(a\\). Therefore,\n\\[ π_t(s) = \\arg\\max_{a \\in [0, s]} Q_t(s,a) = 0. \\]\nMoreover, \\[ V_t(s) = Q_t(s, π_t(s)) = \\log s.\\]\nThis completes the induction step.\n\n\n\n\n\n\n\n\n\nProof when \\(p > 0.5\\)\n\n\n\n\n\nAs in the previous case, let \\(p = \\PR(W_t = 1)\\) and \\(q = \\PR(W_t = -1)\\). Then \\(p > 0.5\\) implies that \\(p > 1 - p = q\\).\nWe proceed by backward induction. For \\(t = T\\), we have that \\(V_T(s) = \\log s\\). This forms the basis of induction. Now assume that for \\(t+1\\), \\(V_{t+1}(s) = \\log s + (T -t - 1)C\\). Now consider\n\\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \\]\nDifferentiating both sides w.r.t. \\(a\\), we get \\[ \\begin{align*}\n  \\frac { \\partial Q_t(s,a) } {\\partial a} &=\n   \\frac p { s + a} - \\frac q { s - a }\n   \\\\\n   & = \\frac { (p - q) s - (p + q) a } { s^2 - a^2 }\n   \\\\\n   & =\n   \\frac { (p - q) s - a } {s^2 - a^2 }\n  \\end{align*}   \n\\]\nSetting \\(\\partial Q_t(s,a)/\\partial a = 0\\), we get that the optimal action is\n\\[ π_t(s) = (p-q) s. \\]\nNote that \\((p-q) \\in (0,1]\\)\n\\[\n  \\frac { \\partial^2 Q_t(s,a) } {\\partial a^2} =\n   - \\frac p { (s + a)^2 } - \\frac q { (s - a)^2 }\n  < 0;\n\\] hence the above action is indeed the maximizer. Moreover, \\[ \\begin{align*}\n  V_t(s) &= Q_t(s, π_t(s))  \\\\\n  &= p V_{t+1}(s + π_t(s)) + q V_{t+1}( s - π_t(s) )\\\\\n  &= \\log s + p \\log (1 + (p-q)) + q \\log (1 - (p-q)) + (T - t -1)C \\\\\n  &= \\log s + p \\log 2p + q \\log 2q + (T - t + 1)C \\\\\n  &= \\log s + (T - t) C\n  \\end{align*}   \n\\]\nThis completes the induction step."
  },
  {
    "objectID": "mdps/gambling.html#generalized-model",
    "href": "mdps/gambling.html#generalized-model",
    "title": "4  Optimal gambling",
    "section": "4.3 Generalized model",
    "text": "4.3 Generalized model\nSuppose that the terminal reward \\(r_T(s)\\) is monotone increasing2 in \\(s\\).2 I use the convention that increasing means weakly increasing. The alternative term non-decreasing implicitly assumes that we are talking about a totally ordered set.\n\nTheorem 4.2 For the generalized optimal gambling problem:\n\nFor each \\(t\\), the value function \\(V_t(s)\\) is monotone increasing in \\(s\\).\nFor each \\(s\\), the value function \\(V_t(s)\\) is monotone decreasing in \\(t\\).\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(s\\)\n\n\n\n\n\nWe proceed by backward induction. \\(V_T(s) = r_T(s)\\) which is monotone increasing in \\(s\\). Assume that \\(V_{t+1}(s)\\) is increasing in \\(s\\). Now, consider \\(V_t(s)\\). Consider \\(s_1, s_2 \\in \\reals_{\\ge 0}\\) such that \\(s_1 \\le s_2\\). Then for any \\(a \\le s_1\\), we have that\n\\[ \\begin{align*}\n    Q_t(s_1, a) &= p V_{t+1}(s_1+a) + q V_{t+1}(s_1-a) \\\\\n    & \\stackrel{(a)}{\\le} p V_{t+1}(s_2 + a) + q V_{t+1}(s_2  - a) \\\\\n    & = Q_t(s_2, a),\n  \\end{align*}\n\\] where \\((a)\\) uses the induction hypothesis. Now consider\n\\[ \\begin{align*}\n  V_t(s_1) &= \\max_{a \\in [0, s_1]} Q_t(s_1, a) \\\\\n  & \\stackrel{(b)}{\\le} \\max_{a \\in [0, s_1]} Q_t(s_2, a) \\\\\n  & \\le \\max_{a \\in [0, s_2]} Q_t(s_2, a) \\\\\n  &= V_t(s_2),\n  \\end{align*}\n\\] where \\((b)\\) uses monotonicity of \\(Q_t\\) in \\(s\\). This completes the induction step.\n\n\n\n\n\n\n\n\n\nProof of monotonicity in \\(t\\)\n\n\n\n\n\nThis is a simple consequence of the following:\n\\[V_t(s) = \\max_{a \\in [0, s]} Q_t(s,a) \\ge Q_t(s,0) = V_{t+1}(s).\\]"
  },
  {
    "objectID": "mdps/gambling.html#exercises",
    "href": "mdps/gambling.html#exercises",
    "title": "4  Optimal gambling",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNote\n\n\n\nThe purpose of these series of exercises is to generalize the basic result to a model where the gambler can bet on many mutually exclusive outcomes (think of betting on multiple horses in a horse race).\n\n\n\nExercise 4.1 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log w_i\\] subject to:\n\n\\(w_i \\ge 0\\)\n\\(\\sum_{i=1}^n w_i \\le s\\).\n\nShow that the optimal solution is given by \\[ w_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 4.2 Given positive numbers \\((p_1, \\dots, p_n)\\), consider the following constraint optimization problem: \\[\\max \\sum_{i=1}^n p_i \\log (s - a + na_i)\\] subject to:\n\n\\(a_i \\ge 0\\)\n\\(a = \\sum_{i=1}^n a_i \\le s\\).\n\nShow that the optimal solution is given by \\[ a_i = \\frac{p_i}{p} s\\] where \\(p = \\sum_{i=1}^n p_i\\).\n\n\nExercise 4.3 Consider an alternative of the optimal gambling problem where, at each time, the gambler can place bets on many mutually exclusive outcomes. Suppose there are \\(n\\) outcomes, with success probabilities \\((p_1, \\dots, p_n)\\). Let \\((A_{1,t}, \\dots, A_{n,t})\\) denote the amount that the gambler bets on each outcome. The total amount \\(A_t := \\sum_{i=1}^n A_{i,t}\\) must be less than the gambler’s fortune \\(S_t\\). If \\(W_t\\) denotes the winning outcome, then the gambler’s wealth evolves according to \\[ S_{t+1} = S_t - A_t + nU_{W_t, t}.\\] For example, if there are three outcomes, gambler’s current wealth is \\(s\\), the gambler bets \\((a_1, a_2, a_3)\\), and outcome 2 wins, then the gambler wins \\(3 a_2\\) and his fortune at the next time is \\[ s - (a_1 + a_2 + a_3) + 3 a_2. \\]\nThe gambler’s utility is \\(\\log S_T\\), the logarithm of his final wealth. Find the strategy that maximizes the gambler’s expected utility.\nHint: Argue that the value function is of the form \\(V_t(s) = \\log s + (T -t)C\\), where  \\[C = \\log n - H(p_1, \\dots, p_n)\\] where \\(H(p_1, \\dots, p_n) = - \\sum_{i=1}^n p_i \\log p_i\\) is the entropy of a random variable with pmf \\((p_1, \\dots, p_n)\\).The constant \\(C\\) is the capacity of a symmetric discrete memoryless with \\(n\\) outputs and for every input, the output probabilities are a permutation of \\((p_1, \\dots, p_n)\\)."
  },
  {
    "objectID": "mdps/gambling.html#notes",
    "href": "mdps/gambling.html#notes",
    "title": "4  Optimal gambling",
    "section": "Notes",
    "text": "Notes\nThe above model (including the model described in the exercise) was introduced by Kelly (1956). However, Kelly restricted attention to “bet a constant fraction of your fortune” betting strategy and found the optimal fraction. This strategy is sometimes referred to as :Kelly criteria. As far as I know, the dynamic programming treatment of the problem is due to Ross (1974). Ross also considered variations where the objective was to maximize the probability of reaching a preassigned fortune or maximizing the time until becoming broke.\nA generalization of the above model to general logarithmic and exponential utilities is presented in Ferguson and Gilstein (2004).\n\n\n\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004. Optimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nKelly, J.L., Jr. 1956. A new interpretation of information rate. Bell System Technical Journal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236."
  },
  {
    "objectID": "mdps/monotone-mdps.html#stochastic-dominance",
    "href": "mdps/monotone-mdps.html#stochastic-dominance",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.1 Stochastic dominance",
    "text": "6.1 Stochastic dominance\n Let \\(\\ALPHABET S\\) be a totally ordered finite set, say \\(\\{1, \\dots, n\\}\\).Stochastic dominance is a partial order on random variables defined on totally ordered sets\n\n\n\n\n\n\n(First order) stochastic dominance\n\n\n\nSuppose \\(S^1\\) and \\(S^2\\) are \\(\\ALPHABET S\\) valued random variables where \\(S^1 \\sim \\mu^1\\) and \\(S^2 \\sim \\mu^2\\). We say \\(S^1\\) stochastically dominates \\(S^2\\) if for any \\(s \\in \\ALPHABET S\\), \\[\\begin{equation}\\label{eq:inc-prob}\n  \\PR(S^1 \\ge s) \\ge \\PR(S^2 \\ge s).\n\\end{equation}\\]\nStochastic domination is denoted by \\(S^1 \\succeq_s S^2\\) or \\(\\mu^1 \\succeq_s \\mu^2\\).\n\n\nLet \\({\\rm M}^1\\) and \\({\\rm M}^2\\) denote the CDF of \\(\\mu^1\\) and \\(\\mu^2\\). Then \\eqref{eq:inc-prob} is equivalent to the following: \\[\\begin{equation}\\label{eq:cdf}\n  {\\rm M}^1_s \\le {\\rm M}^2_s, \\quad \\forall s \\in \\ALPHABET S.\n\\end{equation}\\] Thus, visually, \\(S^1 \\succeq_s S^2\\) means that the CDF of \\(S^1\\) lies below the CDF of \\(S^2\\).\n\n\n\n\n\n\nExample\n\n\n\n\\(\\left[0, \\frac 14, \\frac 14, \\frac 12\\right] \\succeq_s \\left[\\frac 14, 0, \\frac 14, \\frac 12 \\right] \\succeq_s \\left[\\frac 14, \\frac 14, \\frac 14, \\frac 14 \\right].\\)\n\n\nStochastic dominance is important due to the following property.\n\nTheorem 6.1 Let \\(f \\colon \\ALPHABET S \\to \\reals\\) be a (weakly) increasing function and \\(S^1 \\sim \\mu^1\\) and \\(S^2 \\sim \\mu^2\\) are random variables defined on \\(\\ALPHABET S\\). Then \\(S^1 \\succeq_s S^2\\) if and only if \\[\\begin{equation}\\label{eq:inc-fun}\n  \\EXP[f(S^1)] \\ge \\EXP[f(S^2)].\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof (stochastic dominance implies monotone expectations)\n\n\n\n\n\nFor the ease of notation, let \\(f_i\\) to denote \\(f(i)\\) and define \\({\\rm M}^1_0 = {\\rm M}^2_0 = 0\\). Consider the following: \\[\\begin{align*}\n    \\sum_{i=1}^n f_i \\mu^1_i\n    &= \\sum_{i=1}^n f_i ({\\rm M}^1_i - {\\rm M}^1_{i-1})\n    \\\\\n    &= \\sum_{i=1}^n {\\rm M}^1_{i-1} (f_{i-1} - f_{i}) + f_n {\\rm M}^1_n\n    \\\\\n    &\\stackrel{(a)}{\\ge}\n    \\sum_{i=1}^n {\\rm M}^2_{i-1} (f_{i-1} - f_{i}) + f_n {\\rm M}^2_n\n    \\\\\n    &= \\sum_{i=1}^n f_i ({\\rm M}^2_i - {\\rm M}^2_{i-1})\n    \\\\\n    &= \\sum_{i=1}^n f_i \\mu_i,\n\\end{align*}\\] which completes the proof. In the above equations, \\((a)\\) uses the following facts:\n\nFor any \\(i\\), \\({\\rm M}^1_{i-1} \\le {\\rm M}^2_{i-1}\\) (because of \\eqref{eq:cdf}) and \\(f_{i-1} - f_{i} < 0\\) (because \\(f\\) is increasing function). Thus, \\[{\\rm M}^1_{i-1}(f_{i-1} - f_i) \\ge {\\rm M}^2_{i-1}(f_{i-1} - f_i). \\]\n\\({\\rm M}^1_n = {\\rm M}^2_n = 1\\).\n\n\n\n\n\n\n\n\n\n\nProof (monotone expectations implies stochastic monotonicity)\n\n\n\n\n\nSuppose for any increasing function \\(f\\), \\eqref{eq:inc-fun} holds. Given any \\(i \\in \\{1, \\dots, n\\}\\), define the function \\(f_i(k) = \\IND\\{k > i\\}\\), which is an increasing function of \\(k\\). Then, \\[ \\EXP[f_i(S)] = \\sum_{k=1}^n f_i(k) \\mu^1_k = \\sum_{k > i} \\mu^1_k = 1 - {\\rm M}^1_i.\n\\] By a similar argument, we have \\[ \\EXP[f_i(S^2)] = 1 - {\\rm M}^2_i. \\] Since \\(\\EXP[f_i(S)] \\ge \\EXP[f_i(S^2)]\\), we have that \\({\\rm M}^1_i \\le {\\rm M}^2_i\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#stochastic-monotonicity",
    "href": "mdps/monotone-mdps.html#stochastic-monotonicity",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.2 Stochastic monotonicity",
    "text": "6.2 Stochastic monotonicity\nStochastic monotonicity extends the notion of stochastic dominance to Markov chains. Suppose \\(\\ALPHABET S\\) is a totally ordered set and \\(\\{S_t\\}_{t \\ge 1}\\) is a time-homogeneous Markov chain on \\(\\ALPHABET S\\) with transition probability matrix \\(P\\). Let \\(P_i\\) denote the \\(i\\)-th row of \\(P\\). Note that \\(P_i\\) is a PMF.\n\n\n\n\n\n\nStochastic monotonicity\n\n\n\nA Markov chain with transition matrix \\(P\\) is stochastically monotone if \\[ P_i \\succeq_s P_j, \\quad \\forall i > j. \\]\n\n\nAn immediate implication is the following.\n\nTheorem 6.2 Let \\(\\{S_t\\}_{t \\ge 1}\\) be a Markov chain with transition matrix \\(P\\) and \\(f \\colon \\ALPHABET S \\to \\reals\\) is a weakly increasing function. Then, for any \\(s^1, s^2 \\in \\ALPHABET S\\) such that \\(s^1 > s^2\\), \\[ \\EXP[f(S_{t+1}) | S_t = s^1] \\ge \\EXP[ f(S_{t+1}) | S_t = s^2], \\] if and only if \\(P\\) is stochatically monotone."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotonicity-of-value-functions",
    "href": "mdps/monotone-mdps.html#monotonicity-of-value-functions",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.3 Monotonicity of value functions",
    "text": "6.3 Monotonicity of value functions\n\nTheorem 6.3 Consider an MDP where the state space \\(\\ALPHABET S\\) is totally ordered. Suppose the following conditions are satisfied.\nC1. For every \\(a \\in \\ALPHABET A\\), the per-step cost \\(c_t(s,a)\\) is weakly inceasing in \\(s\\).\nC2. For every \\(a \\in \\ALPHABET A\\), the transition matrix \\(P(a)\\) is stochastically monotone.\nThen, the value function \\(V_t(s)\\) is weakly increasing in \\(s\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe result above also applies to models with continuous (and totally ordered) state space provided the measurable selection conditions hold so that the arg min at each step of the dynamic program is attained.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. By definition, \\(V_{T+1}(s) = 0\\), which is weakly increasing. This forms the basis of induction. Assume that \\(V_{t+1}(s)\\) is weakly increasing. Now consider, \\[Q_t(s,a) = c_t(s,a) + \\EXP[V_{t+1}(S_{t+1}) | S_t = s, A_t = a].\\] For any \\(a \\in \\ALPHABET A\\), \\(Q_t(s,a)\\) is a sum of two weakly increasing functions in \\(s\\); hence \\(Q_t(s,a)\\) is weakly increasing in \\(s\\).\nNow consider \\(s_1, s_2 \\in \\ALPHABET S\\) such that \\(s_1 > s_2\\). Suppose \\(a_1^*\\) is the optimal action at state \\(s_1\\). Then \\[\n  V_t(s^1) = Q_t(s^1, a_1^*) \\stackrel{(a)}\\ge Q_t(s^2,a_1^*) \\stackrel{(b)}\\ge V_t(s_2),\n\\] where \\((a)\\) follows because \\(Q_t(\\cdot, u^*)\\) is weakly increasing and \\((b)\\) follows from the definition of the value function."
  },
  {
    "objectID": "mdps/monotone-mdps.html#submodularity",
    "href": "mdps/monotone-mdps.html#submodularity",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.4 Submodularity",
    "text": "6.4 Submodularity\n\n\n\n\n\n\nSubmodularity\n\n\n\nLet \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be partially ordered sets. A function \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) is called submodular if for any \\(x^+ \\ge x^-\\) and \\(y^+ \\ge y^-\\), we have \\[\\begin{equation}\\label{eq:submodular}\n  f(x^+, y^+) + f(x^-, y^-) \\le f(x^+, y^-) + f(x^-, y^+).\n\\end{equation}\\]\nThe function is called supermodular if the inequality in \\eqref{eq:submodular} is reversed.\n\n\nA continuous and differentiable function on \\(\\reals^2\\) is submodular iff \\[ \\frac{ \\partial^2 f(x,y) }{ \\partial x \\partial y } \\le 0,\n  \\quad \\forall x,y.\n\\] If the inequality is reversed, then the function is supermodular.\nSubmodularity is a useful property because it implies monotonicity of the arg min.\n\nTheorem 6.4 Let \\(\\ALPHABET X\\) be a partially ordered set, \\(\\ALPHABET Y\\) be a totally ordered set, and \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) be a submodular function. Suppose that for all \\(x\\), \\(\\arg \\min_{y \\in \\ALPHABET Y} f(x,y)\\) exists. Then, \\[\n  π(x) := \\max \\{ y^* \\in \\arg \\min_{y \\in \\ALPHABET Y} f(x,y) \\}\n\\] is weakly increasing in \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(x^+, x^- \\in \\ALPHABET X\\) such that \\(x^+ \\ge x^-\\). Since \\(f\\) is submodular, for any \\(y \\le π(x^-)\\), we have \\[\\begin{equation}\\label{eq:1}\n  f(x^+, π(x^-)) - f(x^+, y) \\le f(x^-, π(x^-)) - f(x^-, y) \\le 0,\n\\end{equation}\\] where the last inequality follows because \\(π(x^-)\\) is the arg min of \\(f(x^-, y)\\). Eq. \\eqref{eq:1} implies that for all \\(y \\le π(x^-)\\), \\[\n  f(x^+, π(x^-)) \\le f(x^+, y).\n\\] Thus, \\(π(x^+) \\ge π(x^-)\\).\n\n\n\nThe analogue of Theorem 6.4 for supermodular functions is as follows.\n\nTheorem 6.5 Let \\(\\ALPHABET X\\) be a partially ordered set, \\(\\ALPHABET Y\\) be a totally ordered set, and \\(f \\colon \\ALPHABET X \\times \\ALPHABET Y \\to \\reals\\) be a supermodular function. Suppose that for all \\(x\\), \\(\\arg \\min_{y \\in \\ALPHABET Y} f(x,y)\\) exists. Then, \\[\n  π(x) := \\min \\{ y^* \\in \\arg \\min_{y \\in \\ALPHABET Y} f(x,y) \\}\n\\] is weakly decreasing in \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is similar to Theorem 6.4.\nConsider \\(x^+, x^- \\in \\ALPHABET X\\) such that \\(x^+ \\ge x^-\\). Since \\(f\\) is supermodular, for any \\(y \\ge π(x^-)\\), we have \\[\\begin{equation}\\label{eq:2}\n  f(x^+, y) - f(x^+, π(x^-)) \\ge f(x^-, y) - f(x^-, π(x^-)) \\ge 0,\n\\end{equation}\\] where the last inequality follows because \\(π(x^-)\\) is the arg min of \\(f(x^-, y)\\). Eq. \\eqref{eq:2} implies that for all \\(y \\ge π(x^-)\\), \\[\n  f(x^+, y) \\ge f(x^+, π(x^-)).\n\\] Thus, \\(π(x^+) \\le π(x^-)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotonicity-of-optimal-policy",
    "href": "mdps/monotone-mdps.html#monotonicity-of-optimal-policy",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.5 Monotonicity of optimal policy",
    "text": "6.5 Monotonicity of optimal policy\n\nTheorem 6.6 Consider an MDP where the state space \\(\\ALPHABET S\\) and the action space \\(\\ALPHABET A\\) are totally ordered. Suppose that, in addition to (C1) and (C2), the following condition is satisfied.\nC3. For any weakly increasing function \\(v\\), \\[ c_t(s,a) + \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\] is submodular in \\((s,a)\\).\nLet \\(π^*_t(s) = \\max\\{ a^* \\in \\arg \\min_{a \\in \\ALPHABET A} Q_t(s,a) \\}\\). Then, \\(π^*(s)\\) is weakly increasing in \\(s\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConditions (C1) and (C2) imply that the value function \\(V_{t+1}(s)\\) is weakly increasing. Therefore, condition (C3) implies that \\(Q_t(s,a)\\) is submodular in \\((s,a)\\). Therefore, the arg min is weakly increasing in \\(x\\)\n\n\n\nIt is difficult to verify condition (C3). The following conditions are sufficient for (C3).\n\nLemma 6.1 Consider an MDP with totally ordered state and action spaces. Suppose\n\n\\(c_t(s,a)\\) is submodular in \\((s,a)\\).\nFor all \\(s' \\in \\ALPHABET S\\), \\(H(s' | s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a)\\) is submodular in \\((s,a)\\).\n\nThe condition (C3) of the previous theorem holds.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s^+, s^- \\in \\ALPHABET S\\) and \\(a^+, a^- \\in \\ALPHABET A\\) such that \\(s^+ > s^-\\) and \\(a^+ > a^-\\). Define\n\\[\\begin{align*}\n  μ_1(s) &= \\tfrac 12 P_{s^- s}(a^-) + \\tfrac 12 P_{s^+ s}(a^+), \\\\\n  μ_2(s) &= \\tfrac 12 P_{s^- s}(a^+) + \\tfrac 12 P_{s^+ s}(a^-).\n\\end{align*}\\] Since \\(H(s' | s,a)\\) is submodular, we have \\[ H(s' | s^+, a^+) + H(s' | s^-, a^-) \\le H(s' | s^+, a^-) + H(s' | s^-, a^+) \\] or equivalently, \\[\\sum_{z \\le s'} \\big[ P_{s^+ z}(a^+) + P_{s^- z}(a^-) \\big]\n  \\ge\n  \\sum_{z \\le s'} \\big[ P_{s^+ z}(a^-) + P_{s^- z}(a^+) \\big]. \\] which implies \\[ M_1(s') \\ge M_2(s')\\] where \\(M_1\\) and \\(M_2\\) are the CDFs of \\(μ_1\\) and \\(μ_2\\). Thus, \\(μ_1 \\preceq_s μ_2\\).\nHence, for any weakly increasing function \\(v \\colon \\ALPHABET S \\to \\reals\\), \\[ \\sum_{s' \\in \\ALPHABET S} μ_1(s') v(s') \\le\n   \\sum_{s' \\in \\ALPHABET S} μ_2(s') v(s').\\] Or, equivalently, \\[H(s^+, a^+) + H(s^-, a^-) \\le H(s^-, a^+) + H(s^+, a^-)\\] where \\(H(s,a) = \\EXP[ v(X_{t+1}) | X_t = s, U_t = a]\\).\nTherefore, \\(c_t(s,a) + H_t(s,a)\\) is submodular in \\((s,a)\\).\n\n\n\nThe analogue of Theorem 6.6 for supermodular functions is as follows.\n\nTheorem 6.7 Consider an MDP where the state space \\(\\ALPHABET S\\) and the action space \\(\\ALPHABET A\\) are totally ordered. Suppose that, in addition to (C1) and (C2), the following condition is satisfied.\nC4. For any weakly increasing function \\(v\\), \\[ c_t(s,a) + \\EXP[ v(S_{t+1}) | S_t = s, A_t = a]\\] is supermodular in \\((s,a)\\).\nLet \\(π^*_t(s) = \\min\\{ a^* \\in \\arg \\min_{a \\in \\ALPHABET S} Q_t(s,a) \\}\\). Then, \\(π^*(s)\\) is weakly decreasing in \\(s\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConditions (C1) and (C2) imply that the value function \\(V_{t+1}(s)\\) is weakly increasing. Therefore, condition (C4) implies that \\(Q_t(s,a)\\) is supermodular in \\((s,a)\\). Therefore, the arg min is decreasing in \\(s\\)\n\n\n\nIt is difficult to verify condition (C4). The following conditions are sufficient for (C4).\n\nLemma 6.2 Consider an MDP with totally ordered state and action spaces. Suppose\n\n\\(c_t(s,a)\\) is supermodular in \\((s,a)\\).\nFor all \\(s' \\in \\ALPHABET S\\), \\(H(s' | s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a)\\) is supermodular in \\((s,a)\\).\n\nThe condition (C4) of the previous theorem holds.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s^+, s^- \\in \\ALPHABET S\\) and \\(a^+, a^- \\in \\ALPHABET A\\) such that \\(s^+ > s^-\\) and \\(a^+ > a^-\\). Define\n\\[\\begin{align*}\n  μ_1(s) &= \\tfrac 12 P_{s^- s}(a^-) + \\tfrac 12 P_{s^+ s}(a^+), \\\\\n  μ_2(s) &= \\tfrac 12 P_{s^- s}(a^+) + \\tfrac 12 P_{s^+ s}(a^-).\n\\end{align*}\\] Since \\(H(s' | s,a)\\) is supermodular, we have \\[ H(s' | s^+, a^+) + H(s' | s^-, a^-) \\ge H(s' | s^+, a^-) + H(s' | s^-, a^+) \\] or equivalently, \\[\\sum_{s' \\le s'} \\big[ P_{s^+ s'}(a^+) + P_{s^- s'}(a^-) \\big]\n  \\le\n  \\sum_{s' \\le s'} \\big[ P_{s^+ s'}(a^-) + P_{s^- s'}(a^+) \\big]. \\] which implies \\[ M_1(s') \\le M_2(s')\\] where \\(M_1\\) and \\(M_2\\) are the CDFs of \\(μ_1\\) and \\(μ_2\\). Thus, \\(μ_1 \\succeq_s μ_2\\).\nHence, for any weakly increasing function \\(v \\colon \\ALPHABET S \\to \\reals\\), \\[ \\sum_{s' \\in \\ALPHABET S} μ_1(s') v(s') \\ge\n   \\sum_{s' \\in \\ALPHABET S} μ_2(s') v(s').\\] Or, equivalently, \\[H(s^+, a^+) + H(s^-, a^-) \\ge H(s^-, a^+) + H(s^+, a^-)\\] where \\(H(s,a) = \\EXP[ v(X_{t+1}) | X_t = s, U_t = a]\\).\nTherefore, \\(c_t(s,a) + H_t(s,a)\\) is supermodular in \\((s,a)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#constraints-on-actions",
    "href": "mdps/monotone-mdps.html#constraints-on-actions",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.6 Constraints on actions",
    "text": "6.6 Constraints on actions\nIn the results above, we have assumed that the action set \\(\\ALPHABET A\\) is the same for all states. The results also extend to the case when the action at state \\(s\\) must belong to some set \\(\\ALPHABET A(s)\\) provided the following conditions are satisfied:\n\nFor any \\(s \\ge s'\\), \\(\\ALPHABET A(s) \\supseteq \\ALPHABET A(s')\\)\nFor any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A(s)\\), \\(a' < a\\) implies that \\(a' \\in \\ALPHABET A(s)\\)."
  },
  {
    "objectID": "mdps/monotone-mdps.html#monotone-dynamic-programming",
    "href": "mdps/monotone-mdps.html#monotone-dynamic-programming",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.7 Monotone dynamic programming",
    "text": "6.7 Monotone dynamic programming\nIf we can establish that the optimal policy is monontone, then we can use this structure to implement the dynamic program more efficient. Suppose \\(\\ALPHABET S = \\{1, \\dots, n\\}\\) and \\(\\ALPHABET A = \\{1, \\dots. m\\}\\). The main idea is as follows. Suppose \\(V_{t+1}(\\cdot)\\) has been caclulated. Insead of computing \\(Q_t(s,a)\\) and \\(V_t(s)\\), proceed as follows:\n\nSet \\(s = 1\\) and \\(α = 1\\).\nFor all \\(u \\in \\{α, \\dots, m\\}\\), compute \\(Q_t(s,a)\\) as usual.\nCompute\n\\[V_t(s) = \\min_{ α \\le a \\le m } Q_t(s,a)\\]\nand set\n\\[π_t^*(s) = \\max \\{ a \\in \\{α, \\dots, m\\} : V_t(s) = Q_t(s,a) \\}.\\]\nIf \\(s = n\\), then stop. Otherwise, set \\(α = π_t^*(s)\\) and \\(s = s+1\\) and go to step 2."
  },
  {
    "objectID": "mdps/monotone-mdps.html#example-a-machine-replacement-model",
    "href": "mdps/monotone-mdps.html#example-a-machine-replacement-model",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "6.8 Example: A machine replacement model",
    "text": "6.8 Example: A machine replacement model\nLet’s revisit the machine replacement problem presented at the beginning of this section. For simplicity, we’ll assume that \\(n = ∞\\), i.e., the state space is countable. In this case, the transition matrices are given by \\[ P_{sz}(0) = \\begin{cases}\n  0, & z < s \\\\\n  μ_{z - s}, & z \\ge s\n\\end{cases}\n\\quad\\text{and}\\quad\nP_sz(1) = μ_z.\n\\] where \\(μ\\) is the PMF of \\(W\\).\n\nProposition 6.1 For the machine replacement problem, there exist a series of thresholds \\(\\{s^*_t\\}_{t = 1}^T\\) such that the optimal policy at time \\(t\\) is a threshold policy with threshold \\(s_t\\), i.e., \\[\n  π_t(s) = \\begin{cases}\n  0 & \\text{if $s < s_t^*$} \\\\\n  1 & \\text{otherwise}\n\\end{cases}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by verifying conditions (C1)–(C4) to establish that the optimal policy is monotone.\nC1. For \\(a = 0\\), \\(c(s,0) = h(s)\\), which is weakly increasing by assumption. For \\(a = 1\\), \\(c(s,1) = K\\), which is trivially weakly increasing.\nC2. For \\(a = 0\\), \\(P(0)\\) is stochastically monotone (because the CDF of \\(P(\\cdot | s, 0)\\) lies above the CDF of \\(P(\\cdot | s+1, 0)\\)). For \\(a = 1\\), all rows of \\(P(1)\\) are the same; therefore \\(P(1)\\) is stochastically monotone.\nSince (C1) and (C2) are satisfied, by Theorem 6.3, we can assert that the value function is weakly increasing.\nC3. \\(c(s,1) - c(s,0) = K - h(s)\\), which is weakly decreasing in \\(s\\). Therefore, \\(c(s,a)\\) is submodular in \\((s,a)\\).\nC4. Recall that \\(H(s'|s,a) = 1 - \\sum_{z \\le s'} P_{sz}(a).\\) Therefore,\n\\[H(s'|s,0) = 1 - \\sum_{z = s}^{s'} μ_{z -s} = 1 - \\sum_{k = 0}^{s' - s} μ_k\n= 1 - M_{s' - s},\\] where \\(M\\) is the CMF of \\(μ\\), and \\[H(s'|s,1) = 1 - \\sum_{z \\le s'} μ_z = 1 - M_{s'},\\]\nTherefore, \\(H(s'|s,1) - H(s'|s,0) = M_{s'-s} - M_{s'}\\). For any fixed \\(s'\\), \\(H(s'|s,1) - H(s'|s,0)\\) is weakly decreasing in \\(s\\). There \\(H(s'|s,a)\\) is submodular in \\((s,a)\\).\nSince (C1)–(C4) are satisfied, the optimal policy is weakly increasing in~\\(s\\). Since there are only two actions, it means that for every time, there exists a state \\(s^*_t\\) with the property that if \\(s\\) exceeds \\(s^*_t\\), the optimal decision is to replace the machine; and if \\(s \\le s^*_t\\), then the optimal decision is to operate the machine for another period."
  },
  {
    "objectID": "mdps/monotone-mdps.html#exercises",
    "href": "mdps/monotone-mdps.html#exercises",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 6.1 Let \\(T\\) denote a upper triangular matrix with 1’s on or below the diagonal and 0’s above the diagonal. Then \\[ T^{-1}_{ij} = \\begin{cases}\n  1, & \\text{if } i = j, \\\\\n-1, & \\text{if } i = j + 1, \\\\\n  0, & \\text{otherwise}.\n\\end{cases}\\]\nFor example, for a \\(4 \\times 4\\) matrix \\[\n  T = \\MATRIX{1 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 1},\n  \\quad\n  T^{-1} = \\MATRIX{1 & -1 & 0 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 1 & -1 \\\\\n  0 & 0 & 0 & 1 }.\n\\]\nShow the following:\n\nFor any two PMFs \\(μ^1\\) and \\(μ^2\\), \\(\\mu^1 \\succeq_s \\mu^2\\) iff \\(\\mu^1 T \\ge \\mu^2 T\\).\nA Markov transition matrix \\(P\\) is stochastic monotone iff \\(T^{-1} P T \\ge 0\\).\n\n\n\nExercise 6.2 Show that the following are equivalent:\n\nA transition matrix \\(P\\) is stochastically monotone\nFor any two PMFs \\(μ^1\\) and \\(μ^2\\), if \\(\\mu^1 \\succeq_s \\mu^2\\) then \\(\\mu^1P \\succeq_s \\mu^2P\\).\n\n\n\nExercise 6.3 Show that if two transition matrices \\(P\\) and \\(Q\\) have the same dimensions and are stochastically monotone, then so are:\n\n\\(\\lambda P + (1 - \\lambda) Q\\), where \\(\\lambda \\in (0,1)\\).\n\\(P Q\\)\n\\(P^k\\), for \\(k \\in \\integers_{> 0}\\).\n\n\n\nExercise 6.4 Let \\(\\mu_t\\) denote the distribution of a Markov chain at time \\(t\\). Suppose \\(\\mu_0 \\succeq_s \\mu_1\\). Then \\(\\mu_t \\succeq_s \\mu_{t+1}\\).\n\n\n\nExercise 6.5 Consider the example of machine repair presented in notes on matrix formulation of MDPs. Prove that the optimal policy for that model is weakly increasing.\n\n\nExercise 6.6 Suppose the state space \\(\\ALPHABET S\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L-1, L\\}\\) and the action space \\(\\ALPHABET A\\) is discrete. Let \\(\\ALPHABET X_{\\ge 0}\\) denote the set \\(\\{0, \\dots, L\\}\\).\nLet \\(P(a)\\) denote the controlled transition matrix and \\(c_t(s,a)\\) denote the per-step cost. To avoid ambiguity, we define the optimal policy as \\[\nπ^*_t(s) = \\begin{cases}\n    \\max\\bigl\\{ a' \\in \\arg\\min_{a \\in \\ALPHABET A} Q_t(s,a) \\bigr\\},\n    & \\text{if } s \\ge 0 \\\\\n    \\min\\bigl\\{ a' \\in \\arg\\min_{a \\in \\ALPHABET A} Q_t(s,a) \\bigr\\},\n    & \\text{if } s < 0\n\\end{cases}\\] The purpose of this exercise is to identify conditions under which the value function and the optimal policy are even and :quasi-convex. We do so using the following steps.\n\nWe say that the transition probability matrix \\(P(a)\\) is even if for all \\(s, s' \\in \\ALPHABET S\\), \\(P(s'|s,a) = P(-s'|-s,a)\\). Prove the following result.\n\n\nProposition 6.2 Suppose the MDP satisfies the following properties:\n(A1) For every \\(t\\) and \\(a \\in \\ALPHABET A\\), \\(c_t(s,a)\\) is even function of \\(s\\).\n(A2) For every \\(a \\in \\ALPHABET A\\), \\(P(a)\\) is even.\nThen, for all \\(t\\), \\(V_t\\) and \\(π_t\\) are even functions.\n\n\nGiven any probability mass function \\(μ\\) on \\(\\ALPHABET S\\), define the folded probability mass function \\(\\tilde μ\\) on \\(\\ALPHABET X_{\\ge 0}\\) as follows: \\[ \\tilde μ(s) = \\begin{cases}\n   μ(0), & \\text{if } s = 0 \\\\\n   μ(s) + μ(-s), & \\text{if } s > 0.\n\\end{cases} \\]\n\nFor ease of notation, we use \\(\\tilde μ = \\mathcal F μ\\) to denote this folding operation. Note that an immediate consequence of the definition is the following (you don’t have to prove this).\n\nLemma 6.3 If \\(f \\colon \\ALPHABET S \\to \\reals\\) is even, then for any probability mass function \\(μ\\) on \\(\\ALPHABET S\\) and \\(\\tilde μ = \\mathcal F μ\\), we have \\[\n  \\sum_{s \\in \\ALPHABET S} f(s) μ(s) =\n  \\sum_{s \\in \\ALPHABET X_{\\ge 0}} f(s) \\tilde μ(s). \\]\n\nThus, the expectation of the function \\(f \\colon \\ALPHABET S \\to \\reals\\) with respect to the PMF \\(μ\\) is equal to the expectation of the function \\(f \\colon \\ALPHABET X_{\\ge 0} \\to \\reals\\) with respect to the PMF \\(\\tilde μ = \\mathcal F μ\\).\nNow given any probability transition matrix \\(P\\) on \\(\\ALPHABET S\\), we can define a probability transition matrix \\(\\tilde P\\) on \\(\\ALPHABET X_{\\ge 0}\\) as follows: for any \\(s \\in \\ALPHABET S\\), \\(\\tilde P_s = \\mathcal F P_s\\), where \\(P_s\\) denotes the \\(s\\)-th row of \\(P\\). For ease of notation, we use \\(\\tilde P = \\mathcal F P\\) to denote this relationship.\nNow prove the following:\n\nProposition 6.3 Given the MDP \\((\\ALPHABET S, \\ALPHABET A, P, \\{c_t\\})\\), define the folded MDP as \\((\\ALPHABET S_{\\ge 0}, \\ALPHABET A, \\tilde P, \\{c_t\\})\\), where \\(\\tilde P(a) = \\mathcal F P(a)\\) for all \\(a \\in \\ALPHABET A\\). Let \\(\\tilde Q_t \\colon \\ALPHABET S_{\\ge 0} \\times \\ALPHABET A \\to \\reals\\), \\(\\tilde V_t \\colon \\ALPHABET S_{\\ge 0} \\to \\reals\\) and \\(\\tilde π_t^* \\colon \\ALPHABET S_{\\ge 0} \\to \\ALPHABET A\\) denote the action-value function, value function and the policy of the folded MDP. Then, if the original MDP satisfies conditions (A1) and (A2) then, for any \\(s \\in \\ALPHABET S\\) and \\(a \\in \\ALPHABET A\\), \\[ Q_t(s,a) = \\tilde Q_t(|s|, a),\n\\quad\n  V_t(s) = \\tilde V_t(|s|),\n\\quad\n  π_t^*(s) = \\tilde π_t^*(|s|).\n\\]\n\n\nThe result of the previous part implies that if the value function \\(\\tilde V_t\\) and the policy \\(\\tilde π^*_t\\) are monotone increasing, then the value function \\(V_t\\) and the policy \\(π^*_t\\) are even and quasi-convex. This gives us a method to verify if the value function and optimal policy are even and quasi-convex.\nNow, recall the model of the Internet of Things presented in Q2 of Assignment 3. The numerical experiments that you did in Assignment 3 suggest that the value function and the optimal policy are even and quasi-convex. Prove that this is indeed the case.\nNow suppose the distribution of \\(W_t\\) is not Gaussian but is some general probability density \\(\\varphi(\\cdot)\\) and the cost function is \\[ c(e,a) = \\lambda a + (1 - a) d(e). \\] Find conditions on \\(\\varphi\\) and \\(d\\) such that the value function and optimal policy are even and quasi-convex."
  },
  {
    "objectID": "mdps/monotone-mdps.html#notes",
    "href": "mdps/monotone-mdps.html#notes",
    "title": "6  Monotonicity of value function and optimal policies",
    "section": "Notes",
    "text": "Notes\nStochastic dominance has been employed in various areas of economics, finance, and statistics since the 1930s. See Levy (1992) and Levy (2015) for detailed overviews. The notion of stochastic monotonicity for Markov chains is due to Daley (1968). For a generalization of stochastic monotonicity to continuous state spaces, see Serfozo (1976). The characterization of stochastic monotonicity in Exercise 6.1–Exercise 6.4 are due to Keilson and Kester (1977).\nRoss (1974) has an early treatment of monotonicity of optimal policies. The general theory was developed by Topkis (1998). The presentation here follows Puterman (2014). Exercise 6.6 is from Chakravorty and Mahajan (2018).\n\n\n\n\nChakravorty, J. and Mahajan, A. 2018. Sufficient conditions for the value function and optimal strategy to be even and quasi-convex. IEEE Transactions on Automatic Control 63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nDaley, D.J. 1968. Stochastically monotone markov chains. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete 10, 4, 305–317. DOI: 10.1007/BF00531852.\n\n\nKeilson, J. and Kester, A. 1977. Monotone matrices and monotone markov processes. Stochastic Processes and their Applications 5, 3, 231–241.\n\n\nLevy, H. 1992. Stochastic dominance and expected utility: Survey and analysis. Management Science 38, 4, 555–593. DOI: 10.1287/mnsc.38.4.555.\n\n\nLevy, H. 2015. Stochastic dominance: Investment decision making under uncertainty. Springer. DOI: 10.1007/978-3-319-21708-6.\n\n\nPuterman, M.L. 2014. Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons. DOI: 10.1002/9780470316887.\n\n\nRoss, S.M. 1974. Dynamic programming and gambling models. Advances in Applied Probability 6, 3, 593–606. DOI: 10.2307/1426236.\n\n\nSerfozo, R.F. 1976. Monotone optimal policies for markov decision processes. In: Mathematical programming studies. Springer Berlin Heidelberg, 202–215. DOI: 10.1007/bfb0120752.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#dynamic-program",
    "href": "mdps/power-delay-tradeoff.html#dynamic-program",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "7.1 Dynamic program",
    "text": "7.1 Dynamic program\nWe can assume \\(Y_t = X_t - A_t\\) as a post-decision state in the above model and write the dynamic program as follows:\n\\[ V_{T+1}(x,s) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\\begin{align*}\n  H_t(y,s) &= \\lambda d(y) + \\EXP[ V_{t+1}(y + W_t, S_{t+1}) | S_t = s ], \\\\\n  V_t(x,s) &= \\min_{0 \\le a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\big\\}\n\\end{align*}\\]\n\n7.1.1 Monotonicity of value functions\n\nLemma 7.1 For all \\(t\\), \\(V_t(x,s)\\) and \\(H_t(y,s)\\) are increasing in both variables.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that the constraint set \\(\\ALPHABET A(x) = \\{0, \\dots, x\\}\\) satisfies the conditions that generalize the result of monotonicity to constrained actions.\nWe prove the two monotonicity properties by backward induction. First note that \\(V_{T+1}(x,s)\\) is trivially monotone. This forms the basis of induction. Now suppose \\(V_{t+1}(x,s)\\) is increasing in \\(x\\) and \\(s\\). Since \\(\\{S_t\\}_{t \\ge 1}\\) is stochastically monotone, \\[H_t(y,s) = \\lambda d(y) + \\EXP[ V_{t+1}(y + W_t, S_{t+1}) | S_t = s ]\\] is increasing in \\(s\\). Moreover, since both \\(d(y)\\) and \\(V_{t+1}(y + w, s)\\) are increasing in \\(y\\), so is \\(H_t(y,s)\\).\nNow, for every \\(a\\), \\(p(a) q(s)\\) and \\(H_t(x-a, s)\\) is increasing in \\(x\\) and \\(s\\). So, the pointwise minima over \\(a\\) is also increasing in \\(x\\) and \\(s\\).\n\n\n\n\n\n7.1.2 Convexity of value functions\n\nLemma 7.2 For all time \\(t\\) and channel state \\(s\\), \\(V_t(x,s)\\) and \\(H_t(y,s)\\) are convex in the first variable.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe proceed by backward induction. First note that \\(V_{T+1}(x,s)\\) is trivially convex in \\(x\\). Now assume that \\(V_{t+1}(x,s)\\) is convex in \\(x\\). Then, \\(\\EXP[V_{t+1}(y + W_t, S_{t+1}) | S_t = s]\\) is weighted sum of convex functions and is, therefore, convex in \\(y\\). Therefore, \\(H_t(y,s)\\) is a sum of two convex functions and, therefore, convex in \\(y\\).\nWe cannot directly show the convexity of \\(V_t(x,s)\\) because the pointwise minimum of convex functions is not convex. So, we consider the following argument. Fix \\(s\\) and pick \\(x > 1\\). Let \\(\\underline a = π^*_t(x-1,s)\\) and \\(\\bar a = π^*_t(x+1,s)\\). Let \\(\\underline v = \\lfloor (\\underline a + \\bar a)/2 \\rfloor\\) and \\(\\bar v = \\lceil (\\underline a + \\bar a)/2 \\rceil\\). Note that both \\(\\underline v\\) and \\(\\bar v\\) are feasible at \\(x\\). Then, \\[ \\begin{align*}\n  \\hskip 2em & \\hskip -2em\n  V_t(x-1, s) + V_t(x+1, s)\n  \\\\\n  &=\n  [ p(\\underline a) + p(\\bar a) ] q(s) + H_t(x - 1 - \\underline a, s)\n  + H_t(x + 1 - \\bar a, s)\n  \\\\\n  &\\stackrel{(a)}\\ge [ p(\\underline v) + p(\\bar v)] q(s) +\n    H_t(x - \\underline v, s) + H_t(x - \\bar v, s) \\\\\n  &\\ge 2 \\min_{a \\le x} \\big\\{ p(a) q(s) + H_t(x-a, s) \\\\\n  &= 2 V_t(x,s),\n\\end{align*} \\] where \\((a)\\) follows from convexity of \\(p(\\cdot)\\) and \\(H_t(\\cdot, s)\\). Thus, \\(V_t(x,s)\\) is convex in \\(x\\). This completes the induction step.\n\n\n\n\n\n7.1.3 Monotonicity of optimal policy in queue length\n\nTheorem 7.1 For all time \\(t\\) and channel state s\\(s\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is increasing in the queue length \\(x\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn the previous lemma, we have shown that \\(H_t(y,s)\\) is convex in \\(y\\). Therefore, \\(H_t(x-a, s)\\) is submodular in \\((x,a)\\).\n\nThus, for a fixed \\(s\\), \\(p(a)q(s) + H_t(x-a, s)\\) is submodular in \\((x,a)\\). Therefore, the optimal policy is increasing in \\(x\\).\n\n\n\nOne can show submodularity by finite difference, but for simplicity, we assume that \\(H_t(y,s)\\) is twice differentiable. Then, \\(\\partial^2 H_t(x - a, s)/ \\partial x \\partial a \\le 0\\) (by convexity of \\(H_t\\)).\n\n7.1.4 Monotonicity of optimal policy in channel state\nIt is natural to expect that for a fixed \\(x\\) the optimal policy is decreasing in \\(s\\). However, it is not possible to obtain the monotonicity of optimal policy in channel state in general. To see why this is difficult, let us impose a mild assumption on the arrival distribution.\n\nThe packet arrival distribution is weakly decreasing, i.e., for any \\(v,w \\in \\integers_{\\ge 0}\\) such that \\(v \\le w\\), we have that \\(P_W(v) \\ge P_W(w)\\).\n\nWe first start with a slight generalization of stochastic monotonicity result.\n\nLemma 7.3 Let \\(\\{p_i\\}_{i \\ge 0}\\) and \\(\\{q_i\\}_{i \\ge 0}\\) be real-valued non-negative sequences satisfying \\[ \\sum_{i \\le j} p_i \\le \\sum_{i \\le j} q_i, \\quad \\forall j.\\] Then, for any increasing sequence \\(\\{v_i\\}_{i \\ge 0}\\), we have \\[ \\sum_{i = 0}^\\infty p_i v_i \\ge \\sum_{i=0}^\\infty q_i v_i. \\]\n\nThe proof is similar to the proof for stochastic monotonicity.\n\nLemma 7.4 Under (asm-power-delay-density?), for all \\(t\\), \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nThe idea of the proof is similar to Lemma 1 of the notes on monotone MDPs.\nFix \\(y^+, y^- \\in \\integers_{\\ge 0}\\) and \\(s^+, s^- \\in \\ALPHABET S\\) such that \\(y^+ > y^-\\) and \\(s^+ > s^-\\). Now, for any \\(y' \\in \\integers_{\\ge 0}\\) and \\(s' \\in \\ALPHABET S\\) define \\[\\begin{align*}\n  π(y',s') = P_W(y' - y^+)P_S(s'|s^+) +\n             P_W(y' - y^-)P_S(s'|s^-),\n             \\\\\n  μ(y',s') = P_W(y' - y^-)P_S(s'|s^+) +\n             P_W(y' - y^+)P_S(s'|s^-).\n\\end{align*}\\]\nSince \\(P_S\\) is stochastically monotone, we have that for any \\(σ \\in \\ALPHABET S\\), \\[ \\sum_{s'=1}^{σ} P_S(s'|s^+) \\le \\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Moreover, due to (asm-power-delay-density?), we have that \\(P_W(y' - y^-) \\le P_W(y' - y^+)\\). Thus, \\[ [P_W(y' - y^+) - P_W(y' - y^-)] \\sum_{s'=1}^{σ} P_S(s'|s^+)\n\\le [P_W(y' - y^+) - P_W(y' - y^-)]\\sum_{s'=1}^{σ} P_S(s'|s^-). \\] Rearranging terms, we get \\[ \\sum_{s'=1}^σ π(y',s') \\le \\sum_{s'=1}^σ μ(y',s'). \\] Thus, for any \\(y'\\), the sequence \\(π(y',s')\\) and \\(ν(y',s')\\) satisfy the condition of Lemma 7.3.\nNow, in Lemma 7.1, we have established that for any \\(y'\\), \\(V_{t+1}(y',s')\\) is increasing in \\(s'\\). Thus, from Lemma 7.3, we have \\[  \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Summing up over \\(y'\\), we get \\[  \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} π(y', s') V_{t+1}(y', s')\n\\ge \\sum_{y' \\in \\integers_{\\ge 0}} \\sum_{s' \\in \\ALPHABET S} μ(y', s') V_{t+1}(y', s'), \\] Or equivalently, \\[\\begin{align*}\n\\hskip 2em & \\hskip -2em\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^-) ]\n\\\\\n& \\ge\n\\EXP[ V_{t+1}(y^- + W, S_{t+1}) | S_t = s^+) ]\n+\n\\EXP[ V_{t+1}(y^+ + W, S_{t+1}) | S_t = s^-) ] .\n\\end{align*}\\] Thus, \\(H_t(y,s)\\) is supermodular in \\((y,s)\\).\n\n\n\n\n\n\n\n\n\nEven under (asm-power-delay-density?), we cannot establish the monotonicity of \\(π^*_t(x,s)\\) is \\(s\\).\n\n\n\nNote that we have established that \\(H_t(y,s)\\) is supermodular in \\((y,s)\\). Thus, for any fixed \\(x\\), \\(H_t(x-a,s)\\) is submodular in \\((a,s)\\). Furthermore the function \\(p(a)q(s)\\) is increasing in both variables and therefore supermodular in \\((a,s)\\). Therefore, we cannot say anything specific about \\(p(a)q(s) + H_t(x-a, s)\\) which is a sum of submodular and supermodular functions."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#exercises",
    "href": "mdps/power-delay-tradeoff.html#exercises",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 7.1 In this exercise, we provide sufficient conditions for the optimal policy to be monotone in the channel state. Suppose that the channel state \\(\\{S_t\\}_{t \\ge 1}\\) is an i.i.d. process. Then prove that for all time \\(t\\) and queue state \\(x\\), there is an optimal strategy \\(π^*_t(x,s)\\) which is decreasing in channel state \\(s\\)."
  },
  {
    "objectID": "mdps/power-delay-tradeoff.html#notes",
    "href": "mdps/power-delay-tradeoff.html#notes",
    "title": "7  Power-delay tradeoff in wireless communication",
    "section": "Notes",
    "text": "Notes\nThe mathematical model of power-delay trade-off is taken from Berry (2000), where the monotonicty results were proved using first principles. More detailed characterization of the optimal transmission strategy when the average power or the average delay goes to zero are provided in Berry and Gallager (2002) and Berry (2013). A related model is presented in Ding et al. (2016).\nFor a broader overview of power-delay trade offs in wireless communication, see Berry et al. (2012) and Yeh (2012).\nThe remark after Lemma 7.4 shows the difficulty in establishing monotonicity of optimal policies for a multi-dimensional state space. In fact, sometimes even when monotonicity appears to be intuitively obvious, it may not hold. See Sayedana and Mahajan (2020) for an example. For general discussions on monotonicity for multi-dimensional state spaces, see Topkis (1998) and Koole (2006). As an example of using such general conditions to establish monotonicity, see Sayedana et al. (2020).\n\n\n\n\nBerry, R.A. 2000. Power and delay trade-offs in fading channels. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay tradeoffs in fading channels—small-delay asymptotics. IEEE Transactions on Information Theory 59, 6, 3939–3952. DOI: 10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002. Communication over fading channels with delay constraints. IEEE Transactions on Information Theory 48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M. 2012. Energy-efficient scheduling under delay constraints for wireless networks. Synthesis Lectures on Communication Networks 5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A. 2016. On monotonicity of the optimal transmission policy in cross-layer adaptive \\(m\\) -QAM modulation. IEEE Transactions on Communications 64, 9, 3771–3785. DOI: 10.1109/TCOMM.2016.2590427.\n\n\nKoole, G. 2006. Monotonicity in markov reward and decision chains: Theory and applications. Foundations and Trends in Stochastic Systems 1, 1, 1–76. DOI: 10.1561/0900000002.\n\n\nSayedana, B. and Mahajan, A. 2020. Counterexamples on the monotonicity of delay optimal strategies for energy harvesting transmitters. IEEE Wireless Communications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E. 2020. Cross-layer communication over fading channels with adaptive decision feedback. International symposium on modeling and optimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nTopkis, D.M. 1998. Supermodularity and complementarity. Princeton University Press.\n\n\nYeh, E.M. 2012. Fundamental performance limits in cross-layer wireless optimization: Throughput, delay, and energy. Foundations and Trends in Communications and Information Theory 9, 1, 1–112. DOI: 10.1561/0100000014."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#preliminaries",
    "href": "mdps/lipschitz-mdps.html#preliminaries",
    "title": "12  Lipschitz MDPs",
    "section": "12.1 Preliminaries",
    "text": "12.1 Preliminaries\n\nLipschitz continuous functions\nGiven two metric spaces \\((\\ALPHABET X, d_X)\\) and \\((\\ALPHABET Y, d_Y)\\), the Lipschitz constant of function \\(f \\colon \\ALPHABET X \\to \\ALPHABET Y\\) is defined by \\[ \\| f\\|_{L} = \\sup_{x_1 \\neq x_2}\n    \\left\\{ \\frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } :\n    x_1, x_2 \\in \\ALPHABET X \\right\\} \\in [0, ∞]. \\] The function is called Lipschitz continuous if its Lipschitz constant is finite.\nIntuitively, a Lipschitz continuous function is limited by how fast it can change. For example, the following image from Wikipedia shows that for a Lipschitz continuous function, there exists a double cone (white) whose origin can be moved along the graph so that the whole graph always stays outside the double cone.\n\n\n\n\n\nImage credit: https://en.wikipedia.org/wiki/File:Lipschitz_Visualisierung.gif\n\n\nLet \\(\\ALPHABET Z\\) be an arbitrary set. A function \\(f \\colon \\ALPHABET X × \\ALPHABET Z \\to \\ALPHABET Y\\) is said to be uniformly Lipschitz in \\(u\\) if \\[ \\sup_{z \\in \\ALPHABET Z} \\| f(\\cdot, z) \\|_L  =\n  \\sup_{z \\in \\ALPHABET Z} \\sup_{x_1 \\neq x_2}\n  \\dfrac{ d_Y(f(x_1,z), f(x_2, z)) }{ d_X(x_1, x_2) } < ∞. \\]\n\n\nSome examples\nA function \\(f \\colon \\reals \\to \\reals\\) is Lipschitz continuous if and only if it has bounded first derivative. The Lipschitz constant of such a function is equal to the maximum absolute value of the derivative.\nHere are some examples of Lipschitz continuous functions:\n\nThe function \\(f(x) = \\sqrt{x^2 + 1}\\) defined over \\(\\reals\\) is Lipschitz continuous because it is everywhere differentiable and the maximum value of the derivative is \\(L = 1\\).\nThe function \\(f(x) = |x|\\) defined over \\(\\reals\\) is Lipschitz continuous with Lipschitz constant equal to \\(1\\). Note that this function is continuous but not differentiable.\nThe function \\(f(x) = x + \\sin x\\) defined over \\(\\reals\\) is Lipschitz continuous with a Lipschitz constant equal to \\(1\\).\nThe function \\(f(x) = \\sqrt{x}\\) defined over \\([0,1]\\) is not Lipschitz continuous because the function becomes infinitely steep as \\(x\\) approaches \\(0\\).\nThe function \\(f(x) = x^2\\) defined over \\(\\reals\\) is not Lipschitz continuous because it becomes arbitrarily steep as \\(x\\) approaches infinity.\nThe function \\(f(x) = \\sin(1/x)\\) is bounded but not Lipschitz because becomes infinitely steep as \\(x\\) approaches \\(0\\).\n\n\n\nProperties of Lipschitz functions\n\nProposition 12.1 Lipschitz continuous functions have the following properties:\n\nIf a function \\(f \\colon (\\ALPHABET X, d_X) \\to (\\ALPHABET Y, d_Y)\\) is Lipschitz continuous, then \\(f\\) is uniformly continuous and measurable.\n\\(\\| f\\|_L = 0\\) if and only if \\(f\\) is a constant.\nIf \\(f \\colon (\\ALPHABET X, d_X) \\to (\\ALPHABET Y, d_Y)\\) and \\(g \\colon (\\ALPHABET Y, d_Y) \\to (\\ALPHABET Z, d_Z)\\) are Lipschitz continuous, then \\[ \\| f \\circ g \\|_L \\le \\| f \\|_L \\cdot \\| g \\|_L. \\]\nThe \\(\\| \\cdot \\|_{L}\\) is a seminorm on the vector space of Lipschitz functions from a metric space \\((\\ALPHABET X, d_X)\\) to \\((\\ALPHABET Y, d_Y)\\). In particular, \\(\\| \\cdot \\|_L\\) has the following properties: \\(\\| f \\|_L \\in [0, ∞]\\), \\(\\| α f\\|_L = |α| \\cdot \\|f\\|_L\\) for any \\(α \\in \\reals\\), and \\(\\| f_1 + f_2 \\|_L \\le \\|f_1 \\|_L + \\|f_2 \\|_L\\).\nGiven a family of functions \\(f_i\\), \\(i \\in I\\), on the same metric space such that \\(\\sup_{i \\in I} f_i < ∞\\), \\[ \\| \\sup_{i \\in I} f_i \\|_{L} \\le \\sup_{i \\in I} \\| f_i \\|_{L}. \\]\nLet \\(f_n\\), \\(n \\in \\integers_{\\ge 1}\\), and \\(f\\) be functions from \\((\\ALPHABET X, d_X)\\) to \\((\\ALPHABET Y, d_Y)\\). If \\(f_n\\) converges pointwise to \\(f\\) for \\(n \\to ∞\\), then \\[ \\| f \\|_{L} \\le \\lim\\inf_{n \\to ∞} \\| f_i \\|_{L}. \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#kantorovich-distance",
    "href": "mdps/lipschitz-mdps.html#kantorovich-distance",
    "title": "12  Lipschitz MDPs",
    "section": "12.2 Kantorovich distance",
    "text": "12.2 Kantorovich distance\nLet \\(\\mu\\) and \\(\\nu\\) be probability measures on \\((\\ALPHABET X, d_X)\\). The Kantorovich distance between distributions \\(\\mu\\) and \\(\\nu\\) is defined as: \\[ K(\\mu,\\nu) = \\sup_{f : \\| f\\|_L \\le 1 }\n   \\left| \\int_{\\ALPHABET X} f\\, d\\mu - \\int_{\\ALPHABET X} f\\, d\\nu \\right|. \\]\nThe next results follow immediately from the definition of Kantorovich distance.\n\nProposition 12.2 For any Lipschitz function \\(f \\colon (\\ALPHABET X, d_X) \\to (\\reals, \\lvert \\cdot \\rvert)\\), and \\(μ,ν\\) are probability measures on \\((\\ALPHABET X, d_X)\\), \\[ \\left|\n  \\int_{\\ALPHABET X} f\\, dμ - \\int_{\\ALPHABET X} f\\, dν \\right| \\le\n  \\| f \\|_L \\cdot K(μ,ν). \\]\n\n\nSome examples\n\nLet \\((\\ALPHABET X, d_X)\\) be a metric space and for any \\(x,y \\in \\ALPHABET X\\), let \\(δ_x\\) and \\(δ_y\\) denote the Dirac delta distributions centered at \\(x\\) and \\(y\\). Then, \\[ K(δ_x, δ_y) = d_X(x,y). \\]\nLet \\((\\ALPHABET X, d_X)\\) be a Euclidean space with Euclidean norm. Let \\(μ \\sim \\mathcal{N}(m_1, \\Sigma_1)\\) and \\(ν \\sim \\mathcal{N}(m_2, \\Sigma_2)\\) be two Gaussian distributions on \\(\\ALPHABET X\\). Then, \\[K(μ,ν) = \\sqrt{ \\| m_1 - m_2 \\|_2^2\n+ \\text{Tr}( \\Sigma_1 + \\Sigma_2 - 2(\\Sigma_2^{1/2} \\Sigma_1 \\Sigma_2^{1/2})^{1/2} ) }. \\] If the two covariances commute, i.e. \\(\\Sigma_1\\Sigma_2 = \\Sigma_2 \\Sigma_1\\), then, \\[K(μ,ν) = \\sqrt{ \\| m_1 - m_2 \\|_2^2\n+ \\| \\Sigma_1^{1/2} - \\Sigma_2^{1/2} \\|^2_F},\\] where \\(\\| ⋅ \\|_{F}\\) denotes the Frobeinus norm of a matrix.\nWhen \\(\\Sigma_1 = \\Sigma_2\\), we have \\[K(μ,ν) = \\| m_1 - m_2 \\|_2. \\]\nIf \\(\\ALPHABET X = \\reals\\) and \\(d_X = | \\cdot |\\), then for any two distributions \\(μ\\) and \\(ν\\), \\[ K(μ,ν) = \\int_{-∞}^∞ \\left| F_μ(x) - F_ν(x) \\right| dx, \\] where \\(F_μ\\) and \\(F_ν\\) denote the CDF of \\(μ\\) and \\(ν\\).\nFurthermore, if \\(μ\\) is stochastically dominated by \\(ν\\), then \\(F_μ(x) \\ge F_ν(x)\\). Thus, \\[ K(μ, ν) = \\bar μ - \\bar ν \\] where \\(\\bar μ\\) and \\(\\bar ν\\) are the means of \\(μ\\) and \\(ν\\)."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#lipschitz-mdps",
    "href": "mdps/lipschitz-mdps.html#lipschitz-mdps",
    "title": "12  Lipschitz MDPs",
    "section": "12.3 Lipschitz MDPs",
    "text": "12.3 Lipschitz MDPs\nNow consider an MDP where the state and action spaces are Metric spaces. We denote the corresponding metric by \\(d_S\\) and \\(d_A\\) respectively. For ease of exposition, we define a metric \\(d\\) on \\(\\ALPHABET S × \\ALPHABET A\\) by \\[ d( (s_1, a_1), (s_2, a_2) ) = d_S(s_1, s_2) + d_A(a_1, a_2). \\]\nWe allow for randomized policies. Thus, given any state \\(s \\in \\ALPHABET S\\), \\(π(\\cdot | s)\\) is a probability distribution on \\(\\ALPHABET A\\). We say that a (possibly) randomized policy \\(π\\) has a Lipschitz constant of \\(L_π\\) if for any \\(s_1, s_2 \\in \\ALPHABET S\\), \\[ K(π(\\cdot| s_1), π(\\cdot | s_2)) \\le L_π d_S(s_1, s_2). \\]\nNote that if \\(π\\) is deterministic, then due to property of Kantorovich distance between delta distributions, the above relationship simplifies to \\[ d_A(π(s_1), π(s_2)) \\le L_π d_S(s_1, s_2). \\]\n\nDefinition 12.1 An MDP is \\((L_c, L_p)\\)-Lipschitz if for all \\(s_1, s_2 \\in \\ALPHABET S\\) and \\(a_1, a_2 \\in \\ALPHABET A\\),\n\n\\(| c(s_1, a_1) - c(s_2, a_2) | \\le L_c\\bigl( d_S(s_1, s_2) + d_A(a_1, a_2) \\bigr)\\).\n\\(K(p(\\cdot | s_1, a_1), p(\\cdot | s_2, a_2)) \\le L_p\\bigl( d_S(s_1, s_2) + d_A(a_1, a_2) \\bigr)\\).\n\n\n\nLipschitz continuity of Bellman updates\nWe now prove a series of results for the Lipschitz continuity of Bellman updates.\n\nLemma 12.1 Let \\(V \\colon \\ALPHABET S \\to \\reals\\) be \\(L_V\\)-Lipschitz continuity. Define \\[ Q(s,a) = c(s,a) + γ \\int V(y) p(y|s,a)dy. \\] Then \\(Q\\) is \\((L_c + γ L_p L_V)\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider, \\[\\begin{align*}\n| Q(s_1, a_1) - Q(s_2, a_2) | &\\stackrel{(a)}\\le\n| c(s_1, a_1) - c(s_2, a_2) | \\\\\n& \\quad +\n\\beta \\left|\\int V(y) p(y|s_1, a_1) dy -\n             \\int V(y) p(y|s_2, a_2) dy \\right|\n  \\\\\n  &\\stackrel{(b)}\\le  L_c d( (s_1, a_1), (s_2, a_2) ) +\n  \\beta L_V L_p d( (s_1, a_1), (s_2, a_2) ),\n\\end{align*}\\] where \\((a)\\) follows from the triangle inequality and \\((b)\\) follows from Proposition 12.2. Thus, \\(L_Q = L_c + γ L_p L_V\\).\n\n\n\n\nLemma 12.2 Let \\(Q \\colon \\ALPHABET S × \\ALPHABET A \\to \\reals\\) be \\(L_Q\\)-Lipschitz continuous. Define \\[V(s) = \\min_{a \\in \\ALPHABET A} Q(s,a).\\] Then \\(V\\) is \\(L_Q\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(s_1, s_2 \\in \\ALPHABET S\\) and let \\(a_1\\) and \\(a_2\\) denote the corresponding optimal action. Then, \\[ \\begin{align*}\nV(s_1) - V(s_2) &= Q(s_1, a_1) - Q(s_2, a_2) \\\\\n&\\stackrel{(a)}\\le Q(s_1, a_2) - Q(s_2, a_2) \\\\\n&\\stackrel{(b)}\\le L_Q( d_S(s_1, s_2) + d_A(a_2, a_2) )\\\\\n&= L_Q d_S(s_1, s_2).\n\\end{align*} \\]\nBy symmetry, \\[ V(s_2) - V(s_1) \\le L_Q d_S(s_2, s_1). \\] Thus, \\[ | V(s_1) - V(s_2) | \\le L_Q d_S(s_1, s_2). \\] Thus, \\(V\\) is \\(L_Q\\)-Lipschitz continuous.\n\n\n\n\nLemma 12.3 Let \\(Q \\colon \\ALPHABET S × \\ALPHABET A \\to \\reals\\) be \\(L_Q\\)-Lipschitz continuous and \\(π\\) be a (possibly randomized) \\(L_π\\)-Lipschitz policy. Define \\[V_π(s) = \\int Q(s, a) π(a | s) du.\\] Then, \\(V_π\\) is \\(L_Q( 1 + L_π)\\)-Lipschitz continuous.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(s_1, s_2 \\in \\ALPHABET S\\), consider \\[ \\begin{align}\n| V_π(s_1) - V_π(s_2) | &=\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_2) du \\right|\n\\notag \\\\\n&\\stackrel{(a)}\\le\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_1) du \\right|\n\\notag \\\\\n& \\quad +\n\\left| \\int Q(s_2, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_2) du \\right|\n\\label{eq:split}\n\\end{align} \\] where \\((a)\\) follows from the triangle inequality. Now we consider both terms separately.\nThe first term of \\eqref{eq:split} simplifies as follows: \\[\\begin{align}\n\\left| \\int Q(s_1, a) π(a | s_1) du - \\int Q(s_2, a) π(a | s_1) du \\right|\n&\\stackrel{(b)}\\le\n\\int \\left|Q(s_1, a) - Q(s_2, a)\\right| π(a | s_1) du \\notag \\\\\n&\\stackrel{(c)}\\le\n\\int L_Q d_S(s_1, s_2) π(a | s_1) du \\notag \\\\\n&= L_Q d_S(s_1, s_2), \\label{eq:first}\n\\end{align} \\] where \\((b)\\) follows from the triangle inequality and \\((c)\\) follows from Lipschitz continuity of \\(Q\\).\nThe second term of \\eqref{eq:split} simplifies as follows: \\[ \\begin{align}\n  \\left| \\int Q(s, a) π(a | s_1) du - \\int Q(s,a) π(a | s_2) du \\right|\n  &\\stackrel{(d)}\\le L_Q K (π(\\cdot | s_1), π(\\cdot | s_2))\n  \\notag \\\\\n  &\\stackrel{(e)}\\le L_Q L_π d_S(s_1, s_2),\n  \\label{eq:second}\n  \\end{align}\n\\] where the \\((d)\\) inequality follows from Proposition 12.2 and \\((e)\\) follows from the definition of Lipschitz continuous policy.\nSubstituting \\eqref{eq:first} and \\eqref{eq:second} in \\eqref{eq:split}, we get \\[ \\begin{align*}\n| V_π(s_1) - V_π(s_2) | &\\le L_Q d_S(s_1, s_2) + L_Q L_π d_S(s_1, s_2)\n\\\\\n&= L_Q(1 + L_π) d_S(s_1, s_2).\n\\end{align*} \\] Thus, \\(V\\) is Lipschitz continuous with Lipschitz constant \\(L_Q(1 + L_π)\\)."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#lipschitz-continuity-of-value-iteration",
    "href": "mdps/lipschitz-mdps.html#lipschitz-continuity-of-value-iteration",
    "title": "12  Lipschitz MDPs",
    "section": "12.4 Lipschitz continuity of value iteration",
    "text": "12.4 Lipschitz continuity of value iteration\n\nLemma 12.4 Consider a discounted infinite horizon MDP which is \\((L_c, L_p)\\)-Lipschitz. Start with \\(V^{(0)} = 0\\) and recursively define\n\n\\(\\displaystyle Q^{(n+1)}(s,a) = c(s,a) + γ \\int V^{(n)}(y) p(y|s,a) dy.\\)\n\\(\\displaystyle V^{(n+1)}(s) = \\min_{a \\in \\ALPHABET A} Q^{(n+1)}(s,a).\\)\n\nThen, \\(V^{(n)}\\) is Lipschitz continuous and its Lipschitz constant \\(L_{V^{(n)}}\\) satisfies the following recursion: \\[L_{V^{(n+1)}} = L_c + γ L_p L_{V^{(n)}}.\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by induction. For \\(n=1\\), \\(Q^{(1)}(s,a) = c(s,a)\\), which is Lipschitz with Lipschitz constant \\(L_{Q^{(1)}} = L_c\\). Then, by Lemma 12.2, \\(V^{(1)}\\) is Lipschitz with Lipschitz constant \\(L_{V^{(1)}} = L_{Q^{(1)}} = L_c\\). This forms the basis of induction. Now assume that \\(V^{(n)}\\) is \\(L_{V^{(n)}}\\)-Lipschitz. Then, by Lemma 12.1, \\(Q^{(n+1)}\\) is \\((L_c + γL_p L_{V^{(n)}})\\)-Lipschitz. Therefore, by Lemma 12.2, \\(V^{(n+1)}\\) is Lipschitz with constant \\[ L_{V^{(n+1)}} = L_c + γ L_p L_{V^{(n)}}. \\space\\Box\\]\n\n\n\n\nLemma 12.5 Consider a discounted infinite horizon MDP which is \\((L_c, L_p)\\)-Lipschitz and let \\(π\\) be any randomized time-homogeneous policy which is \\(L_π\\)-Lipschitz. Start with \\(V^{(0)} = 0\\) and then recursively define\n\n\\(V^{(n)}_π(s) = \\int Q^{(n)}_π(s,a)π(a|s) du.\\)\n\\(\\displaystyle Q^{(n+1)}_π(s,a) = c(s,a) + γ \\int V^{(n)}_π(y) p(y|s,a) dy.\\)\n\nThen, then \\(Q^{(n)}_π\\) is Lipschitz continuous and its Lipschitz constant \\(L_{Q^{(n)}_π}\\) satisfies the follwoing recursion: \\[ L_{Q^{(n+1)}_π} + L_c + \\beta(1 + L_π)L_p L_{Q^{(n)}_π}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the result by induction. For \\(n=1\\), \\(Q^{(1)}_π(s,a) = c(s,a)\\), which is Lipschitz with Lipschitz constant \\(L_{Q^{(1)}_π} = L_c\\). This forms the basis of induction. Now assume that \\(Q^{(n)}_π\\) is \\(L_{Q^{(n)}_π}\\)-Lipschitz. Then, by Lemma 12.3, \\(V^{(n)}_π\\) is Lipschitz with Lipschitz constant \\(L_{V^{(n)}_π} = L_{Q^{(n)}_π}(1 + L_π)\\) and by Lemma 12.1, \\(Q^{(n+1)}_π\\) is Lipschitz with Lipschitz constant \\(L_{Q^{(n+1)}_π} = L_c + γL_p L_{V^{(n)}_π}.\\) Combining these two we get \\[ L_{Q^{(n+1)}_π} + L_c + \\beta(1 + L_π)L_p L_{Q^{(n)}_π}. \\]\n\n\n\n\nTheorem 12.1 Given any \\((L_c, L_p)\\)-Lipschitz MDP, if \\(\\beta L_p < 1\\), then the infinite horizon \\(\\beta\\)-discounted value function \\(V\\) is Lipschitz continuous with Lipschitz constant \\[ L_{V} = \\frac{L_c}{1 - γ L_p} \\] and the action-value function \\(Q\\) is Lipschitz with Lipschitz constant \\[ L_Q = L_V = \\frac{L_c}{1 - γ L_p}. \\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the sequence of \\(L_n = L_{V^{(n)}}\\) values. For simplicity write \\(α = γ L_p\\). Then the sequence \\(\\{L_n\\}_{n \\ge 1}\\) is given by: \\(L_1 = L_c\\) and for \\(n \\ge 1\\), \\[ L_{n+1} = L_c + α L_n. \\] Hence, \\[ L_n = L_c + α L_c + \\dots + α^{n-1} L_c = \\frac{1 - α^n}{1 - α} L_c. \\] This sequence converges if \\(|α| < 1\\). Since \\(α\\) is non-negative, this is equivalent to \\(α < 1\\), which is true by hypothesis. Hence \\(L_n\\) is a convergent sequence. At convergence, the limit \\(L_V\\) must satisfy the fixed point of the recursion relationship introduced in Lemma 12.4, hence \\[ L_V = L_c + γ L_p L_V. \\] Consequently, the limit is equal to \\[ L_V = \\frac{L_c}{1 - γ L_p}. \\] The Lipschitz constant of \\(Q\\) follows from Lemma 12.1.\n\n\n\n\nTheorem 12.2 Given any \\((L_c, L_p)\\)-Lipschitz MDP and an \\(L_π\\)-Lipschitz (possibly randomized) time-homogeneous policy \\(π\\), if \\(\\beta (1 + L_π) L_p < 1\\), then the infinite horizon \\(\\beta\\)-discounted value-action function \\(Q_π\\) is Lipschitz continuous with Lipschitz constant \\[ L_{Q_π} = \\frac{L_c}{1 - γ(1 + L_π) L_p} \\] and the value function \\(V_π\\) is Lipschitz with Lipschitz constant \\[ L_{V_π} = L_{Q_π}(1 + L_π) =\n   \\frac{L_c(1 + L_π)}{1 - γ(1 + L_π) L_p}. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nThe restrictive assumption in the result is that \\(γ(1 + L_π)L_p < 1\\). For a specific model, even when this assumption does not hold, it may be possible to directly check if the \\(Q\\)-function is Lipschitz continuous. Such a direct check often gives a better Lipschitz constant.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the sequence of \\(L_n = L_{Q^{(n)}_π}\\) values. For simplicity, write \\(α = γ(1 + L_π)L_p\\). Then, the sequence \\(\\{L_n\\}_{n \\ge 1}\\) is given by: \\(L_1 = L_c\\) and for \\(n \\ge 1\\), \\[L_{n+1} = L_c + α L_n. \\] Hence, \\[ L_n = L_c + α L_c + \\dots + α^{n-1} L_c = \\frac{1 - α^n}{1 - α} L_c. \\] This sequence converges if \\(|α| < 1\\). Since \\(α\\) is non-negative, this is equivalent to \\(α < 1\\), which is true by hypothesis. Hence \\(L_n\\) is a convergent sequence. At convergence, the limit \\(L_{Q_π}\\) must satisfy the fixed point of the recursion relationship introduced in Lemma 12.5, hence \\[ L_{Q_π} = L_c + γ(1 + L_π)L_p L_{Q_π}. \\] Consequently, the limit is equal to \\[ L_{Q_π} = \\frac{L_c}{1 - γ(1 + L_π) L_p}. \\]\nThe Lipschitz constant of \\(V_π\\) follows from Lemma 12.3."
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#influence-radius",
    "href": "mdps/lipschitz-mdps.html#influence-radius",
    "title": "12  Lipschitz MDPs",
    "section": "12.5 Influence Radius",
    "text": "12.5 Influence Radius\nWhen the \\(Q\\)-function of an MDP is Lipschitz continuous, then the optimal action does not change too abruptly. More precisely, suppose an action \\(a\\) is optimal at state \\(s\\). Then, we can identify a hyperball \\(B(s, ρ(s))\\) of radius \\(ρ(s)\\) centered around \\(s\\) such that \\(a\\) is guaranteed to be the dominating action in \\(ρ(s)\\). This radius \\(ρ(s)\\) is called the influence radius.\nLet \\(π\\) denote the optimal policy, i.e., \\[ π(s) = \\arg \\min_{a \\in \\ALPHABET A} Q(s,a) \\] and \\(h\\) denote the second best action, i.e., \\[ h(s) = \\arg \\min_{a \\in \\ALPHABET A \\setminus \\{π(s)\\}} Q(s,a). \\] Define the domination value of state \\(s\\) to be \\[ Δ(s) = Q(s, h(s)) - Q(s, π(s)). \\]\n\nTheorem 12.3 For a Lipschitz continuous \\(Q\\)-function, the influence radius at state \\(s\\) is given by \\[ ρ(s) = \\frac{ Δ(s) }{ 2 L_Q }. \\]\n\n\n\n\n\n\n\nRemark\n\n\n\nCombining Theorem 12.2 and Theorem 12.3 implies that under the condition of Theorem 12.2, the influence radius at state \\(s\\) is at least \\[ ρ(s) = Δ(s)(1 - γ(1 + L_π)L_p)/2L_c. \\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe intuition behind the proof is the following. The value of the action \\(π(s)\\) can only decrease by \\(L_Q ρ(s)\\) in \\(B(s, ρ(s))\\), while the value of the second best action \\(h(s)\\) can only increase by \\(L_Q ρ(s)\\). So, the shortest distance \\(ρ(s)\\) from \\(s\\) needed for an action \\(h(s)\\) to “catch-up” with action \\(π(s)\\) should satisfy \\(2 L_Q ρ(s) = Δ(s)\\) or \\(ρ(s) = Δ(s)/2L_Q\\).\nFormally, for any \\(s' \\in B(s,ρ(s))\\), \\(d_S(s,s') \\le ρ(s)\\). Thus, for any action \\(a \\in \\ALPHABET A\\), \\[ | Q(s,a) - Q(s',a)| \\le L_Q d_S(s,s') \\le L_Q ρ(s). \\] Equivalently, \\[ Q(s,a) - L_Q ρ(s) \\le Q(s',a) \\le Q(s,a) + L_Q ρ(s) \\] which states that as \\(s'\\) moves away from \\(s\\), the value of \\(Q(s',a)\\) remains within a symmetric bound that depends on the radius \\(ρ(s)\\). Since this bound holds for all \\(a\\), they also hold for \\(a = π(s)\\). Thus, \\[ Q(s, π(s)) - L_Q ρ(s) \\le Q(s', π(s)) \\le Q(s, π(s)) + L_Q ρ(s). \\]\nSince \\(π(s)\\) is the optimal action, for any other action \\(a \\neq π(s)\\), \\[ Q(s,π(s)) \\le Q(s,a). \\] Thus, the action \\(π(s)\\) is optimal as long as the upper bound on \\(Q(s', π(s))\\) is lower than the lower bound on \\(Q(s',a)\\), i.e., \\[ Q(s, π(s)) + L_Q ρ(s) \\le Q(s,a) - L_Q ρ(s).  \\] Thus, the maximum value of \\(ρ(s)\\) is when the relationship holds with equality, i.e., \\[ ρ(s) = \\frac{Q(s,a) - Q(s,π(s))}{2 L_Q} \\ge \\frac{Δ(s)}{2 L_Q}. \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#exercises",
    "href": "mdps/lipschitz-mdps.html#exercises",
    "title": "12  Lipschitz MDPs",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 12.1 Let \\((\\ALPHABET S, d_S)\\) be a metric space and \\(s, s' \\in \\ALPHABET S\\). Consider two Bernoulli measures \\[ μ = a δ_s + (1-a) δ_{s'}, \\qquad\n      ν = b δ_s + (1-b) δ_{s'}. \\]\nShow that \\[ K(μ,ν) = |a - b| d(s,s'). \\]"
  },
  {
    "objectID": "mdps/lipschitz-mdps.html#notes",
    "href": "mdps/lipschitz-mdps.html#notes",
    "title": "12  Lipschitz MDPs",
    "section": "Notes",
    "text": "Notes\nThe material in this section is taken from Rachelson and Lagoudakis (2010) and Hinderer (2005).\n\n\n\n\nHinderer, K. 2005. Lipschitz continuity of value functions in Markovian decision processes. Mathematical Methods of Operations Research 62, 1, 3–22. DOI: 10.1007/s00186-005-0438-1.\n\n\nRachelson, E. and Lagoudakis, M.G. 2010. On the locality of action domination in sequential decision making. Proceedings of 11th international symposium on artificial intelligence and mathematics. Available at: https://oatao.univ-toulouse.fr/17977/."
  },
  {
    "objectID": "pomdps/intro.html#history-dependent-dynamic-program",
    "href": "pomdps/intro.html#history-dependent-dynamic-program",
    "title": "13  Introduction",
    "section": "13.1 History dependent dynamic program",
    "text": "13.1 History dependent dynamic program\nOur first step to develop an efficient dynamic programming decomposition is to simply ignore efficiency and develop a dynamic programming decomposition. We start by deriving a recursive formula to compute the performance of a generic history dependent strategy \\(π = (π_1, \\dots, π_T)\\).\n\nPerformance of history-dependent strategies\nLet \\(H_t = (Y_{1:t}, A_{1:t-1})\\) denote all the information available to the decision maker at time \\(t\\). Thus, given any history dependent strategy \\(π\\), we can write \\(A_t = π_t(H_t)\\). Define the cost-to-go functions as follows: \\[\n  J_t(h_t; π) = \\EXP^π\\biggl[ \\sum_{s=t}^T c_s(S_s, A_s) \\biggm| H_t = h_t\n  \\biggr].\n\\] Note that \\(J_t(h_t; π)\\) only depends on the future strategy \\((π_t, \\dots, π_T)\\). These functions can be computed recursively as follows: \\[\\begin{align*}\n  J_t(h_t; π) &= \\EXP^π\\biggl[ \\sum_{s=t}^T c_s(H_s, π_s(H_s)) \\biggm|\n    H_t = h_t \\biggr] \\\\\n    &\\stackrel{(a)}= \\EXP^π \\biggl[ c_t(h_t, π_t(h_t)) + \\EXP^π\\biggl[\n    \\sum_{s=t+1}^T c_s(S_s, π_s(S_s)) \\biggm| H_{t+1} \\biggr] \\biggm|\n    H_t = h_t \\biggr]  \\\\\n    &= \\EXP^π[ c_t(h_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid H_t = h_t ],\n\\end{align*}\\] where \\((a)\\) follows from the towering property of conditional expectation and the fact that \\(H_t \\subseteq H_{t+1}\\).\nThus, we can use the following dynamic program to recursively compute the performance of a history-dependent strategy: \\(J_{T+1}(h_{T+1}) = 0\\) and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\nJ_t(h_t; π) = \\EXP^π [ c_t(h_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid\n  H_t = h_t ].\n\\]\n\n\nHistory-dependent dynamic programming decomposition\nWe can use the above recursive formulation for performance evaluation to derive a history-dependent dynamic program.\n\nTheorem 13.1 Recursively define _value functions \\(\\{V_t\\}_{t = 1}^{T+1}\\), where \\(V_t \\colon \\ALPHABET H_t \\to \\reals\\) as follows: \\[\\begin{equation}\n  V_{T+1}(h_{T+1}) = 0\n\\end{equation}\\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n  Q_t(h_t, a_t) &= \\EXP[ c_t(S_t, a_t) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = a_t ] \\\\\n  V_t(h_t) &= \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t)\n\\end{align}\\] Then, a history-dependent policy \\(π\\) is optimal if and only if it satisfies \\[\\begin{equation} \\label{eq:history-verification}\n  π_t(h_t) \\in \\arg \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t).\n\\end{equation}\\]\n\nThe proof idea is similar to the proof for MDPs. Instead of proving the above result, we prove a related result.\n\nTheorem 13.2 (The comparison principle) For any history-dependent strategy \\(π\\) \\[ J_t(h_t; π) \\ge V_t(h_t) \\] with equality at \\(t\\) if and only if the future straegy \\(π_{t:T}\\) satisfies the verification step \\eqref{eq:history-verification}.\n\nNote that the comparison principle immediately implies that the strategy obtained using dynamic program of Theorem 13.1 is optimal. The proof of the comparison principle is almost identical to the proof for MDPs.\n\n\n\n\n\n\nProof of the comparison principle\n\n\n\n\n\nThe proof proceeds by backward induction. Consider any history dependent policy \\(π = (π_1, \\dots, π_T)\\). For \\(t = T+1\\), the comparison principle is satisfied by definition and this forms the basis of induction. We assume that the result holds for time \\(t+1\\), which is the induction hypothesis. Then for time \\(t\\), we have \\[\\begin{align*}\n  V_t(h_t) &= \\min_{a_t \\in \\ALPHABET A} Q_t(h_t, a_t) \\\\\n  &\\stackrel{(a)}= \\min_{a_t \\in \\ALPHABET A}\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = π_t(h_t) ]\n  \\\\\n  &\\stackrel{(b)}\\le\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + V_{t+1}(H_{t+1}) \\mid\n  H_t = h_t, A_t = π_t(h_t)]\n  \\\\\n  &\\stackrel{(c)}\\le\n   \\EXP^π[ c_t(S_t, π_t(h_t)) + J_{t+1}(H_{t+1}; π) \\mid\n  H_t = h_t, A_t = π_t(h_t)]\n  \\\\\n  &= J_t(h_t, π).\n\\end{align*}\\] where \\((a)\\) follows from the definition of the \\(Q\\)-function; \\((b)\\) follows from the definition of minimization; and \\((c)\\) follows from the induction hyothesis. We have the equality at step \\((b)\\) iff \\(π_t\\) satisfies the verification step \\eqref{eq:history-verification} and have the equality in step \\((c)\\) iff \\(π_{t+1:T}\\) is optimal (this is part of the induction hypothesis). Thus, the result is true for time \\(t\\) and, by the principle of induction, is true for all time."
  },
  {
    "objectID": "pomdps/intro.html#the-notion-of-an-information-state",
    "href": "pomdps/intro.html#the-notion-of-an-information-state",
    "title": "13  Introduction",
    "section": "13.2 The notion of an information state",
    "text": "13.2 The notion of an information state\nNow that we have obtained a dynamic programming decomposition, let’s try to simplify it. To do so, we define the notion of an information state.\n\n\n\n\n\n\nInformation state\n\n\n\nA stochastic process \\(\\{Z_t\\}_{t = 1}^T\\), \\(Z_t \\in \\ALPHABET Z\\), is called an information state if \\(Z_t\\) be a function of \\(H_t\\) (which we denote by \\(Z_t = φ_t(H_t)\\)) and satisfies the following two properties:\nP1. Sufficient for performance evaluation, i.e., \\[ \\EXP^π[ c_t(S_t, A_t) \\mid H_t = h_t, A_t = a_t]\n    =  \\EXP[ c_t(S_t, A_t) \\mid Z_t = φ_t(h_t), A_t = a_t ] \\]\nP2. Sufficient to predict itself, i.e., for any Borel measurable subset \\(B\\) of \\(\\ALPHABET Z\\), we have \\[ \\PR^π(Z_{t+1} \\in B \\mid H_t = h_t, A_t = a_t) =\n       \\PR(Z_{t+1} \\in B \\mid Z_t = φ_t(h_t), A_t = a_t).\n    \\]\n\n\nInstead of (P2), the following sufficient conditions are easier to verify in some models:\n\n\n\n\n\n\nAn equivalent characterization\n\n\n\nP2a. Evolves in a state-like manner, i.e., there exist measurable functions \\(\\{ψ_t\\}_{t=1}^T\\) such that \\[ Z_{t+1} = ψ_t(Z_t, Y_{t+1}, A_t). \\]\nP2b. Is sufficient for predicting future observations, i.e., for any Borel subset \\(B\\) of \\(\\ALPHABET Y\\), \\[ \\PR^π(Y_{t+1} \\in B | H_t = h_t, A_t = a_t) =\n        \\PR(Y_{t+1} \\in B | Z_t = φ_t(h_t), A_t = a_t).\n     \\]\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThe right hand sides of (P1) and (P2) as well as (P2a) and (P2b) do not depend on the choice of the policy \\(π\\).\n\n\n\nProposition 13.1 : (P2a) and (P2b) imply (P2).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any Borel measurable subset \\(B\\) of \\(\\ALPHABET Z\\), we have \\[\\begin{align*}\n  \\hskip 1em & \\hskip -1em\n  \\PR(Z_{t+1} \\in B \\mid H_t = h_t, A_t = a_t)  \n  \\stackrel{(a)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(Y_{t+1} = y_{t+1}, Z_{t+1} \\in B\n  \\mid H_t = h_t, A_t = a_t ]\n  \\\\\n  &\\stackrel{(b)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\IND\\{ ψ_t(φ_t(h_t), y_{t+1}, a_t) \\}\n  \\PR(Y_{t+1} = y_{t+1} \\mid H_t = h_t, A_t = a_t)\n  \\\\\n  &\\stackrel{(c)}= \\sum_{y_{t+1} \\in \\ALPHABET Y} \\IND\\{ ψ_t(φ_t(h_t), y_{t+1}, a_t) \\}\n  \\PR(Y_{t+1} = y_{t+1} \\mid Z_t = φ_t(h_t), A_t = a_t)\n  \\\\\n  &\\stackrel{(d)}=\n  \\PR(Z_{t+1} \\in B \\mid Z_t = φ_t(h_t), A_t = a_t)  \n\\end{align*}\\] where \\((a)\\) follows from the law of total probability, \\((b)\\) follows from (P2a), \\((c)\\) follows from (P2b), and \\((d)\\) from the law of total probability."
  },
  {
    "objectID": "pomdps/intro.html#examples-of-an-information-state",
    "href": "pomdps/intro.html#examples-of-an-information-state",
    "title": "13  Introduction",
    "section": "13.3 Examples of an information state",
    "text": "13.3 Examples of an information state\nWe start by define the belief state \\(b_t \\in Δ(\\ALPHABET S)\\) as follows: for any \\(s \\in \\ALPHABET S\\) \\[ b_t(s) = \\PR^π(S_t = s \\mid H_t = h_t). \\] The belief state is a function of the history \\(h_t\\). When we want to explicitly show the dependence of \\(b_t\\) on \\(h_t\\), we write it as \\(b_t[h_t]\\).\n\nLemma 13.1 The belief state \\(b_t\\) does not depend on the policy \\(π\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is an extremely important result which has wide-ranging implications in stochastic control. For a general discussion of this point, see Witsenhausen (1975).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the law of total probability and Bayes rule, we have \\[\\begin{equation} \\label{eq:belief}\n  \\PR(s_t | y_{1:t}, a_{1:t-1})\n  = \\sum_{s_{1:t-1}} \\PR(s_{1:t} | y_{1:t}, a_{1:t-1})\n  = \\sum_{s_{1:t-1}}\n   \\frac{\\PR(s_{1:t}, y_{1:t}, a_{1:t-1})}\n   {\\sum_{s'_{1:t}} \\PR(s'_{1:t}, y_{1:t}, a_{1:t-1})}\n\\end{equation}\\]\nNow consider \\[\\begin{align*}\n  \\PR(s_{1:t}, y_{1:t}, a_{1:t-1}) &=\n  \\PR(s_1) \\PR(y_1 | s_1) \\IND\\{ a_1 = π_1(y_1) \\} \\\\\n  & \\times\n  \\PR(s_2 | s_1, a_1) \\PR(y_2 | s_2) \\IND \\{ a_2 = π_2(y_{1:2}, a_1)\\} \\\\\n  & \\times \\cdots \\\\\n  & \\times\n  \\PR(s_{t-1} | s_{t-2}, a_{t-2}) \\PR(y_{t-1} | s_{t-1}) \\IND \\{ a_{t-1} =\n  π_{t-1}(y_{1:t-1}, a_{1:t-2}) \\} \\\\\n  & \\times\n  \\PR(s_{t} | s_{t-1}, a_{t-1}) \\PR(y_{t} | s_{t}).\n\\end{align*}\\] Substitute the above expression in both the numerator and the denominator of \\eqref{eq:belief}. Observe that the terms of the form \\(\\IND\\{ a_s = π_s(y_{1:s}, a_{1:s-1})\\) are common to both the numerator and the denominator and cancel each other. Thus, \\[\\begin{equation} \\label{eq:belief-fn}\n  \\PR(s_t | y_{1:t}, a_{1:t-1}) = \\sum_{s_{1:t-1}}\n  \\frac{ \\prod_{s=1}^t \\PR(s_s \\mid s_{s-1}, a_{s-1}) \\PR(y_s \\mid s_s) }\n  { \\sum_{s'_{1:t}} \\prod_{s=1}^t \\PR(s'_s \\mid s'_{s-1}, a_{s-1}) \\PR(y_s \\mid s'_s) }.\n\\end{equation}\\] None of the terms here depend on the policy \\(π\\). Hence, the belief state does not depend on the policy \\(π\\).\n\n\n\n\nLemma 13.2 The belief state \\(b_t\\) updates in a state like manner. In particular, for any \\(s_{t+1} \\in \\ALPHABET S\\), we have \\[\n  b_{t+1}(s_{t+1}) = \\sum_{s_t \\in \\ALPHABET S}\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n   { \\sum_{s'_{t:t+1}} \\PR(y_{t+1} | s'_{t+1}) \\PR(s'_{t+1} | s'_t, a_t) b_t(s'_t) }.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any \\(s_{t+1} \\in \\ALPHABET S\\), consider\n\\[\\begin{align}\nb_{t+1}(s_{t+1}) &= \\PR(s_{t+1} | y_{1:t+1}, a_{1:t}) \\notag \\\\\n&= \\sum_{s_t} \\PR(s_{t:t+1} | y_{1:t+1}, a_{1:t}) \\notag \\\\\n&= \\sum_{s_t} \\frac{ \\PR(s_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) }\n  {\\sum_{s'_{t:t+1}}\\PR(s'_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) }.\n\\label{eq:update-1}\n\\end{align}\\]\nNow consider \\[\\begin{align}\n\\hskip 1em & \\hskip -1em\n\\PR(s_{t:t+1}, y_{t+1}, a_t | y_{1:t}, a_{1:t-1}) \\notag \\\\\n&= \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t)\n   \\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\n   \\PR(s_t | y_{1:t}, a_{1_t-1}) \\notag \\\\\n&= \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t)\n   \\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\n   b_t(s_t). \\label{eq:belief-2}\n\\end{align}\\] Substitute the above expression in both the numerator and the denominator of \\eqref{eq:update-1}. Observe that \\(\\IND\\{ a_t = π_t(y_{1:t}, a_{1:t-1}) \\}\\) is common to both the numerator and the denominator and cancels out. Thus, we get the result of the lemma.\n\n\n\nNow, we present three examples of information state here. See the Exercises for more examples.\n\nExample 13.1 The complete history \\(H_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove that \\(Z_t = H_t\\) satisfies properties (P1), (P2a), and (P2b).\nP1. \\(\\displaystyle \\EXP^π[ c_t(S_t, A_t) | H_t = h_t, A_t = a_t ] = \\sum_{s_t \\in \\ALPHABET S} c_t(s_t, a_t) b_t[h_t](s_t)\\).\nP2a. \\(H_{t+1} = (H_t, Y_{t+1}, A_t)\\)\nP2b. \\(\\displaystyle \\PR^π(y_{t+1} | y_{1:t}, a_{1:t}) = \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR( s_{t+1} | s_t, a_t) \\PR(s_t | y_{1:t}, a_{1:t})\\). Note that in the last term \\(\\PR^π(s_t | y_{1:t}, a_{1:t})\\) we can drop \\(a_t\\) from the conditioning because it is a function of \\((y_{1:t}, a_{1:t-1})\\). Thus, \\[ \\PR^π(s_t | y_{1:t}, a_{1:t}) = \\PR^π(s_t | y_{1:t}, a_{1:t-1}) =\nb_t[h_t](s_t).\\] Note that in the last step, we have used Lemma 13.1. Thus, \\(\\displaystyle \\PR^π(y_{t+1} | y_{1:t}, a_{1:t}) = \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR( s_{t+1} | s_t, a_t) b_t[h_t](s_t)\\).\n\n\n\n\nExample 13.2 The belief state \\(b_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe belief state \\(b_t\\) is a function of the history \\(h_t\\). (The exact form of this function is given by \\eqref{eq:belief-fn}). In the proof of Example 13.1, we have already shown that \\(b_t\\) satisfies (P1) and (P2b). Moreover Lemma 13.2 implies that the belief update satisfies (P2a).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nBoth the above information states are generic information states which work for all models. For specific models, it is possible to identify other information states as well. We present some examples of such an information state below.\n\n\n\nExample 13.3 An MDP is a special case of a POMDP where \\(Y_t = S_t\\). For an MDP \\(Z_t = S_t\\) is an information state.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will show that \\(Z_t = S_t\\) satisfies (P1) and (P2).\n(P1) is satisfied because the per-step cost is a function of the \\((S_t, A_t)\\). (P2) is equivalent to the control Markov property."
  },
  {
    "objectID": "pomdps/intro.html#information-state-based-dynamic-program",
    "href": "pomdps/intro.html#information-state-based-dynamic-program",
    "title": "13  Introduction",
    "section": "13.4 Information state based dynamic program",
    "text": "13.4 Information state based dynamic program\nThe main feature of an information state is that one can always write a dynamic program based on an information state.\n\nTheorem 13.3 Let \\(\\{Z_t\\}_{t=1}^T\\) be any information state, where \\(Z_t = φ_t(H_t)\\). Recursively define value functions \\(\\{ \\hat V_t \\}_{t=1}^T\\), where \\(\\hat V_t \\colon \\ALPHABET Z \\to \\reals\\), as follows: \\[ \\hat V_{T+1}(z_{T+1}) = 0 \\] and for \\(t \\in \\{T, \\dots, 1\\}\\): \\[\\begin{align}\n  \\hat Q_t(z_t, a_t) &= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}(Z_{t+1}) \\mid\n  Z_t = z_t, A_t = a_t] \\\\\n  \\hat V_t(z_t) &= \\min_{a_t \\in \\ALPHABET A} \\hat Q_t(z_t, a_t).\n\\end{align}\\] Then, we have the following: for any \\(h_t\\) and \\(a_t\\), \\[\\begin{equation} \\label{eq:history-info}\n  Q_t(h_t, a_t) = \\hat Q_t(φ_t(h_t), a_t)\n  \\quad\\text{and}\\quad\n  V_t(h_t) = \\hat V_t(φ_t(h_t)).\n\\end{equation}\\] Any strategy \\(\\hat π = (\\hat π_1, \\dots, \\hat π_T)\\), where \\(\\hat π_t \\colon \\ALPHABET Z \\to \\ALPHABET A\\), is optimal if and only if \\[\\begin{equation}\\label{eq:info-verification}\n    \\hat π_t(z_t) \\in \\arg\\min_{a_t \\in \\ALPHABET A} \\hat Q_t(z_t, a_t).\n\\end{equation}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs usual, we prove the result by backward induction. By construction, Eq. \\eqref{eq:history-info} is true at time \\(T+1\\). This forms the basis of induction. Now assume that \\eqref{eq:history-info} is true at time \\(t+1\\) and consider the system at time \\(t\\). Then, \\[\\begin{align*}\nQ_t(h_t, a_t) &= \\EXP[ c_t(S_t, A_t) + V_{t+1}(H_{t+1}) | H_t = h_t, A_t = a_t\n] \\\\\n&\\stackrel{(a)}= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}( φ_t(H_{t+1}) ) | H_t =\nh_t, A_t = a_t ]  \\\\\n&\\stackrel{(b)}= \\EXP[ c_t(S_t, A_t) + \\hat V_{t+1}( φ_t(H_{t+1}) ) | Z_t =\nφ_t(h_t), A_t = a_t ]  \\\\\n&\\stackrel{(c)}= \\hat Q_t(φ_t(h_t), a_t),\n\\end{align*}\\] where \\((a)\\) follows from the induction hypothesis, \\((b)\\) follows from the properties (P1) and (P2) of the information state, and \\((c)\\) follows from the definition of \\(\\hat Q_t\\). This shows that the action value functions are equal. By minimizing over the actions, we get that the value functions are also equal."
  },
  {
    "objectID": "pomdps/intro.html#belief-state-based-dynamic-program",
    "href": "pomdps/intro.html#belief-state-based-dynamic-program",
    "title": "13  Introduction",
    "section": "13.5 Belief state based dynamic program",
    "text": "13.5 Belief state based dynamic program\nAs shown in Example 13.2, the belief state \\(b_t\\) is an information state. Therefore, Theorem 13.3 implies that we can write a dynamic program based on \\(b_t\\). This is an important and commonly used formulation, so we study it separately and present some properties of the value functions. The belief state based dynamic program is given by: \\(V_{T+1}(b_{T+1}) = 0\\) and for \\(t \\in \\{T, \\dots, 1\\}\\), \\[\n  Q_t(b_t, a_t) =\n  \\EXP [ c_t(S_t, A_t) + V_{t+1}(B_{t+1}) \\mid B_t = b_t, A_t = a_t ].\n\\] and \\[ V_t(b_t) = \\min_{a_t \\in \\ALPHABET A} Q_t(b_t, a_t). \\]\nDefine \\[ \\PR(y_{t+1} | b_t, a_t) =\n   \\sum_{s_{t:t+1}} \\PR(y_{t+1} | s_{t+1}) \\PR(s_{t+1} | s_t, a_t) b_t(s_t).\n\\] Then, the belief update expression in Lemma 13.2 can be written as: \\[\n  b_{t+1}(s_{t+1}) =\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n  { \\PR(y_{t+1} | b_t, a_t) }.\n\\] For the ease of notation, we write this expression as \\(b_{t+1} = ψ(b_t, y_{t+1}, a_t)\\).\n\\[\\begin{align*}\n  Q_t(b_t, a_t) &= \\sum_{s_t \\in \\ALPHABET S} c_t(s_t, a_t) b_t(s_t) \\\\\n  & \\quad +  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  V_{t+1}( φ(b_t, y_{t+1}, a_t) ).\n\\end{align*}\\]\nA key property of the belief-state based value functions is the following.\n\nTheorem 13.4 The belief-state based value functions are piecewise linear and concave.\n\n\n\n\n\n\nAn illustration of a piecewise linear and concave function. Move the points around to see how the shape of the function changes.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAs usual, we prove the result using backward induction. For any \\(a_T\\), \\[ Q_T(b_T, a_T) = \\sum_{s_T \\in \\ALPHABET S} c_T(s_T, a_T) b_T(s_T) \\] is linear in \\(b_T\\). Therefore, \\[ V_T(b_T) = \\min_{a_T \\in \\ALPHABET A} Q_T(b_T, a_T) \\] is the minimum of a finite number of linear functions. Hence \\(V_T(b_T)\\) is piecewise linear and concave.\nNow assume that \\(V_{t+1}(b_{t+1})\\) is piecewise linear and concave (PWLC). Any PWLC function can be represented as a minimum of a finite number of hyperplanes. Therefore, we can find a finite set of vectors \\(\\{ A_i \\}_{i \\in I}\\) indexed by finite set \\(I\\) such that \\[\n  V_{t+1}(b) = \\min_{i \\in I} \\langle A_i, b \\rangle.\n\\]\nWe need to show that \\(V_t(b_t)\\) is piecewise linear and concave (PWLC). We first show that \\(Q_t(b_t, a_t)\\) is PWLC. For any fixed \\(a_t\\), the first term \\(\\sum_{s_t} c_t(s_t, a_t) b_t(s_t)\\) is linear in \\(b_t\\). Now consider the second term: \\[\\begin{align*}\n  \\hskip 1em & \\hskip -1em\n  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  V_{t+1}( φ(b_t, y_{t+1}, a_t) ) \\\\\n  &=\n  \\sum_{y_{t+1} \\in \\ALPHABET Y} \\PR(y_{t+1} | b_t, a_t)\n  \\min_{i \\in I}\n  \\left\\langle A_i,\n  \\frac{ \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t) }\n  { \\PR(y_{t+1} | b_t, a_t) } \\right\\rangle \\\\\n  &=\n  \\sum_{y_{t+1} \\in \\ALPHABET Y}\n  \\min_{i \\in I}\n  \\Big\\langle A_i,\n   \\PR(y_{t+1} | s_{t+1}) \\sum_{s_t} \\PR(s_{t+1} | s_t, a_t) b_t(s_t)\n   \\Big\\rangle\n\\end{align*}\\] which is the sum of PWLC functions of \\(b_t\\) and therefore PWLC in \\(b_t\\).\nThus, \\(Q_t(b_t, a_t)\\) is PWLC. Hence, \\(V_t(b_t)\\) which is the pointwise minimum of PWLC functions is PWLC. Hence, the result holds due to principle of induction.\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\nSince the value function is PWLC, we can identify a finite index set \\(I_t\\), and a set of vectors \\(\\{ A^i_t \\}_{i \\in I_t}\\) such that \\[\n    V_t(b) = \\min_{i \\in I_t} \\langle A^i_t, b \\rangle.\n\\] Smallwood and Sondik (1973) presented a “one-pass” algorithm to recursively compute \\(I_t\\) and \\(\\{ A^i_t \\}_{i \\in I_t}\\) which allows us to exactly compute the value function. Various efficient refinements of these algorithms have been presented in the literature, e.π., the linear-support algorithm (Cheng 1988), the witness algorithm (Cassandra et al. 1994), incremental pruning (Zhang and Liu 1996; Cassandra et al. 1997), duality based approach (Zhang 2009), and others. See https://pomdp.org/ for an accessible introduction to these algorithms."
  },
  {
    "objectID": "pomdps/intro.html#exercises",
    "href": "pomdps/intro.html#exercises",
    "title": "13  Introduction",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 13.1 Consider an MDP where the state space \\(\\ALPHABET S\\) is a symmetric subset of integers of the form \\(\\{-L, -L + 1, \\dots, L -1 , L\\}\\) and the action space \\(\\ALPHABET A\\) is discrete. Suppose the transition matrix \\(P(a)\\) and the cost function \\(c_t(s,a)\\) satisfy properties (A1) and (A2) of Exercise 6.6. Show that \\(Z_t = |S_t|\\) is an information state.\n\n\nExercise 13.2 Consider a linear system with state \\(x_t \\in \\reals^n\\), observations \\(y_t \\in \\reals^p\\), and action \\(u_t \\in \\reals^m\\). Note that we will follow the standard notation of linear systems and denote the system variables by lower case letters \\((x,u)\\) rather than upper case letter \\((S,A)\\). The dynamics of the system are given by \\[\\begin{align*}\n  x_{t+1} &= A x_t + B u_t + w_t  \\\\\n  y_t &= C x_t + n_t\n\\end{align*}\\] where \\(A\\), \\(B\\), and \\(C\\) are matrices of appropriate dimensions. The per-step cost is given by \\[\n  c(x_t, u_t) = x_t^\\TRANS Q x_t + u_t^\\TRANS R u_t,\n\\] where \\(Q\\) is a positive semi-definite matrix and \\(R\\) is a positive definite matrix. We make the standard assumption that the primitive random variables \\(\\{s_1, w_1, \\dots, w_T, n_1, \\dots, n_T \\}\\) are independent.\nShow that if the primitive variables are Guassian, then the conditional estimate of the state \\[\n  \\hat x_t = \\EXP[ x_t | y_{1:t}, u_{1:t-1} ]\n\\] is an information state.\n\n\nExercise 13.3 Consider a machine which can be in one of \\(n\\) ordered state where the first state is the best and the last state is the worst. The production cost increases with the state of the machine. The state evolves in a Markovian manner. At each time, an agent has the option to either run the machine or stop and inspect it for a cost. After inspection, the agent may either repair the machine (at a cost that depends on the state) or replace it (at a fixed cost). The objective is to identify a maintenance policy to minimize the cost of production, inspection, repair, and replacement.\nLet \\(τ\\) denote the time of last inspection and \\(S_τ\\) denote the state of the machine after inspection, repair, or replacement. Show that \\((S_τ, t-τ)\\) is an information state."
  },
  {
    "objectID": "pomdps/intro.html#notes",
    "href": "pomdps/intro.html#notes",
    "title": "13  Introduction",
    "section": "Notes",
    "text": "Notes\nThe discussion in this section is taken from Subramanian et al. (2022). Information state may be viewed as a generalization of the traditional notion of state Nerode (1958), which is defined as a statistic (i.e., a function of the observations) sufficient for input-output mapping. In contrast, we define an information state as a statistic sufficient for performance evaluation (and, therefore, for dynamic programming). Such a definition is hinted in Witsenhausen (1976). The notion of information state is also related to sufficient statistics for optimal control defined in Striebel (1965) for systems with state space models.\nAs far as we are aware, the informal definition of information state was first proposed by Kwakernaak (1965) for adaptive control systems. Formal definitions for linear control systems were given by Bohlin (1970) for discrete time systems and by Davis and Varaiya (1972) for continuous time systems. Kumar and Varaiya (1986) define an information state as a compression of past history which satisfies property (P2a) but do not formally show that such an information state always leads to a dynamic programming decomposition.\n\n\n\n\nBohlin, T. 1970. Information pattern for linear discrete-time models with stochastic coefficients. IEEE Transactions on Automatic Control (TAC) 15, 1, 104–106.\n\n\nCassandra, A., Littman, M.L., and Zhang, N.L. 1997. Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes. Proceedings of the thirteenth conference on uncertainty in artificial intelligence.\n\n\nCassandra, A.R., Kaelbling, L.P., and Littman, M.L. 1994. Acting optimally in partially observable stochastic domains. AAAI, 1023–1028.\n\n\nCheng, H.-T. 1988. Algorithms for partially observable markov decision processes.\n\n\nDavis, M.H.A. and Varaiya, P.P. 1972. Information states for linear stochastic systems. Journal of Mathematical Analysis and Applications 37, 2, 384–402.\n\n\nKumar, P.R. and Varaiya, P. 1986. Stochastic systems: Estimation identification and adaptive control. Prentice Hall.\n\n\nKwakernaak, H. 1965. Theory of self-adaptive control systems. In: Springer, 14–18.\n\n\nNerode, A. 1958. Linear automaton transformations. Proceedings of American Mathematical Society 9, 541–544.\n\n\nSmallwood, R.D. and Sondik, E.J. 1973. The optimal control of partially observable markov processes over a finite horizon. Operations Research 21, 5, 1071–1088. DOI: 10.1287/opre.21.5.1071.\n\n\nStriebel, C. 1965. Sufficient statistics in the optimal control of stochastic systems. Journal of Mathematical Analysis and Applications 12, 576–592.\n\n\nSubramanian, J., Sinha, A., Seraj, R., and Mahajan, A. 2022. Approximate information state for approximate planning and reinforcement learning in partially observed systems. Journal of Machine Learning Research 23, 12, 1–83. Available at: http://jmlr.org/papers/v23/20-1165.html.\n\n\nWitsenhausen, H.S. 1975. On policy independence of conditional expectation. Information and Control 28, 65–75.\n\n\nWitsenhausen, H.S. 1976. Some remarks on the concept of state. In: Y.C. Ho and S.K. Mitter, eds., Directions in large-scale systems. Plenum, 69–75.\n\n\nZhang, H. 2009. Partially observable Markov decision processes: A geometric technique and analysis. Operations Research.\n\n\nZhang, N. and Liu, W. 1996. Planning in stochastic domains: Problem characteristics and approximation. Hong Kong Univeristy of Science; Technology."
  },
  {
    "objectID": "linear-algebra/svd.html",
    "href": "linear-algebra/svd.html",
    "title": "14  Singular value decomposition",
    "section": "",
    "text": "15 Best rank-\\(k\\) approximations\nThere are two important matrix norms, the Frobenius norm which is defined as \\[\n  \\| A \\|_{F} = \\sqrt{ \\sum_{i,j} a_{ij}^2 }\n\\] and the induced norm which is defined as \\[\n  \\| A \\|_2 = \\max_{\\|x \\| = 1} \\| A x \\|.\n\\]\nNote that the Frobenius norm is equal to the square root of the sum of squares of the singular values and the \\(2\\)-norm is the largest singular value.\nLet \\(A\\) be an \\(n × d\\) matrix and think of \\(A\\) as the \\(n\\) points in \\(d\\)-dimensional space. The Frobenius norm of \\(A\\) is the square root of the sum of squared distance of the points to the origin. The induced norm is the square root of the sum of squared distances to the origin along the direction that maximizes this quantity."
  },
  {
    "objectID": "linear-algebra/svd.html#singular-values",
    "href": "linear-algebra/svd.html#singular-values",
    "title": "14  Singular value decomposition",
    "section": "14.1 Singular values",
    "text": "14.1 Singular values\nLet \\(A\\) be a \\(n × d\\) matrix. Then, the matrix \\(A^\\TRANS A\\) is a symmetric \\(d × d\\) matrix, so its eigenvalues are real. Moreover, \\(A^\\TRANS A\\) is positive semi-definite, so the eigen values are non-negative. Let \\(\\{ λ_1, \\dots, λ_d \\}\\) denote the eigenvalues of \\(A^\\TRANS A\\), with repetitions. Order then so that \\(λ_1 \\ge λ_2 \\ge \\dots \\ge λ_d \\ge 0\\). Let \\(σ_i = \\sqrt{λ_i}\\), so that \\(σ_1 \\ge σ_2 \\ge \\dots σ_d \\ge 0\\). These numbers are called the singular values of \\(A\\).\n\nProperties of singular values\n\nThe number of non-zero singular values of \\(A\\) equals to the rank of \\(A\\). In particular, if \\(A\\) is \\(n × d\\) where \\(n < d\\), then \\(A\\) has at most \\(n\\) nonzero singular values.\nIt can be shown that\n\\[ σ_1 = \\max_{\\|x\\| = 1}  \\| A x \\| . \\]\nLet \\(v_1\\) denote the arg-max of the above optimization. \\(v_1\\) is called the first singular vector of \\(A\\). Then,\n\\[ σ_2 = \\max_{ x \\perp v_1, \\|x \\| = 1}  \\| A x\\|. \\]\nLet \\(v_2\\) denote the arg-max of the above optimization. \\(v_2\\) is called the second singular vector of \\(A\\), and so on.\nLet \\(A\\) be a \\(n × d\\) matrix and \\(v_1, \\dots, v_r\\) be the singular vectors, where \\(r = \\text{rank}(A)\\). Then for any \\(k \\in \\{1, \\dots, r\\}\\), let \\(V_k\\) be the subspace spanned by \\(\\{v_1, \\dots, v_k\\}\\). Then, \\(V_k\\) is the best \\(k\\)-dimensional subspace for \\(A\\).\nFor any matrix \\(A\\), \\[ \\sum_{i =1}^r σ_i^2(A) = \\| A \\|_{F}^2\n   := \\sum_{j,k} a_{jk}^2. \\]\nAny vector \\(v\\) can be written as a linear combination of \\(v_1, \\dots, v_r\\) and a vector perpendicular to \\(V_r\\) (defined above). Now, \\(Av\\) can be written as the same linear combination of \\(Av_1, Av_2, \\dots, Av_r\\). So, \\(Av_1, \\dots, Av_r\\) form a fundamental set of vectors associated with \\(A\\). We normalize them to length one by \\[ u_i = \\frac{1}{σ_i(A)} A v_i. \\] The vectors \\(u_1, \\dots, u_r\\) are called the left singular vectors of \\(A\\). The \\(v_i\\) are called the right singular vectors.\nBoth the left and the right singular vectors are orthogonal.\n\n\n\n\n\n\n\nSingular value decomposition\n\n\n\nFor any matrix \\(A\\), \\[ A = \\sum_{i=1}^r σ_i u_i v_i^\\TRANS \\] where \\(u_i\\) and \\(v_i\\) are the left and right singular vectors, and \\(σ_i\\) are the singular values.\nEquivalently, in matrix notation: \\[ A = U D V^\\TRANS \\] where the columns of \\(U\\) and \\(V\\) consist of the left and right singular vectors, respectively, and \\(D\\) is a diagonal matrix whose diagonal entries are the singular values of \\(A\\).\n\n\nIf \\(A\\) is a positive definite square matrix, then the SVD and the eigen-decomposition coincide."
  },
  {
    "objectID": "linear-algebra/svd.html#notes",
    "href": "linear-algebra/svd.html#notes",
    "title": "14  Singular value decomposition",
    "section": "Notes",
    "text": "Notes\nThe chapter on SVD in Hopcroft and Kannan (2012) contains a nice intuitive explanation of SVD.\n\n\n\n\nHopcroft, J. and Kannan, R. 2012. Computer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf."
  },
  {
    "objectID": "linear-algebra/rkhs.html#review-of-linear-operators",
    "href": "linear-algebra/rkhs.html#review-of-linear-operators",
    "title": "15  Reproducing Kernel Hilbert Space",
    "section": "15.1 Review of Linear Operators",
    "text": "15.1 Review of Linear Operators\n\n\n\n\n\n\nLinear Operator\n\n\n\nLet \\(\\mathcal F\\) and \\(\\mathcal G\\) be normed vector spaces over \\(\\reals\\). A function \\(A \\colon \\mathcal F \\to \\mathcal G\\) is called a linear operator if it satisfies the following properties:\n\nHonogeneity: For any \\(α \\in \\reals\\) and \\(f \\in \\mathcal F\\), \\(A(αf) = α (Af)\\).\nAdditivity: For any \\(f,g \\in \\mathcal F\\), \\(A(f + g) = Af + Ag\\).\n\nThe operator norm of a linear operator is defined as \\[ \\NORM{A} = \\sup_{f \\in \\mathcal F} \\frac{ \\NORM{A f}_{\\mathcal G}}\n{\\NORM{f}}_{\\mathcal F}. \\]\nIf \\(\\NORM{A} < ∞\\), then the operator is said to be a bounded operator.\n\n\nAs an example, suppose \\(\\mathcal F\\) is an inner product space. For a \\(g \\in \\mathcal F\\), the operator \\(A_g \\colon \\mathcal F \\to \\reals\\) defined by \\(A_g(f) = \\langle f, g \\rangle\\) is a linear operator. Such scalar valued operators are called functionals on \\(\\mathcal F\\).\nLinear operators satisfy the following property.\n\nTheorem 15.1 If \\(A \\colon \\mathcal F \\to \\mathcal G\\) is a linear operator, then the following three conditions are equivalent:\n\n\\(A\\) is a bounded operator.\n\\(A\\) is continuous on \\(\\mathcal F\\).\n\\(A\\) is continious at one point of \\(\\mathcal F\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "href": "linear-algebra/rkhs.html#dual-of-a-linear-operator",
    "title": "15  Reproducing Kernel Hilbert Space",
    "section": "15.2 Dual of a linear operator",
    "text": "15.2 Dual of a linear operator\nThere are two notions of dual of a linear operator: algebraic dual and topological dual. If \\(\\mathcal F\\) is a normed space, then the space of all linear functionals \\(A \\colon \\mathcal F \\to \\reals\\) is the algebraic dual space of \\(\\mathcal F\\); the space of all continuous linear functions \\(A \\colon \\mathcal F \\to \\reals\\) is the topological dual space of \\(\\mathcal F\\).\nIn finite-dimensional space, the two notions of dual spaces coincide (every linear operator on a normed, finite dimensional space is bounded). But this is not the case for infinite dimensional spaces.\n\nTheorem 15.2 (Riesz representation) In a Hilbert space \\(\\mathcal F\\), all continuous linear functionals are of the form \\(\\langle\\cdot, g\\rangle\\), for some \\(g \\in \\mathcal F\\).\n\nTwo Hilbert spaces \\(\\mathcal F\\) and \\(\\mathcal G\\) are said to be isometrically isomorphic if there is a linear bijective map \\(U \\colon \\mathcal F \\to \\mathcal G\\) which preserves the inner product, i.e., \\(\\langle f_1, f_2 \\rangle_{\\mathcal F} = \\langle U f_1, U f_2 \\rangle_{\\mathcal G}\\).\nNote that Riesz representation theorem gives a natural isometric isomorphism \\(\\psi \\colon g \\mapsto \\langle \\cdot, g \\rangle_{\\mathcal F}\\) between \\(\\mathcal F\\) and its topological dual \\(\\mathcal F'\\), whereby \\(\\NORM{ψ(g)}_{\\mathcal F'} = \\NORM{g}_{\\mathcal F}\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "href": "linear-algebra/rkhs.html#reproducing-kernel-hilbert-space",
    "title": "15  Reproducing Kernel Hilbert Space",
    "section": "15.3 Reproducing kernel Hilbert space",
    "text": "15.3 Reproducing kernel Hilbert space\nLet \\(\\mathcal H\\) be a Hilbert space of functions mapping from some non-empty set \\(\\ALPHABET X\\) to \\(\\reals\\). Note that for every \\(x \\in \\ALPHABET X\\), there is a very special functional on \\(\\mathcal H\\): the one that assigns to each \\(f \\in \\mathcal H\\), its value at \\(x\\). This is called the evaluation functional and denoted by \\(δ_x\\). In particular, \\(δ_x \\colon \\mathcal H \\to \\reals\\), where \\(δ_x \\colon f \\mapsto f(x)\\).\n\n\n\n\n\n\nReproducing kernel Hilbert space (RKHS)\n\n\n\nA Hilbert space \\(\\mathcal H\\) of functions \\(f \\colon \\ALPHABET X \\to \\reals\\) defined on a non-empty set \\(\\ALPHABET X\\) is said to be a RKHS if \\(δ_x\\) is continuous for all \\(x \\in \\ALPHABET X\\).\n\n\nIn view of Theorem 15.1, an equivalent definition is that a Hilbert space \\(\\mathcal H\\) is RKHS if the evaluation functionals \\(δ_x\\) are bounded, i.e., for every \\(x \\in \\ALPHABET X\\), there exists a \\(M_x\\) such that \\[ | δ_x | = | f(x) | \\le M_x \\| f \\|_{\\mathcal H}, \\quad \\forall f \\in \\mathcal H\\]\nAn immediate implication of the above property is that two functions which agree in RKHS norm agree at every point: \\[ | f(x) - g(x) | = | δ_x(f - g) | \\le M_x \\| f - g \\|_{\\mathcal H},\n   \\quad \\forall f,g \\in \\mathcal H. \\]\nFor example, the \\(L_2\\) space of square integrable functions i.e., \\(\\int_{\\reals^n} f(x)^2 dx < ∞\\) with inner product \\(\\int_{\\reals^n} f(x) g(x)dx\\) is a Hilbert space, but not an RKHS because the delta function, which has the reproducing property \\[ f(x) = \\int_{\\reals^n} δ(x - y) f(y) dy \\] is not bounded.\nRKHS are particularly well behaved. In particular, if we have a sequence of functions \\(\\{f_n\\}_{n \\ge 1}\\) which converges to a limit \\(f\\) in the Hilbert-space norm, i.e., \\(\\lim_{n \\to ∞} \\NORM{f_n - f}_{\\mathcal H} = 0\\), then they also converge pointwise, i.e., \\(\\lim_{n \\to ∞} f_n(x) = f(x)\\) for all \\(x \\in \\ALPHABET X\\)."
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-rhks",
    "href": "linear-algebra/rkhs.html#properties-of-rhks",
    "title": "15  Reproducing Kernel Hilbert Space",
    "section": "15.4 Properties of RHKS",
    "text": "15.4 Properties of RHKS\nRKHS has many useful properties:\n\nFor any RKHS, there exists a unique kernel \\(k \\colon \\ALPHABET X × \\ALPHABET X \\to \\reals\\) such that\n\nfor any \\(x \\in \\ALPHABET X\\), \\(k(\\cdot, x) \\in \\mathcal H\\),\nfor any \\(x \\in \\ALPHABET X\\) and \\(f \\in \\mathcal H\\), \\(\\langle f, k(\\cdot, x) \\rangle = f(x)\\) (the reproducing property).\n\nIn particular, for any \\(x,y \\in \\ALPHABET X\\), \\[ k(x,y) = \\langle k(\\cdot, x), k(\\cdot, y) \\rangle. \\] Thus, the kernel is a symmetric function.\nThe kernel is positive definite, i.e., for any \\(n \\ge 1\\), for all \\((a_1, \\dots, a_n) \\in \\reals^n\\) and \\((x_1, \\dots, x_n) \\in \\ALPHABET X^n\\), \\[ \\sum_{i=1}^n \\sum_{j=1}^n a_i a_i h(x_i, x_j) \\ge 0 \\]\nA conseuqence of positive definiteness is that \\[| k(x, y)|^2 \\le k(x, x) k(y, y). \\]\n(Moore-Aronszajn Theorem) For every positive definite kernel \\(K\\) on \\(\\ALPHABET X × \\ALPHABET X\\), there is a unique RKHS on \\(\\ALPHABET X\\) with \\(K\\) as its reproducing kernel."
  },
  {
    "objectID": "linear-algebra/rkhs.html#examples-of-kernels",
    "href": "linear-algebra/rkhs.html#examples-of-kernels",
    "title": "15  Reproducing Kernel Hilbert Space",
    "section": "15.5 Examples of kernels",
    "text": "15.5 Examples of kernels\nSome common examples of symmetric positive definite kernels for \\(\\ALPHABET X = \\reals^n\\) are as follows:\n\nLinear kernel \\[ k(x,y) = \\langle x, y \\rangle\\]\nGaussian kernel \\[ k(x,y) = \\exp\\biggl( - \\frac{\\| x - y \\|^2}{σ^2} \\biggr),\n   \\quad σ > 0. \\]\nPolynomail kernel \\[ k(x,y) = \\bigl( 1 + \\langle x, y \\rangle \\bigr)^d,\n   \\quad d \\in \\integers_{> 0}. \\]"
  },
  {
    "objectID": "linear-algebra/rkhs.html#properties-of-kernels",
    "href": "linear-algebra/rkhs.html#properties-of-kernels",
    "title": "15  Reproducing Kernel Hilbert Space",
    "section": "15.6 Properties of kernels",
    "text": "15.6 Properties of kernels\n\nSuppose \\(φ \\colon \\ALPHABET X \\to \\reals^n\\) is a feature map, then \\[ k(x,y) := \\langle φ(x), φ(y) \\rangle \\] is a kernel.\nNote that there are no conditions on \\(\\ALPHABET X\\) (e.g., \\(\\ALPHABET X\\) doesn’t need to be an inner product space).\nIf \\(k\\) is a kernel on \\(\\ALPHABET X\\), then for any \\(α > 0\\), \\(αk\\) is also a kernel.\nIf \\(k_1\\) and \\(k_2\\) are kernels on \\(\\ALPHABET X\\), then \\(k_1 + k_2\\) is also a kernel.\nIf \\(\\ALPHABET X\\) and \\(\\ALPHABET Y\\) be arbitrary sets and \\(A \\colon \\ALPHABET X \\to \\ALPHABET Y\\) is a map. Let \\(k\\) be a kernel on \\(\\ALPHABET Y\\). Then, \\(k(A(x_1), A(x_2))\\) is a kernel on \\(\\ALPHABET X\\).\nIf \\(k_1 \\colon \\ALPHABET X_1 × \\ALPHABET X_1 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_1\\) and \\(k_2 \\colon \\ALPHABET X_2 × \\ALPHABET X_2 \\to \\reals\\) is a kernel on \\(\\ALPHABET X_2\\), then \\[ k( (x_1, x_2), (y_1, y_2) ) = k_1(x_1, y_1) k_2(x_2, y_2) \\] is a kernel on \\(\\ALPHABET X_1 × \\ALPHABET X_2\\).\n(Mercer-Hilber-Schmit theorems) If \\(k\\) is positive definite kernel (that is continous with finite trace), then there exists an infinite sequence of eiegenfunctions \\(\\{ φ_i \\colon \\ALPHABET X \\to \\reals \\}_{i \\ge 1}\\) and real eigenvalues \\(\\{λ_i\\}_{i \\ge 1}\\) such that we can write \\(k\\) as: \\[ k(x,y) = \\sum_{i=1}^∞ λ_i φ_i(x) φ_i(y). \\] This is analogous to the expression of a matrix in terms of its eigenvector and eigenvalues, except in this case we have functions and an infinity of them.\nUsing this property, we can define the inner product of RKHS in a simpler form. First, for any \\(f \\in \\mathcal H\\), define \\[ f_i = \\langle f, φ_i \\rangle.\\] Then, for any \\(f, g \\in \\mathcal H\\), \\[ \\langle f, g \\rangle = \\sum_{i=1}^∞ \\frac{ f_i g_i } { λ_i }. \\]"
  },
  {
    "objectID": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "href": "linear-algebra/rkhs.html#kernel-ridge-regression",
    "title": "15  Reproducing Kernel Hilbert Space",
    "section": "15.7 Kernel ridge regression",
    "text": "15.7 Kernel ridge regression\nGiven labelled data \\(\\{ (x_i, y_i) \\}_{i=1}^n\\), and a feature map \\(φ \\colon \\ALPHABET X \\to \\ALPHABET Z\\), define the RKHS \\(\\ALPHABET H\\) of functions from \\(\\ALPHABET Z \\to \\reals\\) with the kernel \\(k(x,y) = \\langle φ(x), φ(y) \\rangle_{\\mathcal H}\\). Now, consider the problem of minimizing\n\\[f^* = \\arg \\min_{f \\in \\ALPHABET H}\n\\biggl(\n  \\sum_{i=1}^n \\bigl( y_i - \\langle f, φ(x_i) \\rangle_{\\mathcal{H}} \\bigr)^2 +\n  λ \\NORM{f}^2_{\\mathcal H}\n\\bigr).\\]\n\nTheorem 15.3 (The representer theoreom (simple version)) Given a loss function \\(\\ell \\colon \\ALPHABET Z^n \\to \\reals\\) and a penalty function \\(Ω \\colon \\reals \\to \\reals\\), there is as a solution of \\[ f^* = \\arg \\min_{f \\in \\mathcal H} \\ell(f(x_1), \\dots, f(x_n))\n        + Ω(\\NORM{f}^2_{\\mathcal H}). \\] that takes the the form \\[ f^* = \\sum_{i=1}^n α_i k(\\cdot, x_i).\\]\nIf \\(Ω\\) is strictly increasing, all solutions have this form.\n\nUsing the representer theorem, we know that the solution is of the form \\[ f = \\sum_{i=1}^n α_i φ(x_i). \\] Then, \\[\n\\sum_{i=1}^n \\bigl( y_i - \\langle f, φ_i(x_i) \\rangle_{\\mathcal H} \\bigr)^2\n  + λ \\NORM{f}_{\\mathcal H}^2\n= \\NORM{ y - K α}^2 + λ α^\\TRANS K α. \\]\nDifferentiating wrt \\(α\\) and setting this to zero, we get \\[\n  α^* = (K + λI_n)^{-1} y.\n\\]"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arrow, K.J., Harris, T., and Marschak, J.\n1952. Optimal inventory policy. Econometrica 20, 1,\n250–272. DOI: 10.2307/1907830.\n\n\nBerry, R.A. 2000. Power and delay\ntrade-offs in fading channels. Available at: https://dspace.mit.edu/handle/1721.1/9290.\n\n\nBerry, R.A. 2013. Optimal power-delay\ntradeoffs in fading channels—small-delay asymptotics. IEEE\nTransactions on Information Theory 59, 6, 3939–3952. DOI:\n10.1109/TIT.2013.2253194.\n\n\nBerry, R.A. and Gallager, R.G. 2002.\nCommunication over fading channels with delay constraints.\nIEEE Transactions on Information Theory\n48, 5, 1135–1149. DOI: 10.1109/18.995554.\n\n\nBerry, R., Modiano, E., and Zafer, M.\n2012. Energy-efficient scheduling under delay constraints for wireless\nnetworks. Synthesis Lectures on Communication Networks\n5, 2, 1–96. DOI: 10.2200/S00443ED1V01Y201208CNT011.\n\n\nBitar, E., Poolla, K., Khargonekar, P.,\nRajagopal, R., Varaiya, P., and Wu, F. 2012. Selling random wind.\n2012 45th hawaii international conference on system sciences,\nIEEE, 1931–1937.\n\n\nBlackwell, D. 1964. Memoryless strategies\nin finite-stage dynamic programming. The Annals of Mathematical\nStatistics 35, 2, 863–865. DOI: 10.1214/aoms/1177703586.\n\n\nBohlin, T. 1970. Information pattern for\nlinear discrete-time models with stochastic coefficients. IEEE\nTransactions on Automatic Control (TAC) 15, 1, 104–106.\n\n\nCassandra, A., Littman, M.L., and Zhang,\nN.L. 1997. Incremental pruning: A simple, fast, exact method for\npartially observable Markov decision processes.\nProceedings of the thirteenth conference on uncertainty\nin artificial intelligence.\n\n\nCassandra, A.R., Kaelbling, L.P., and Littman,\nM.L. 1994. Acting optimally in partially observable stochastic\ndomains. AAAI, 1023–1028.\n\n\nChakravorty, J. and Mahajan, A. 2018.\nSufficient conditions for the value function and optimal strategy to be\neven and quasi-convex. IEEE Transactions on Automatic Control\n63, 11, 3858–3864. DOI: 10.1109/TAC.2018.2800796.\n\n\nCheng, H.-T. 1988. Algorithms for\npartially observable markov decision processes.\n\n\nDaley, D.J. 1968. Stochastically monotone\nmarkov chains. Zeitschrift für\nWahrscheinlichkeitstheorie und verwandte Gebiete 10, 4,\n305–317. DOI: 10.1007/BF00531852.\n\n\nDavis, M.H.A. and Varaiya, P.P. 1972.\nInformation states for linear stochastic systems. Journal of\nMathematical Analysis and Applications 37, 2, 384–402.\n\n\nDing, N., Sadeghi, P., and Kennedy, R.A.\n2016. On monotonicity of the optimal transmission policy in cross-layer\nadaptive m -QAM modulation.\nIEEE Transactions on Communications 64, 9, 3771–3785.\nDOI: 10.1109/TCOMM.2016.2590427.\n\n\nEdgeworth, F.Y. 1888. The mathematical\ntheory of banking. Journal of the Royal Statistical Society\n51, 1, 113–127. Available at: https://www.jstor.org/stable/2979084.\n\n\nFerguson, T.S. and Gilstein, C.Z. 2004.\nOptimal investment policies for the horse race model\". Available at: https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf.\n\n\nHinderer, K. 2005. Lipschitz continuity\nof value functions in Markovian decision processes.\nMathematical Methods of Operations Research 62, 1,\n3–22. DOI: 10.1007/s00186-005-0438-1.\n\n\nHopcroft, J. and Kannan, R. 2012.\nComputer science theory for the information age. Available at: https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf.\n\n\nKeilson, J. and Kester, A. 1977. Monotone\nmatrices and monotone markov processes. Stochastic Processes and\ntheir Applications 5, 3, 231–241.\n\n\nKelly, J.L., Jr. 1956. A new\ninterpretation of information rate. Bell System Technical\nJournal 35, 4, 917–926. DOI: 10.1002/j.1538-7305.1956.tb03809.x.\n\n\nKoole, G. 2006. Monotonicity in markov\nreward and decision chains: Theory and applications. Foundations and\nTrends in Stochastic Systems 1, 1, 1–76. DOI:\n10.1561/0900000002.\n\n\nKumar, P.R. and Varaiya, P. 1986.\nStochastic systems: Estimation identification and adaptive\ncontrol. Prentice Hall.\n\n\nKwakernaak, H. 1965. Theory of\nself-adaptive control systems. In: Springer, 14–18.\n\n\nLevy, H. 1992. Stochastic dominance and\nexpected utility: Survey and analysis. Management Science\n38, 4, 555–593. DOI: 10.1287/mnsc.38.4.555.\n\n\nLevy, H. 2015. Stochastic dominance:\nInvestment decision making under uncertainty. Springer. DOI: 10.1007/978-3-319-21708-6.\n\n\nMorse, P. and Kimball, G. 1951.\nMethods of operations research. Technology Press of MIT.\n\n\nNerode, A. 1958. Linear automaton\ntransformations. Proceedings of American Mathematical\nSociety 9, 541–544.\n\n\nPorteus, E.L. 2008. Building intuition:\nInsights from basic operations management models and principles. In: D.\nChhajed and T.J. Lowe, eds., Springer, 115–134. DOI: 10.1007/978-0-387-73699-0.\n\n\nPuterman, M.L. 2014. Markov decision\nprocesses: Discrete stochastic dynamic programming. John Wiley\n& Sons. DOI: 10.1002/9780470316887.\n\n\nRachelson, E. and Lagoudakis, M.G. 2010.\nOn the locality of action domination in sequential decision making.\nProceedings of 11th international symposium on artificial\nintelligence and mathematics. Available at: https://oatao.univ-toulouse.fr/17977/.\n\n\nRoss, S.M. 1974. Dynamic programming and\ngambling models. Advances in Applied Probability 6, 3,\n593–606. DOI: 10.2307/1426236.\n\n\nSayedana, B. and Mahajan, A. 2020.\nCounterexamples on the monotonicity of delay optimal strategies for\nenergy harvesting transmitters. IEEE Wireless\nCommunications Letters, 1–1. DOI: 10.1109/lwc.2020.2981066.\n\n\nSayedana, B., Mahajan, A., and Yeh, E.\n2020. Cross-layer communication over fading channels with adaptive\ndecision feedback. International symposium on modeling and\noptimization in mobile, ad hoc, and wireless networks (WiOPT), 1–8.\n\n\nSerfozo, R.F. 1976. Monotone optimal\npolicies for markov decision processes. In: Mathematical programming\nstudies. Springer Berlin Heidelberg, 202–215. DOI: 10.1007/bfb0120752.\n\n\nSmallwood, R.D. and Sondik, E.J. 1973.\nThe optimal control of partially observable markov processes over a\nfinite horizon. Operations Research 21, 5, 1071–1088.\nDOI: 10.1287/opre.21.5.1071.\n\n\nStriebel, C. 1965. Sufficient statistics\nin the optimal control of stochastic systems. Journal of\nMathematical Analysis and Applications 12, 576–592.\n\n\nSubramanian, J., Sinha, A., Seraj, R., and\nMahajan, A. 2022. Approximate information state for approximate\nplanning and reinforcement learning in partially observed systems.\nJournal of Machine Learning Research 23, 12, 1–83.\nAvailable at: http://jmlr.org/papers/v23/20-1165.html.\n\n\nTopkis, D.M. 1998. Supermodularity\nand complementarity. Princeton University Press.\n\n\nWhitin, S. 1953. The theory of\ninventory management. Princeton University Press.\n\n\nWhittle, P. 1996. Optimal control:\nBasics and beyond. Wiley.\n\n\nWitsenhausen, H.S. 1975. On policy\nindependence of conditional expectation. Information and\nControl 28, 65–75.\n\n\nWitsenhausen, H.S. 1976. Some remarks on\nthe concept of state. In: Y.C. Ho and S.K. Mitter, eds., Directions\nin large-scale systems. Plenum, 69–75.\n\n\nWitsenhausen, H.S. 1979. On the structure\nof real-time source coders. Bell System Technical Journal\n58, 6, 1437–1451.\n\n\nYeh, E.M. 2012. Fundamental performance\nlimits in cross-layer wireless optimization: Throughput, delay, and\nenergy. Foundations and Trends in Communications and Information\nTheory 9, 1, 1–112. DOI: 10.1561/0100000014.\n\n\nZhang, H. 2009. Partially observable\nMarkov decision processes: A geometric technique and\nanalysis. Operations Research.\n\n\nZhang, N. and Liu, W. 1996. Planning\nin stochastic domains: Problem characteristics and approximation.\nHong Kong Univeristy of Science; Technology."
  },
  {
    "objectID": "assignments/01.html",
    "href": "assignments/01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Exercise 1.1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 1.2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.\nExercise 2.3 from the notes on the newsvendor problem. Provide an analytic solution to the problem, similar to the derivation of the analytic solution for the case of continuous demand and actions in the notes."
  }
]